# 19 - å·ç§¯å±‚é‡Œçš„å¤šè¾“å…¥å¤šè¾“å‡ºé€šé“

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰
[![Bilibil](https://i1.hdslb.com/bfs/archive/66e9026b84cf669fc6c3862ac4648b8d3349275a.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1MB4y1F7of)
## å¤šä¸ªè¾“å…¥ä¸è¾“å‡ºé€šé“

### å¤šä¸ªè¾“å…¥é€šé“

å½©è‰²å›¾åƒå¯èƒ½æœ‰RGBä¸‰ä¸ªé€šé“

è½¬æ¢ä¸ºç°åº¦ä¼šä¸¢å¤±ä¿¡æ¯

![](\Images/thumb_photoshop-tutorial-r-separate-an-image-into-rgb-color-layers-57693846.png)

æ¯ä¸ªé€šé“éƒ½æœ‰ä¸€ä¸ªå·ç§¯æ ¸ï¼Œè¾“å‡ºæ˜¯æ‰€æœ‰é€šé“å·ç§¯ç»“æœåœ°å’Œã€‚

è¾“å…¥$\bf X$ï¼š$c_i\times n_h\times n_w$
æ ¸$\bf W$ï¼š$c_i\times k_h\times k_w$
è¾“å‡º$\bf Y$ï¼š$m_h\times m_w$

$$\bf Y=\sum_{i=0}^{c_i}X_{i,:,:}\star W_{i,:,:}$$

### å¤šä¸ªè¾“å‡ºé€šé“

æˆ‘ä»¬å¯ä»¥æœ‰å¤šä¸ªä¸‰ä½å·ç§¯æ ¸ï¼Œæ¯ä¸ªæ ¸ç”Ÿæˆä¸€ä¸ªè¾“å‡ºé€šé“ï¼›

è¾“å…¥$\bf X$ï¼š$c_i\times n_h\times n_w$
æ ¸$\bf W$ï¼š$c_o\times c_i\times k_h\times k_w$
è¾“å‡º$\bf Y$ï¼š$c_o\times m_h\times m_w$

$$\bf Y_{i,:,:}=X\star W_{i,:,:,:}$$

### å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºé€šé“

æ¯ä¸ªè¾“å‡ºé€šé“å¯ä»¥è¯†åˆ«ç‰¹å®šæ¨¡å¼ï¼›
è¾“å…¥é€šé“æ ¸è¯†åˆ«å¹¶ç»„åˆè¾“å…¥ä¸­çš„æ¨¡å¼ï¼›

**$1\times 1$å·ç§¯å±‚**

$k_h=k_w=1$æ˜¯ä¸€ä¸ªå—æ¬¢è¿çš„é€‰æ‹©ï¼Œå®ƒä¸è¯†åˆ«ç©ºé—´æ¨¡å¼ï¼Œåªæ˜¯èåˆé€šé“ã€‚

**äºŒç»´å·ç§¯å±‚**

è¾“å…¥$\bf X$ï¼š$c_i\times n_h\times n_w$
æ ¸$\bf W$ï¼š$c_o\times c_i\times k_h\times k_w$
åå·®$\bf B$ï¼š$c_o\times c_i$
è¾“å‡º$\bf Y$ï¼š$c_o\times m_h\times m_w$

$$\bf Y=X\star W+B$$

è®¡ç®—å¤æ‚åº¦ï¼ˆæµ®ç‚¹è®¡ç®—æ•°FLOPï¼‰ï¼š$O(c_ic_ok_hk_wm_hm_w)$

$$\begin{array} {l}
c_i=c_o=100\\
k_h=h_w=5\\
m_h=m_w=64\end{array}\quad\rightarrow 1GFLOP$$

10å±‚ï¼Œ1Mæ ·æœ¬ï¼Œ10PFlops

**æ€»ç»“**

è¾“å‡ºé€šé“æ•°æ—¶å·ç§¯å±‚çš„è¶…å‚æ•°
æ¯ä¸ªè¾“å…¥é€šé“æœ‰ç‹¬ç«‹çš„äºŒç»´å·ç§¯æ ¸ï¼Œæ‰€æœ‰é€šé“ç»“æœç›¸åŠ å¾—åˆ°ä¸€ä¸ªè¾“å‡ºé€šé“ç»“æœ
æ¯ä¸ªè¾“å‡ºé€šé“æœ‰ç‹¬ç«‹çš„ä¸‰ç»´å·ç§¯æ ¸

### ä»£ç å®ç°
```
import torch
from d2l import torch as d2l
#å¤šè¾“å…¥
def corr2d_multi_in(X, K):
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
#å¤šè¾“å…¥å¤šè¾“å‡º
def corr2d_multi_in_out(X, K):
    return torch.stack([corr2d_multi_in(X, k) for k in K], dim=0)
# stack():Concatenates a sequence of tensors along a new dimension.
# All tensors need to be of the same size.
# dim=0

K = torch.stack((K, K + 1, K + 2), dim=0)
K.shape
#1X1éªŒè¯
def corr2d_multi_in_out_1x1(X, K):
    c_i, h, w = X.shape
    c_o = K.shape[0]
    X = X.reshape((c_i, h * w))
    #æ‹‰æˆä¸€ä¸ªçŸ©é˜µ
    K = K.reshape((c_o, c_i))
    #æ‹‰æˆä¸€ä¸ªçŸ©é˜µ
    Y = torch.matmul(K, X)
    #çŸ©é˜µä¹˜æ³•
    return Y.reshape((c_o, h, w))
    #è¿”å›ä¸‰ç»´

X = torch.normal(0, 1, (3, 3, 3))
K = torch.normal(0, 1, (2, 3, 1, 1))

Y1 = corr2d_multi_in_out_1x1(X, K)
Y2 = corr2d_multi_in_out(X, K)
assert float(torch.abs(Y1 - Y2).sum()) < 1e-6
#è°ƒç”¨æ¨¡å—
nn.Conv2d(input_channel, output_channel, kernel_size, padding, stride)
```


