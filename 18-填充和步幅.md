## 填充和步幅

给定$(32\times32)$输入图像，应用$(5\times5)$卷积核，第$1$层得到输出大小$28\times28$，第$7$层得到输出大小$4\times4$。

更大的卷积核可以更快地减小输出大小：形状从$n_k\times n_w$减小到$(n_h-n_k+1)\times(n_w-k_w+1)$

![](\Images/0rs9l.gif)

### 填充

在输入周围添加额外的行和列。

填充$p_h$行和$p_w$列，输出形状为：

$$(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)$$

通常取$p_h=k_h-1$，$p_w=k_w-1$，使输入输出形状相同；

当$k_h$为奇数：在上下两侧填充$p_h/2$
当$k_h$为偶数：在上侧填充$\lceil p_h/2\rceil$，下侧填充$\lfloor p_h/2\rfloor$

### 步幅

填充减小的输出大小与层数线性相关：给定输入大小$224\times 224$，在使用$5\times5$卷积核的情况下，需要$55$层将输出降低到$4\times4$，所以需要的大量的输入层。

![](\Images/src=http___img-blog.csdnimg.cn_20210329203509241.gif&refer=http___img-blog.csdnimg.gif)

步幅是指行、列的滑动补偿。

给定高度$s_h$和宽度$s_w$的步幅，输出形状是：

$$\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor\times\lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor$$

如果$p_h=k_h-1$，$p_w=k_w-1$

$$\lfloor(n_h+s_h-1)/s_h\rfloor\times\lfloor(n_w+s_w-1)/s_w\rfloor$$

如果输入高度和宽度可以被步幅整除（步幅与卷积核大小相当）

$$(n_h/s_h)\times(n_w/s_w)$$

填充和步幅使卷积层的超参数；

填充在输入周围添加额外的行、列，来控制输出形状的减少量；

步幅是每次滑动核窗口时的行、列步长，可以成倍地减少输出形状。

## 代码实现

![](\Images/1_ulfFYH5HbWpLTIfuebj5mQ.gif)

**padding**

```
import torch
from torch import nn

def comp_conv2d(conv2d, X):
    X = X.reshape((1, 1) + X.shape)
    #（1，1）代表input_channel=1, batch_size=1
    Y = conv2d(X)
    return Y.reshape(Y.shape[2:])
    #Y.shape returns tensor(1,1,8,8), slice[2:] returns tensor(8,8)

conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)
#3*3
#padding=1表示上下左右各填充1, 10*10
X = torch.rand(size=(8, 8))
comp_conv2d(conv2d, X).shape
```
```
conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))
#12*10
comp_conv2d(conv2d, X).shape
#8*8
```
**stride**

```
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)
##10*10
comp_conv2d(conv2d, X).shape
##(10-3)/2+1=4.5, 4*4
```
```
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
### 8*10 3*5 3*4
### floor[（8-3）/3+1]=2 florr[(10-5)/4+1]=2
comp_conv2d(conv2d, X).shape
```