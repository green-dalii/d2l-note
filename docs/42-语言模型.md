# 42 - è¯­è¨€æ¨¡å‹

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‡

[![Bilibil](https://i1.hdslb.com/bfs/archive/7831a88cef7a4f169648bd21e8b1cb7fe0ca104d.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1ZX4y1F7K3)

## è¯­è¨€æ¨¡å‹

- ç›®å‰çš„è¯­è¨€æ¨¡å‹ï¼Œä¸»è¦æ€è·¯æ˜¯ç»™å®šæ–‡æœ¬åºåˆ— $x_1,...,x_T$ï¼Œç›®æ ‡ä¸ºä¼°è®¡å…¶å‡ºç°çš„**è”åˆæ¦‚ç‡** $p(x_1,...,x_T)$
- è”åˆæ¦‚ç‡å¯ç”¨äºï¼š
  - åš**é¢„è®­ç»ƒæ¨¡å‹**å» Fine-tune(eg [BERT](57-BERT.md), GPT-3)
  - ç”Ÿæˆæœ¬æ–‡ï¼Œç»™å®šå‰é¢å‡ ä¸ªè¯ï¼Œä¸æ–­åœ°ä½¿ç”¨ $x_t \thicksim p(x_t|x_1,...,x_{t-1})$ï¼Œç”Ÿæˆåç»­æ–‡æœ¬
  - åˆ¤æ–­å¤šä¸ªåºåˆ—ä¸­å“ªä¸ªæ›´å¸¸è§
    - åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸï¼Œe.g. "to recognize a speech" vs "to wreck a nice beach"ï¼Œå¯¹äºä¸¤ä¸ªå‘éŸ³ç›¸è¿‘çš„å¥å­ï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥é¢„æµ‹å…¶å¯èƒ½æ€§(åˆç†æ€§)ã€‚
    - è¾“å…¥æ³•é¢†åŸŸä¹Ÿæ˜¯ç±»ä¼¼ï¼Œç»™å®šä¸€ä¸ªé”®ç›˜è¾“å…¥åºåˆ—ï¼Œåˆ¤æ–­å“ªç§è¾“å‡ºæ›´å¸¸è§

è¿™ä¸ªé¢†åŸŸçš„é‡è¦å‘å±•ï¼Œæœ€æ—©æ˜¯ç”± MIT ç§‘å­¦å®¶ï¼Œåé¢†å¯¼äº†è‘—åçš„ IBM Watson å®éªŒå®¤çš„å¼—é‡Œå¾·é‡Œå…‹Â·è´¾é‡Œå°¼å…‹ï¼ˆFrederick Jelinekï¼Œ[wiki](https://en.wiki.hancel.org/wiki/Frederick_Jelinek)ï¼‰æå‡ºå°†**ä¿¡æ¯è®º**ä¸**ç»Ÿè®¡æ–¹æ³•**æˆåŠŸåº”ç”¨äºè§£å†³è¯­éŸ³è¯†åˆ«é—®é¢˜åï¼Œå¸¦æ¥çš„æ•´ä¸ª NLP é¢†åŸŸæ–¹æ³•è®ºçš„é©æ–°ã€‚æå¼€å¤ä¹Ÿæ˜¯è´¾é‡Œå°¼å…‹çš„å­¦ç”Ÿã€‚

> æœ¬èŠ‚è¯¾æ²ç¥è®²çš„è¿è´¯æ€§ä¸æ˜¯ç‰¹åˆ«å¥½ï¼Œå¦‚æœè¿˜æƒ³äº†è§£è¯­è¨€æ¨¡å‹ç›¸å…³æ¡ˆä¾‹ï¼Œå¼ºçƒˆæ¨èç«¥é‹é˜…è¯»å´å†›è€å¸ˆçš„ç§‘æ™®ä¹¦ã€Šæ•°å­¦ä¹‹ç¾ã€‹ç¬¬ä¸‰ç« ï¼Œå¯¹è¯­è¨€æ¨¡å‹çš„å‰ä¸–ä»Šç”Ÿã€å·¥ä¸šç•Œï¼ˆGoogleï¼‰çš„å‘å±•åº”ç”¨éƒ½æœ‰ååˆ†ç”ŸåŠ¨æœ‰è¶£çš„ä»‹ç» ğŸ‘‡

![book_beauty_of_math](Images/beauty_of_math.jpg)

## ä½¿ç”¨è®¡æ•°æ¥å»ºæ¨¡

> è¿™é‡Œå°†è®¡æ•°é¢‘ç‡ç­‰ä»·äºæ¦‚ç‡æœ‰å‰æï¼Œé‚£å°±æ˜¯â€œ**å¤§æ•°å®šå¾‹**â€ï¼Œè¦æ±‚è¯­æ–™è¦è¶³å¤Ÿå¤§

- å‡è®¾åºåˆ—é•¿åº¦ä¸º 2ï¼Œæˆ‘ä»¬é¢„æµ‹è”åˆæ¦‚ç‡

  $$p(x,x')=p(x)p(x'|x)={n(x)\over n}{n(x,x')\over n(x)}$$

  > $n$ æ˜¯æ€»è¯æ•°ï¼Œ $n(x),n(x,x')$ æ˜¯å•ä¸ªå•è¯å’Œè¿ç»­å•è¯å¯¹ï¼ˆä¸€å‰ä¸€åï¼‰çš„å‡ºç°æ¬¡æ•°

- å¾ˆå®¹æ˜“æ‹“å±•åˆ°é•¿ä¸º 3 çš„æƒ…å†µ

  $$p(x,x',x'')=p(x)p(x'|x)p(x''|x,x')={n(x)\over n}{n(x,x')\over n(x)}{n(x,x',x'')\over n(x,x')}$$

é€šè¿‡æ‰«ä¸€éæ•´ä¸ªè¯­æ–™åº“ï¼Œç»Ÿè®¡è¯¥åºåˆ—ä¸­æ¯ä¸ªè¯å’Œå…¶ä¸å‡ ä¸ªè¯ä¸€èµ·å‡ºç°çš„æ¬¡æ•°ï¼Œæ ¹æ®å¤§æ•°å®šå¾‹ï¼Œå°±å¯ä»¥è®¡ç®—å‡ºä¸Šè¿°è”åˆæ¦‚ç‡ã€‚ä¸è¿‡ç†è®ºä¸Šè™½ç„¶å¯è¡Œï¼Œä½†å½“è¦æ±‚çš„åºåˆ—å¾ˆé•¿æ—¶ï¼ˆæ¯”å¦‚ä¸€ä¸ªé•¿å¥ï¼‰ï¼Œå› ä¸ºæœé›†è¯­æ–™åº“æˆæœ¬æ‰€é™ï¼Œæ–‡æœ¬é‡ä¸å¤Ÿå¤§ï¼Œå¾ˆå¯èƒ½é€ æˆ $n(x_1,...,x_T)$æ¥è¿‘äºé›¶è€Œæ— æ³•è®¡ç®—å¾—åˆ°è¯¥åºåˆ—çš„è”åˆæ¦‚ç‡ã€‚

## N å…ƒæ¨¡å‹ï¼ˆN-gram Modelï¼‰

ä½¿ç”¨**é©¬å°”å¯å¤«å‡è®¾**å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œè¯¥å‡è®¾å¯å°†é—®é¢˜ç®€åŒ–ä¸ºå½“å‰è¯æ¦‚ç‡åªä¸å‰ä¸€ä¸ªè¯ï¼ˆæˆ–å‡ ä¸ªè¯ï¼‰æœ‰å…³ï¼Œä¸å…¶ä»–è¯æ— å…³ï¼Œè¿™å°±é¿å…äº†æ±‚é•¿éš¾å¥åºåˆ—çš„è”åˆæ¦‚ç‡ã€‚è™½ç„¶è¯¥æ–¹æ³•ä¸§å¤±äº†ç²¾ç¡®æ€§ï¼Œä½†æ˜¯åœ¨æ•°å­¦ä¸­ä¹Ÿå¹¿æ³›åº”ç”¨ï¼Œæ•ˆæœä¹Ÿä¸é”™ï¼Œä¸å¤±ä¸€ç§å¯¹å¤æ‚é—®é¢˜è¿‘ä¼¼é€¼è¿‘çš„æ–¹æ³•ã€‚

> ç‰¹åˆ«çš„ï¼Œå½“åªä¸å‰ä¸€ä¸ªè¯æœ‰å…³æ—¶ï¼Œå«åšäºŒå…ƒæ¨¡å‹ï¼ˆBigram Modelï¼‰ï¼Œä¸å‰ä¸¤ä¸ªè¯æœ‰å…³ï¼Œå«ä¸‰å…ƒæ¨¡å‹ï¼Œä¸å‰ N-1 ä¸ªè¯æœ‰å…³ï¼Œåˆ™ç§°ä¸º N å…ƒæ¨¡å‹ï¼ˆN-gram Modelï¼‰ï¼Œé©¬å°”å¯å¤«å‡è®¾ä¸çœŸå®æƒ…å†µçš„ç²¾åº¦å·®å¯ä»¥éšç€ N çš„å¢å¤§è€Œå‡å°ã€‚

ä»¥è®¡ç®—åºåˆ—é•¿ä¸º4çš„è”åˆæ¦‚ç‡ä¸ºä¾‹ï¼Œä½¿ç”¨æ¡ä»¶æ¦‚ç‡å±•å¼€ï¼š

- ä¸€å…ƒæ¨¡å‹ï¼Œå³ä¸Šä¸‹æ–‡æ— å…³ï¼š$\tau=0$

$$
  \begin{aligned}
      p(x_1,x_2,x_3,x_4)
      &=p(x_1)p(x_2)p(x_3)p(x_4)\\ \\
      &={n(x_1)\over n}{n(x_2)\over n}{n(x_3)\over n}{n(x_4)\over n}
  \end{aligned}
$$

- äºŒå…ƒæ¨¡å‹ï¼Œåªä¸å‰ä¸€ä¸ªè¯ç›¸å…³ï¼š$\tau=1$

  $$
  \begin{aligned}
      p(x_1,x_2,x_3,x_4)
      &=p(x_1)p(x_2|x_1)p(x_3|x_2)p(x_4|x_3)\\ \\
      &={n(x_1)\over n}{n(x_1,x_2)\over n(x_1)}{n(x_2,x_3)\over n(x_2)}{n(x_3,x_4)\over n(x_3)}
  \end{aligned}
  $$

- ä¸‰å…ƒæ¨¡å‹ï¼Œåªä¸å‰ä¸¤ä¸ªè¯ç›¸å…³ï¼š $\tau=2$

  $$
  \begin{aligned}
      p(x_1,x_2,x_3,x_4)
      &=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)p(x_4|x_2,x_3)\\ \\
      &={n(x_1)\over n}{n(x_1,x_2)\over n(x_1)}{n(x_1,x_2,x_3)\over n(x_1,x_2)}{n(x_2,x_3,x_4)\over n(x_2,x_3)}
  \end{aligned}
  $$

N å…ƒæ¨¡å‹ç›¸æ¯”ç›´æ¥è®¡æ•°æ¥ç®—è”åˆæ¦‚ç‡çš„æ–¹æ³•ï¼Œä¼˜åŠ¿åœ¨äºå¯ä»¥ç®€ä¾¿è®¡ç®—æ¥è¿‘ä¼¼å¾—åˆ°é•¿åºåˆ—çš„æ¦‚ç‡ã€‚

ä»¥äºŒå…ƒæ¨¡å‹ä¸ºä¾‹ï¼Œè¿‘ä¼¼è®¡ç®—è”åˆæ¦‚ç‡æ—¶ï¼Œæ‰«ä¸€éè¯­æ–™åº“ï¼Œåªéœ€è¦å­˜å‚¨ä¸€ä¸ªäºŒå…ƒè¯ç»„$n(x_i,x_{i-1})$å’Œå•è¯$n(x_i)$å‡ºç°çš„é¢‘æ•°å³å¯è¿‘ä¼¼è®¡ç®—å…¶è”åˆæ¦‚ç‡ã€‚

N å…ƒè¯­æ³•çš„ç©ºé—´å¤æ‚åº¦ä¸ N çš„å–å€¼æ˜¯æŒ‡æ•°å…³ç³»ï¼Œå³$O(|V|^N)$ï¼Œè¿™é‡Œ$|V|$è¡¨ç¤ºä¸€ä¸ªè¯­è¨€çš„è¯æ±‡é‡ã€‚åœ¨è®¡ç®—æœºæ€§èƒ½ä¸é«˜çš„å¹´ä»£ï¼ŒN å–å€¼éƒ½ä¸ä¼šå¾ˆå¤§ï¼Œå¤§å®¶æ›¾ç»å¦‚æœç”¨è¿‡è€å¼è¾“å…¥æ³•å¦‚**æ™ºèƒ½ ABC**ï¼ˆæš´éœ²å¹´é¾„äº†â€¦ğŸ‘´ï¼‰å°±ä¼šç§’æ‡‚ï¼Œåœ¨è¾“å…¥å¥å­æ—¶ï¼Œå¤§å¤šæ•°æƒ…å†µä¹Ÿåªèƒ½ä¸¤ä¸ªå­—ã€ä¸¤ä¸ªå­—åœ°å¾€å¤–è¹¦ï¼Œæ¨æµ‹ç”¨çš„å°±æ˜¯äºŒå…ƒæ¨¡å‹ã€‚

![ABC](Images/zn_abc.jpg)

åæœŸéšç€è®¡ç®—æœºæ€§èƒ½çš„æé«˜ï¼Œè¾“å…¥æ³•ä½¿ç”¨çš„ N æ‰ç¨ç¨å¢å¤§ï¼ˆä½†ä¹Ÿä¸ä¼šå¾ˆå¤§ï¼Œä¸ç„¶ä¸‹è½½ä¸ªè¾“å…¥æ³•åª²ç¾ 3A å¤§ä½œæ¸¸æˆçš„ä½“ç§¯ä½ æ˜¯ä¸ä¼šç­”åº”çš„ ğŸˆ²ï¼‰ï¼Œç°ä»£è¾“å…¥æ³•ä¹Ÿæ–°å¢â€œ**äº‘è¾“å…¥**â€çš„åŠŸèƒ½ï¼Œé€šè¿‡åœ¨æœ¬åœ°è¾“å…¥æ³•è¿è¡Œä¸€ä¸ªå° N æ¨¡å‹ï¼Œåœ¨äº‘ç«¯è¿è¡Œä¸€ä¸ªå¤§ N æ¨¡å‹ï¼Œæ¥è§£å†³è¾“å…¥é•¿éš¾å¥çš„éœ€æ±‚ã€‚

![sougou_cloud](Images/sougou_cloud.jpg)

### æ€»ç»“

- è¯­è¨€æ¨¡å‹ä¼°è®¡æ–‡æœ¬åºåˆ—çš„è”åˆæ¦‚ç‡
- ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ—¶å¸¸ç”¨ N å…ƒæ¨¡å‹ï¼ˆN-gram Modelï¼‰æ¥è¿‘ä¼¼å¾—åˆ°è”åˆæ¦‚ç‡

## ä»£ç å®ç°

- è§‚æµ‹æ•°æ®

```python
import random
import torch
from d2l import torch as d2l

# è¯»å–å¹¶åˆ†è¯è¯­æ–™ï¼Œå½¢æˆtokenäºŒç»´åµŒå¥—åˆ—è¡¨ï¼Œä¸€çº§å…ƒç´ ä¸ºæ¯è¡Œçš„åˆ†è¯åˆ—è¡¨
tokens = d2l.tokenize(d2l.read_time_machine())
# å°†åµŒå¥—åˆ—è¡¨å±•å¹³ä¸ºä¸€ç»´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºæ•´ä¸ªè¯­æ–™åˆ†è¯ç»“æœ
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
# å–å‡ºå‡ºç°é¢‘ç‡å‰10çš„è¯é¢‘ç»“æœ
vocab.token_freqs[:10]
# Out: [('the',2261),(i,1267),('and',1245)â€¦â€¦]

# æå–é¢‘æ•°æ”¾åˆ°åˆ—è¡¨ä¸­
freqs = [freq for token, freq in vocab.token_freqs]
#å•è¯æŒ‰ç´¢å¼•æ ‡å·æ’åºï¼Œä½œä¸ºxè½´
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
         xscale='log', yscale='log')
```

> è¯­è¨€åºåˆ—ä¹Ÿç¬¦åˆäºŒå…«å®šå¾‹ï¼Œå‡ºç°åæ¬¡çš„è¯ä¸è¶…è¿‡äºŒä¸‰ç™¾ï¼Œå‰©ä¸‹çš„åƒä½™è¯å‡ºç°é¢‘ç‡å¾ˆå°‘

![freq](Images/041-02.png)

- äºŒå…ƒæ¨¡å‹è¯é¢‘ç»Ÿè®¡

```python
# è¿™é‡Œæ„é€ æ–¹æ³•å¾ˆå·§å¦™ï¼Œèµ·åˆ°äº†ç±»ä¼¼æ»‘åŠ¨çª—å£çš„æ•ˆæœ
# æŠŠç›¸åŒçš„ä¸¤ä¸ªcorpuså»æ‰æœ€åä¸€ä¸ªå’Œç¬¬ä¸€ä¸ªè¯ï¼Œä½¿å¾—tokensé”™å¼€ä¸€ä½ï¼Œåšä¸¤ä¸¤zip()æ‰“åŒ…
# ç›¸å½“äºæ„å»ºå‡ºå‰åä¸¤ä¸¤åœ¨ä¸€èµ·çš„è¯å¯¹pair
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
# ç»™Vocabåˆ›å»ºä¸€ä¸ªäºŒå…ƒè¯å…¸ï¼ŒVocabæ¥å—ä¸€ä¸ªtokensåˆ†è¯åˆ—è¡¨
bigram_vocab = d2l.Vocab(bigram_tokens)
# å–å‡ºè¯é¢‘å‰10çš„äºŒå…ƒè¯ç»„
bigram_vocab.token_freqs[:10]
# Out:
# [(('of','the'),309),
#  (('in','the'),169),
#  (('i','had'),130),â€¦â€¦]
```

- ä¸‰å…ƒæ¨¡å‹è¯é¢‘ç»Ÿè®¡

```python
# ä»¥ç›¸åŒæ–¹æ³•æ„é€ ä¸‰å…ƒè¯ç»„
trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]
# Out:
# [(('the','time','traveller'),59),
#  (('the','time','machine'),30),
#  (('it','seemed','to'),16),â€¦â€¦]
```

- å¯¹æ¯”ä¸€å…ƒã€äºŒå…ƒã€ä¸‰å…ƒè¯é¢‘ç»Ÿè®¡

```python
bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
#å› ä¸ºè¿”å›çš„æ˜¯å­—å…¸key-valueå…ƒç»„ï¼Œæ‰€ä»¥å¿…é¡»ç”¨ä¸¤ä¸ªå˜é‡å»æ¥
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
```

![plot](Images/041-03.png)

> å¯ä»¥çœ‹å‡ºï¼ŒN å…ƒè¯é¢‘éƒ½æ˜¯å¯¹æ•°å…³ç³»ï¼Œéšç€ï¼Œéšç€ N çš„å¢å¤§ï¼Œè¯é¢‘ä¸‹é™å¾ˆå¿«ï¼Œæ‰€ä»¥å¯¹è¯­æ–™åº“$|V|$çš„å¤§å°è¦æ±‚ä¹Ÿåœ¨å¢å¤§ï¼Œè¯­æ–™åº“å¤ªå°ï¼Œåˆ™å¤§çš„ N-gram æ— æ³•å¾—åˆ°æœ‰æ•ˆè®­ç»ƒã€‚å¦‚æœå¢å¤§è¯­æ–™åº“$|V|$å¤§å°ï¼Œæ ¹æ®ä¸Šæ–‡ä»‹ç»ï¼Œå¤æ‚åº¦åˆæ˜¯$|V|$çš„$N$çš„æŒ‡æ•°ï¼Œæ‰€ä»¥æ ¹æ®ä»¥ä¸Šåˆ†æï¼Œå¤§é‡è¯çš„è¯é¢‘éƒ½æ˜¯æ¯”è¾ƒå°‘çš„ï¼Œå¯ä»¥å°†è¯é¢‘æ•°å°äºæŸä¸ªé˜ˆå€¼çš„è¯åˆ æ‰ï¼Œæ¥é™ä½æ¨¡å‹å¤æ‚åº¦ã€‚

> Stop wordsï¼ˆåœæ­¢è¯ï¼‰
>
> è¿˜æœ‰ä¸€ä¸ªé€”å¾„ï¼ŒNLP ä¸­ä¸ºä¿è¯æ•ˆç‡ï¼Œæ ¹æ®ä¿¡æ¯è®ºï¼Œè¿‡æ»¤æ‰çš„æ‰€å«ä¿¡æ¯é‡å¾ˆä½ï¼ˆå‡ºç°é¢‘ç‡å¾ˆé«˜ï¼‰çš„è¯ï¼Œä¸€èˆ¬æ˜¯æŒ‡æ— æ„ä¹‰çš„å®šå† è¯ï¼Œä¸å®šå† è¯ï¼ˆa,an,theï¼‰, è¿æ¥è¯ï¼ˆof,but...ï¼‰

- å­åºåˆ—é‡‡æ ·

![seq_sample](Images/041-01.png)

- å®šä¹‰éšæœºåºåˆ—ç”Ÿæˆå‡½æ•°

ç”¨ä¸€ç§éšæœºåˆ†å‰²çš„æ–¹å¼å†³å®šèµ·å§‹ä½ç½®ï¼Œéšæœºç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡æ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ä»¥ä¾›è¯»å–ã€‚åœ¨éšæœºé‡‡æ ·ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯åœ¨åŸå§‹çš„é•¿åºåˆ—ä¸Šä»»æ„æ•è·çš„å­åºåˆ—ã€‚

```python
# corpusï¼štokensåˆ—è¡¨ï¼Œnum_stepï¼šsæ—¶é—´å‡è®¾t,æˆ–è€…é©¬å°”å¯å¤«å‡è®¾çš„tau
def seq_data_iter_random(corpus, batch_size, num_steps):
    # ä»index=0~num_step-1åŒºé—´éšæœºæŠ½æ ·ç´¢å¼•ç‚¹ï¼Œå–å‡ºç´¢å¼•ç‚¹åçš„è¯­æ–™
    # ç´¢å¼•ç‚¹å‰é¢çš„æŠ›å¼ƒ
    corpus = corpus[random.randint(0, num_steps - 1):]
    # æ•´é™¤æ¥è®¡ç®—èƒ½å¤Ÿç”Ÿæˆå­åºåˆ—çš„ä»½æ•°æ•°
    num_subseqs = (len(corpus) - 1) // num_steps
    """
    åˆå§‹åŒ–ç´¢å¼•åˆ—è¡¨ï¼Œä»æå¤´çš„corpusçš„ç¬¬ä¸€ä¸ªtokenï¼ˆindex=0ï¼‰å¼€å§‹ï¼Œ
    åˆ°len(corpus) - 1ç»“æŸï¼Œæ­¥é•¿ä¸ºnum_steps=tau
    ä¹Ÿå°±æ˜¯æ¯éš”tauä¸ªå–ä¸€ä¸ªä½œä¸ºç´¢å¼•ï¼Œå…±æœ‰num_subseqsä¸ªç´¢å¼•
    """
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # æ¯æ¬¡æ‰§è¡Œæ­¤å‡½æ•°æ—¶éšæœºæ‰“ä¹±åˆ†å‰²ç´¢å¼•ä¸‹æ ‡
    random.shuffle(initial_indices)

    # è¿”å›ä»posç´¢å¼•å¼€å§‹é•¿åº¦ä¸ºnum_stepsçš„åºåˆ—
    def data(pos):
        return corpus[pos: pos + num_steps]

    #æ¯ä¸ªbatchçš„å­åºåˆ—æ•°é‡
    num_batches = num_subseqs // batch_size

    # éå†æ¯ä¸ªbatch
    for i in range(0, batch_size * num_batches, batch_size):
        # æŠŠæ¯ä¸ªbatché‡Œåˆ†å‰²åºåˆ—çš„ç´¢å¼•ç‚¹ä¸‹æ ‡æ‹¿å‡ºæ¥
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        # æå–åˆ†å‰²ç‚¹çš„è¿ç»­tokenåˆ—è¡¨
        X = [data(j) for j in initial_indices_per_batch]
        # é”™ä¸€ä½tokenæå–åˆ†å‰²ç‚¹çš„è¿ç»­tokenåˆ—è¡¨
        Y = [data(j + 1) for j in initial_indices_per_batch]
        #æ¯æ¬¡è¿”å›ä¸€ä¸ªbatchçš„å­åºåˆ—
        #å¯¹äºä¸€ä¸ªä¸‰å…ƒåºåˆ—
        #ç”¨X[0]é¢„æµ‹Y[0];X[0],X[1]é¢„æµ‹Y[1];X[1],X[2]é¢„æµ‹Y[2];X[2],X[3]é¢„æµ‹Y[3];X[3],X[4]é¢„æµ‹Y[4];
        #Yå°±æ˜¯labels
        yield torch.tensor(X), torch.tensor(Y)
```

- æµ‹è¯•ä»¥ 0 åˆ° 34 çš„åºåˆ—ä¸ºç¤ºä¾‹è¯­æ–™åº“ï¼ŒæŠ½å–`batch_size=2`ï¼Œ`num_steps=5`çš„ batch

```python
my_seq = list(range(35))
# ç°åœ¨0-4é‡Œå–ä¸€ä¸ªæå¤´
# é•¿ä¸º35çš„åºåˆ—ï¼Œå¦‚ä»¥æ­¥é•¿=5å¯åˆ†ä¸º6ä¸ªå­åºåˆ—
# æ‰“ä¹±åˆ†æˆ6/2=3ä¸ªbatchï¼Œæ¯ä¸ªbatchæœ‰ä¸¤ä¸ªå­åºåˆ—
# å†æå–æ¯ä¸ªbatché‡Œçš„æ‰€æœ‰å…ƒç´ 
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
# Out:
# X: tensor([[23,24,25,26,27],
#            [8,9.10,11,12]])
# Y: tensor([[24,25,26,27,28],
#            [9.10,11,12,13]])
# â€¦â€¦
```

- å®šä¹‰å¦ä¸€ç§éšæœºå–æ ·å‡½æ•°ï¼Œä¿è¯æ¯ä¸ªå°æ‰¹é‡çš„ç¬¬ i ä¸ªå…ƒç´ å’Œä¸‹ä¸€ä¸ªå°æ‰¹é‡çš„ç¬¬ i ä¸ªå…ƒç´ åœ¨åŸå§‹æ–‡æœ¬ä¸Šçš„ç›¸é‚»

```python
def seq_data_iter_sequential(corpus, batch_size, num_steps):
    # éšæœºç”Ÿæˆ0~num_stepsä¹‹é—´çš„åç§»ç‚¹
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    #Xå’ŒYç›¸å·®ä¸€ä¸ªtoken
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    # æ±‚å‡ºä¸€ä¸ªbatché‡Œæœ‰å¤šå°‘ä¸ªå­åºåˆ—
    num_batches = Xs.shape[1] // num_steps
    # éå†batch
    for i in range(0, num_steps * num_batches, num_steps):
        #é€šè¿‡å®šä¹‰æ­¥é•¿å¯ä»¥è·³ç€ç´¢å¼•
        #å†³å®šæ¯ä¸ªbatchçš„èµ·å§‹ä½ç½®
        #æŒ‰é¡ºåºå–token
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        #æŠŠreturnæ”¹æˆyield,å°±æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡è¿”å›ä¸€ä¸ªbatchï¼Œæ¯ä¸ªbatchåªæœ‰ä¸€ä¸ªå­åºåˆ—
        yield X, Y
```

- ç”Ÿæˆæ•ˆæœï¼Œæ¯ä¸ª batch é‡Œçš„åºåˆ—å¼€å§‹ä½ç½®éƒ½æ˜¯æ¥ä¸Šä¸ª batch åºåˆ—çš„ç»“æŸ

```python
for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):
    #ç”Ÿæˆå™¨é‡Œçš„for-loopå·²ç»åˆ†å‰²å¥½äº†batchï¼Œè¿™é‡Œçš„for-loopæ˜¯è¿”å›æ‰€æœ‰çš„batch
    print('X: ', X, '\nY:', Y)
# Out:
# X: tensor([[4,5,6,7,8],
#            [19,20.21,22,23]])
# Y: tensor([[9,10,11,12,13],
#            [20.21,22,23,24]])
# â€¦â€¦
```

- å°†ä»¥ä¸Šä¸¤ç§é‡‡æ ·å‡½æ•°åŒ…è£…åˆ°ä¸€ä¸ªç±»ä¸­

```python
class SeqDataLoader:
    """åŠ è½½åºåˆ—æ•°æ®çš„è¿­ä»£å™¨"""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        #__iter__()å¯ä¸€è®©å®ä¾‹æˆä¸ºå¯è¿­ä»£å¯¹è±¡
        #å®šä¹‰__next__()å¯ä»¥è·å¾—å…¶è¿”å›å€¼
        #è¿™é‡Œiterè¿”å›çš„æœ¬èº«å°±æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨self.data_iter_fn()ï¼Œæ‰€ä»¥ä¸éœ€è¦å®šä¹‰next
        #å¯ä»¥ç†è§£ä¸ºfor _ in self.data_iter_fn()
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
        # è¿”å›æ‰¹é‡
```

> è¾“å…¥æ˜¯åŸå§‹çš„ token å’Œæ­¥é•¿ã€æ‰¹é‡ç­‰ï¼Œè¿”å›æ‰¹é‡ï¼Œç±»å°±å¾ˆæ–¹ä¾¿åœ°å°è£…äº†ä¸­é—´çš„åŠŸèƒ½

- å®šä¹‰`load_data_time_machine`å‡½æ•°ï¼Œè¿”å›æ•°æ®è¿­ä»£å™¨å’Œ Vocab

```python
def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):
    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab
```
