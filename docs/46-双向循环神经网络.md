# 46 - åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰
[![Bilibil](https://i2.hdslb.com/bfs/archive/dc2e8e29a1a7cbc3e9f65c94cf08568a13b3c60a.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV12X4y1c71W)
## åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ

æ ¹æ®ä¸Š**ä¸‹**æ–‡ï¼Œé¢„æµ‹æ–‡æœ¬ï¼›æˆ–è€…è¯´æ ¹æ®è¿‡å»å’Œæœªæ¥çš„æ•°æ®åšé¢„æµ‹ã€‚

### åŒå‘RNN

![](\Images/046-01.png)

- ä¸€ä¸ªå‰å‘RNNéšè—å±‚
- ä¸€ä¸ªåå‘RNNéšè—å±‚
- åˆå¹¶ä¸¤ä¸ªéšè—çŠ¶æ€å¾—åˆ°è¾“å‡º
  - å¦‚æœéšè—å±‚sizeä¸º256ï¼Œåˆå¹¶(concat)å°±æ˜¯512

$$\begin{split}
&{\bf H}^{(f)}_{t} = \phi({\bf X}_t{\bf W}_{xh}^{(f)}+{\bf H}_{t-1}^{(f)}{\bf W}_{hh}^{(f)}+{\bf b}_h^{(f)})\\
&{\bf H }_{t}^{(b)} = \phi({\bf X}_t{\bf W}_{xh}^{(b)}+{\bf \vec H}_{t-1}{\bf W}_{hh}^{(b)}+{\bf b}_h^{(b)})\\
&{\bf H}_t=\left[{\bf H}^{(f)}_{t},{\bf H }_t^{(b)}\right]\\
&{\bf O}_t={\bf H }_t{\bf W}_{hq}+{\bf b}_q
\end{split}$$

**æ¨ç†**

- å¯ä»¥åŸºäºå¥å­åšæ¨ç†ï¼ˆåœ¨çœ‹åˆ°æ•´ä¸ªå¥å­çš„æƒ…å†µä¸‹ï¼‰

**æ€»ç»“**

- åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œé€šè¿‡åå‘æ›´æ–°çš„éšè—å±‚æ¥åˆ©ç”¨æ–¹å‘æ—¶é—´ä¿¡æ¯
- é€šå¸¸ç”¨æ¥å¯¹åºåˆ—æŠ½å–ç‰¹å¾ã€å¡«ç©ºï¼Œè€Œä¸æ˜¯é¢„æµ‹æœªæ¥

### ä»£ç å®ç°

```
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps, device = 32, 35, d2l.try_gpu()
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)
# åªåšä¸€ä¸ªæ”¹åŠ¨bidirectional=Trueï¼Œè‡ªåŠ¨åˆ›é€ åå‘éšè—å±‚
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
num_epochs, lr = 500, 1
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
# æ­¤å¤„æ˜¯ä¸€ä¸ªé”™è¯¯ç¤ºèŒƒï¼Œåšé¢„æµ‹ä½†æ²¡æœ‰æœªæ¥ä¿¡æ¯
# æ‰€ä»¥è¾“å‡ºç»“æœæ²¡æœ‰ç†ç”±
```
![](\Images/047-01.png)