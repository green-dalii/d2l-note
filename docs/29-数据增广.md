# 29 - æ•°æ®å¢å¹¿

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‡

[![Bilibil](https://i2.hdslb.com/bfs/archive/31677fa89093f30b98e516884b110b8d983643ca.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV17y4y1g76q)

## æ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰

å¤§å‹æ•°æ®é›†æ˜¯æˆåŠŸåº”ç”¨æ·±åº¦ç¥ç»ç½‘ç»œçš„å…ˆå†³æ¡ä»¶ã€‚ ä»¥å›¾åƒå¢å¹¿ä¸ºä¾‹ï¼Œåœ¨å¯¹è®­ç»ƒå›¾åƒè¿›è¡Œä¸€ç³»åˆ—çš„éšæœºå˜åŒ–ä¹‹åï¼Œç”Ÿæˆç›¸ä¼¼ä½†ä¸åŒçš„è®­ç»ƒæ ·æœ¬ï¼Œä»è€Œæ‰©å¤§äº†è®­ç»ƒé›†çš„è§„æ¨¡ã€‚ æ­¤å¤–ï¼Œåº”ç”¨å›¾åƒå¢å¹¿çš„åŸå› æ˜¯ï¼Œéšæœºæ”¹å˜è®­ç»ƒæ ·æœ¬å¯ä»¥å‡å°‘æ¨¡å‹å¯¹æŸäº›å±æ€§çš„ä¾èµ–ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

åœ¨ä¸€ä¸ªå·²æœ‰æ•°æ®é›†ï¼Œé€šè¿‡æ•°æ®å˜æ¢ï¼Œä½¿å¾—æœ‰æ›´å¤šçš„å¤šæ ·æ€§

- åœ¨è¯­è¨€é‡ŒåŠ å…¥å„ç§ä¸åŒçš„èƒŒæ™¯å™ªéŸ³
- æ”¹å˜å›¾ç‰‡çš„é¢œè‰²ã€å½¢çŠ¶ã€ä½ç½®ã€å˜å½¢ç­‰å¾…å±æ€§

![data_aug](Images/arc.png)

## ä½¿ç”¨å¢å¼ºæ•°æ®è®­ç»ƒ

åœ¨è®­ç»ƒä¸­éšæœºåœ¨çº¿ç”Ÿæˆå¢å¼ºæ•°æ®ï¼Œåœ¨æµ‹è¯•æ—¶ä¸è¿›è¡Œå¢å¼ºæ“ä½œã€‚

- ä¸Šä¸‹ã€å·¦å³ç¿»è½¬(Flip)ï¼š`torchvision.transforms.RandomHorizontalFlip()`

  > ç›¸åº”æ“ä½œè¦ç¬¦åˆè¯­ä¹‰è§£é‡Š

- åˆ‡å‰²ï¼Œä»å›¾ç‰‡ä¸­åˆ‡å‰²ä¸€å—ï¼Œå˜æˆå›ºå®šå½¢çŠ¶ï¼ˆç”± CNN ç‰¹æ€§å†³å®šï¼‰ï¼Œæ‰€ä»¥è¿˜å­˜åœ¨æ‹‰ä¼¸ç­‰(Crop)
  - éšæœºé«˜å®½æ¯”ï¼ˆe.g.[3/4,4/3]ï¼‰
  - éšæœºå¤§å°ï¼ˆe.g.[80%,100%]ï¼‰
  - éšæœºä½ç½®
- é¢œè‰²(Color)
  - è‰²è°ƒ
  - é¥±å’Œåº¦
  - æ˜äº®åº¦

ç±»ä¼¼äºå¯¹å›¾ç‰‡åš PS çš„å˜æ¢

### æ€»ç»“

- æ•°æ®å¢å¹¿é€šè¿‡å˜å½¢æ•°æ®æ¥è·å–å¤šæ ·æ€§ä»è€Œä½¿æ¨¡å‹æ³›åŒ–æ€§èƒ½æ›´å¥½
- å¸¸è§æ–¹æ³•ç¿»è½¬ã€åˆ‡å‰²ã€å˜è‰²ç­‰ç­‰

## ä»£ç å®ç°

```python
%matplotlib inline
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

d2l.set_figsize()
img = d2l.Image.open('./Image/Lions.png')
d2l.plt.imshow(img)

#å®šä¹‰è¾…åŠ©å‡½æ•°ï¼Œå‚æ•°ä¸ºå›¾ç‰‡img,å’Œå¢å¹¿æ–¹æ³•aug
#é»˜è®¤(2, 4)=8å¼ å˜æ¢
def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):
    Y = [aug(img) for _ in range(num_rows * num_cols)]
    d2l.show_images(Y, num_rows, num_cols, scale=scale)
```

- ç¿»è½¬

```python
#æ°´å¹³éšæœºç¿»è½¬
apply(img, torchvision.transforms.RandomHorizontalFlip())
#éšæœºä¸Šä¸‹ç¿»è½¬
apply(img, torchvision.transforms.RandomVerticalFlip())
```

![RandomHorizontalFlip](https://zh.d2l.ai/_images/output_image-augmentation_7d0887_33_0.svg)

- è£å‰ª

```python
#éšæœºå‰ªè£
#size = (200, 200)ï¼Œè£å‡ºæ¥çš„å›¾ç‰‡å¤§å°
#scale=(0.1, 1)ï¼Œè£å¤„åŒºåŸŸé¢ç§¯å åŸå§‹å›¾ç‰‡ç™¾åˆ†æ¯”ï¼Œä»10%åˆ°100%
#ratio=(0.5, 2)ï¼Œé«˜å®½æ¯”ï¼Œ1:2 or 2:1
shape_aug = torchvision.transforms.RandomResizedCrop((200, 200), scale=(0.1, 1), ratio=(0.5, 2))
apply(img, shape_aug)
```

![Crop](https://zh.d2l.ai/_images/output_image-augmentation_7d0887_51_0.svg)

- æ›´æ”¹äº®åº¦`brightness`ã€å¯¹æ¯”åº¦`contrast`ã€é¥±å’Œåº¦`saturation`ã€è‰²è°ƒ`hue`

```python
apply(img, torchvision.transforms.ColorJitter(
    brightness=0.5, contrast=0, saturation=0, hue=0))

apply(img, torchvision.transforms.ColorJitter(
    brightness=0, contrast=0, saturation=0, hue=0.5))
```

![brightness](https://zh.d2l.ai/_images/output_image-augmentation_7d0887_60_0.svg)

![hue](https://zh.d2l.ai/_images/output_image-augmentation_7d0887_69_0.svg)

- å¯ç»“åˆå¤šç§å¢å¹¿æ–¹æ³•

> torchvision.transforms.Compose([aug1, aug2, aug3, aug4])

```python
augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomHorizontalFlip(),
    color_aug, shape_aug])
apply(img, augs)
```

![compose](https://zh.d2l.ai/_images/output_image-augmentation_7d0887_87_0.svg)

- æŒ‡å®šè®­ç»ƒã€æµ‹è¯•æ‰€ç”¨å¢å¹¿æ–¹å¼

```python
train_augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor()
])

test_augs = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor()
])
```

- å®šä¹‰åŠ è½½æ•°æ®é›†å‡½æ•°

```python
def load_cifar10(is_train, augs, batch_size):
    dataset = torchvision.datasets.CIFAR10(
        root='../data', train=is_train,
        transform=augs, download=True)
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=is_train,
        num_workers=4)
    #untils.data.DataLoader()
    #Combines a dataset and a sampler,
    #and provides an iterable over the given dataset.
    #dataset:åŠ è½½çš„æ•°æ®é›†
    #batch_size:æ‰¹é‡å¤§å°
    #shuffleï¼šæ‰“ä¹±
    #augéœ€è¦å¾ˆå¤§çš„è®¡ç®—é‡ï¼Œnum_workerså¯æ ¹æ®CPUæ ¸å¿ƒæ•°è®¾å¤§ä¸€äº›
    return dataloader
```

- å®šä¹‰è®­ç»ƒå‡½æ•°

```python
def train_batch_ch13(net, X, y, loss, trainer, devices):
    if isinstance(X, list):
        X = [x.to(devices[0]) for x in X]
    else:
        X = X.to(devices[0])
    y = y.to(devices[0])
    net.train()
    trainer.zero_grad()
    pred = net(X)
    l = loss(pred, y)
    l.sum().backward()
    trainer.step()
    train_loss_sum = l.sum()
    train_acc_sum = d2l.accuracy(pred, y)
    return train_loss_sum, train_acc_sum

def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
               devices=d2l.try_all_gpus()):
    """ç”¨å¤šGPUè¿›è¡Œæ¨¡å‹è®­ç»ƒ"""
    timer, num_batches = d2l.Timer(), len(train_iter)
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],
                            legend=['train loss', 'train acc', 'test acc'])
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])
    for epoch in range(num_epochs):
        # 4ä¸ªç»´åº¦ï¼šå‚¨å­˜è®­ç»ƒæŸå¤±ï¼Œè®­ç»ƒå‡†ç¡®åº¦ï¼Œå®ä¾‹æ•°ï¼Œç‰¹ç‚¹æ•°
        metric = d2l.Accumulator(4)
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            l, acc = train_batch_ch13(
                net, features, labels, loss, trainer, devices)
            metric.add(l, acc, labels.shape[0], labels.numel())
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[2], metric[1] / metric[3],
                              None))
        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {metric[0] / metric[2]:.3f}, train acc '
          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '
          f'{str(devices)}')
```

- å®šä¹‰ç½‘ç»œã€è®­ç»ƒ

```python
batch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10, 3)

def init_weights(m):
    if type(m) in [nn.Linear, nn.Conv2d]:
        nn.init.xavier_uniform_(m.weight)

net.apply(init_weights)

def train_with_data_aug(train_augs, test_augs, net, lr=0.001):
    train_iter = load_cifar10(True, train_augs, batch_size)
    test_iter = load_cifar10(False, test_augs, batch_size)
    loss = nn.CrossEntropyLoss(reduction="none")
    trainer = torch.optim.Adam(net.parameters(), lr=lr)
    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)

train_with_data_aug(train_augs, test_augs, net)
# Out:
# loss 0.177, train acc 0.939, test acc 0.819
# 5079.7 examples/sec on [device(type='cuda', index=0), device(type='cuda', index=1)]
```

![output_image-augmentation](https://zh.d2l.ai/_images/output_image-augmentation_7d0887_136_1.svg)

## Pytorch æ¨¡å—å‚è€ƒæ–‡æ¡£

- `torchvision.transforms` torchvisonä¸­å›¾ç‰‡å¢å¹¿ç›¸å…³æ–¹æ³• ğŸ§[ä¸­æ–‡](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#containers) | [å®˜æ–¹è‹±æ–‡](https://pytorch.org/vision/stable/transforms.html)