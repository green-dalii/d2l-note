# 27 - æ®‹å·®ç½‘ç»œ ResNet

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‡

[![Bilibil](https://i2.hdslb.com/bfs/archive/300fb344d7e0f1fb18e169c9ed3ecb7af8841143.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1bV41177ap)

## é—®é¢˜å¼•å‡ºï¼šåŠ æ›´å¤šçš„å±‚æ€»æ˜¯æ”¹å–„ç²¾åº¦å—ï¼Ÿ

ä»¥ä¸‹å›¾ç¤ºä¾‹æ¥è¯´ï¼Œå¯¹äºéåµŒå¥—å‡½æ•°ï¼ˆnon-nested functionï¼‰ç±»ï¼Œè¾ƒå¤æ‚çš„å‡½æ•°ç±»å¹¶ä¸æ€»æ˜¯å‘â€œçœŸâ€å‡½æ•° $f^âˆ—$ é æ‹¢ï¼ˆåŒºåŸŸå¤§å°ä»£è¡¨æ¨¡å‹å¤æ‚åº¦ï¼Œå¤æ‚åº¦ç”± $\mathcal{F1}$ å‘ $\mathcal{F6}$ é€’å¢ï¼‰ã€‚ åœ¨ä¸‹å›¾å·¦è¾¹ï¼Œè™½ç„¶ $\mathcal{F3}$ æ¯” $\mathcal{F1}$ æ›´æ¥è¿‘ $f^âˆ—$ ï¼Œä½† $\mathcal{F6}$ å´ç¦»çš„æ›´è¿œäº†ã€‚ ç›¸åå¯¹äºä¸‹å›¾å³ä¾§çš„åµŒå¥—å‡½æ•°ï¼ˆnested functionï¼‰ç±» $\mathcal{F1}\subseteqâ€¦\subseteq \mathcal{F6}$ ï¼Œæˆ‘ä»¬å¯ä»¥é¿å…ä¸Šè¿°é—®é¢˜ã€‚

![functionclasses](https://zh.d2l.ai/_images/functionclasses.svg)

å› æ­¤ï¼Œåªæœ‰**å½“è¾ƒå¤æ‚çš„å‡½æ•°ç±»åŒ…å«è¾ƒå°çš„å‡½æ•°ç±»æ—¶**ï¼Œæˆ‘ä»¬æ‰èƒ½ç¡®ä¿æé«˜å®ƒä»¬çš„æ€§èƒ½ï¼ˆç›¸å½“äºåœ¨åŸæœ‰åŒºåŸŸé€æ¸å¢å¤§è¦†ç›–é¢ç§¯æ¥é€¼è¿‘æœ€ä¼˜è§£ï¼‰ã€‚ å¯¹äºæ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¦‚æœæˆ‘ä»¬èƒ½å°†æ–°æ·»åŠ çš„å±‚è®­ç»ƒæˆ**æ’ç­‰æ˜ å°„**ï¼ˆidentity functionï¼‰$f(x)=x$ï¼Œæ–°æ¨¡å‹å’ŒåŸæ¨¡å‹å°†åŒæ ·æœ‰æ•ˆã€‚ åŒæ—¶ï¼Œç”±äºæ–°æ¨¡å‹å¯èƒ½å¾—å‡ºæ›´ä¼˜çš„è§£æ¥æ‹Ÿåˆè®­ç»ƒæ•°æ®é›†ï¼Œå› æ­¤æ·»åŠ å±‚ä¼¼ä¹æ›´å®¹æ˜“é™ä½è®­ç»ƒè¯¯å·®ã€‚

## æ®‹å·®å—ï¼ˆResidual blocksï¼‰

å‡è®¾æˆ‘ä»¬çš„åŸå§‹è¾“å…¥ä¸º $x$ ï¼Œè€Œå¸Œæœ›å­¦å‡ºçš„ç†æƒ³æ˜ å°„ä¸º $f(x)$ ï¼ˆä½œä¸ºä¸‹å›¾ä¸Šæ–¹æ¿€æ´»å‡½æ•°çš„è¾“å…¥ï¼‰ã€‚ ä¸‹å›¾è™šçº¿æ¡†ä¸­çš„éƒ¨åˆ†éœ€è¦ç›´æ¥æ‹Ÿåˆå‡ºè¯¥æ˜ å°„ $f(x)$ ï¼Œè€Œå³å›¾è™šçº¿æ¡†ä¸­çš„éƒ¨åˆ†åˆ™éœ€è¦æ‹Ÿåˆå‡ºæ®‹å·®æ˜ å°„ $f(x)âˆ’x$ ã€‚ æ®‹å·®æ˜ å°„åœ¨ç°å®ä¸­å¾€å¾€æ›´å®¹æ˜“ä¼˜åŒ–ã€‚ ä»¥æœ¬èŠ‚å¼€å¤´æåˆ°çš„æ’ç­‰æ˜ å°„ä½œä¸ºæˆ‘ä»¬å¸Œæœ›å­¦å‡ºçš„ç†æƒ³æ˜ å°„ $f(x)$ ï¼Œæˆ‘ä»¬åªéœ€å°†ä¸‹å›¾å³ä¾§è™šçº¿æ¡†å†…ä¸Šæ–¹çš„åŠ æƒè¿ç®—ï¼ˆå¦‚ä»¿å°„ï¼‰çš„æƒé‡å’Œåç½®å‚æ•°è®¾æˆ$0$ï¼Œé‚£ä¹ˆ $f(x)$ å³ä¸ºæ’ç­‰æ˜ å°„ã€‚ å®é™…ä¸­ï¼Œå½“ç†æƒ³æ˜ å°„ $f(x)$ ææ¥è¿‘äºæ’ç­‰æ˜ å°„æ—¶ï¼Œæ®‹å·®æ˜ å°„ä¹Ÿæ˜“äº**æ•æ‰æ’ç­‰æ˜ å°„çš„ç»†å¾®æ³¢åŠ¨**ã€‚ å³å›¾æ˜¯ ResNet çš„åŸºç¡€æ¶æ„â€“æ®‹å·®å—ï¼ˆresidual blockï¼‰ã€‚ åœ¨æ®‹å·®å—ä¸­ï¼Œè¾“å…¥å¯é€šè¿‡è·¨å±‚æ•°æ®çº¿è·¯æ›´å¿«åœ°å‘å‰ä¼ æ’­ã€‚

![residual-block](https://zh.d2l.ai/_images/residual-block.svg)

- ä¸²è”ä¸€ä¸ªå±‚æ”¹å˜å‡½æ•°ç±»ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½æ‰©å¤§å‡½æ•°ç±»ã€‚

- æ®‹å·®å—åŠ å…¥å¿«é€Ÿé€šé“ï¼ˆå³è¾¹ï¼‰æ¥å¾—åˆ°$f(x)=x+g(x)$

- ç›¸å½“äºåœ¨åé¢å¤æ‚ç½‘ç»œåµŒå…¥äº†å‰é¢çš„ç®€å•ç½‘ç»œã€‚

### æ®‹å·®å—ç»†èŠ‚

æ®‹å·®å—æœ‰ä¸¤ç§å®ç°æ–¹å¼ï¼Œ ä¸€ç§æ˜¯å½“`use_1x1conv=False`æ—¶ï¼Œåº”ç”¨ ReLU éçº¿æ€§å‡½æ•°ä¹‹å‰ï¼Œå°†è¾“å…¥æ·»åŠ åˆ°è¾“å‡ºã€‚ å¦ä¸€ç§æ˜¯å½“`use_1x1conv=True`æ—¶ï¼Œæ·»åŠ é€šè¿‡ $1Ã—1$ å·ç§¯è°ƒæ•´é€šé“å’Œåˆ†è¾¨ç‡ã€‚

![2-resnet-blocks](https://zh.d2l.ai/_images/resnet-block.svg)

## ResNet ç½‘ç»œç»“æ„

ResNet çš„å‰ä¸¤å±‚è·Ÿä¹‹å‰ä»‹ç»çš„ GoogLeNet ä¸­çš„ä¸€æ ·ï¼š åœ¨è¾“å‡ºé€šé“æ•°ä¸º 64ã€æ­¥å¹…ä¸º 2 çš„ $7Ã—7$ å·ç§¯å±‚åï¼Œæ¥æ­¥å¹…ä¸º 2 çš„ $3Ã—3$ çš„æœ€å¤§æ±‡èšå±‚ã€‚ ä¸åŒä¹‹å¤„åœ¨äº ResNet æ¯ä¸ªå·ç§¯å±‚åå¢åŠ äº†æ‰¹é‡è§„èŒƒåŒ–å±‚ã€‚

![resnet_arch](Images/resnet_arch.png)

æ¯ä¸ªæ¨¡å—æœ‰ 4 ä¸ªå·ç§¯å±‚ï¼ˆä¸åŒ…æ‹¬æ’ç­‰æ˜ å°„çš„ $1Ã—1$ å·ç§¯å±‚ï¼‰ã€‚ åŠ ä¸Šç¬¬ä¸€ä¸ª $7Ã—7$ å·ç§¯å±‚å’Œæœ€åä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œå…±æœ‰ 18 å±‚ã€‚ å› æ­¤ï¼Œè¿™ç§æ¨¡å‹é€šå¸¸è¢«ç§°ä¸º ResNet-18ã€‚ é€šè¿‡é…ç½®ä¸åŒçš„é€šé“æ•°å’Œæ¨¡å—é‡Œçš„æ®‹å·®å—æ•°å¯ä»¥å¾—åˆ°ä¸åŒçš„ ResNet æ¨¡å‹ï¼Œä¾‹å¦‚æ›´æ·±çš„å« 152 å±‚çš„ ResNet-152ã€‚

![resnet18](https://zh.d2l.ai/_images/resnet18.svg)

- é«˜å®½å‡åŠ ResNet å—(stride=2)
- åæ¥å¤šä¸ªé«˜å®½ä¸å˜çš„ ResNet
  - ç”¨ 1x1Conv skip å¯ä»¥æ”¹å˜è¾“å‡ºé€šé“åŒ¹é… ResNet
- ç±»ä¼¼äº VGG å’Œ GooleNet çš„æ€»ä½“æ¶æ„
  - ä¸€èˆ¬æ˜¯ 5 ä¸ª Stage
  - $7Ã—7$ Conv + BN + $3Ã—3$ MaxPooling
  - æ¯ä¸€ä¸ª Stage çš„å…·ä½“æ¡†æ¶å¾ˆçµæ´»
- ä½†æ›¿æ¢æˆäº† ResNet å—

## æ€»ç»“

- æ®‹å·®å—ä½¿å¾—å¾ˆæ·±çš„ç½‘ç»œæ›´åŠ å®¹æ˜“è®­ç»ƒ
  - ç”šè‡³å¯ä»¥è®­ç»ƒä¸€åƒå±‚çš„ç½‘ç»œ
- æ®‹å·®ç½‘ç»œå¯¹éšåçš„æ·±å±‚ç¥ç»ç½‘ç»œè®¾è®¡äº§ç”Ÿäº†æ·±è¿œå½±å“ï¼Œæ— è®ºæ˜¯å·ç§¯ç´¯ç½‘ç»œè¿˜æ˜¯å…¨è¿æ¥ç±»ç½‘ç»œ

## ä»£ç å®ç°

- å®šä¹‰ Residual class

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

class Residual(nn.Module):
    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):
        super().__init__()
        #kernel_size=3, padding=1ï¼Œè¾“å‡ºé«˜å®½ä¸å˜
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)
        #default_stride=1ï¼ŒåŒä¸Šè¾“å‡ºé«˜å®½ä¸å˜
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)
        # å¦‚æœæŒ‡å®šäº†æ—è·¯å·ç§¯
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)
        # self.relu = nn.ReLU(inplace=True)

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
```

- æµ‹è¯•

```python
blk = Residual(3, 3)
X = torch.rand(4, 3, 6, 6)
Y = blk(X)
Y.shape
# Out:
# torch.Size([4, 3, 6, 6])

blk = Residual(3, 6, use_1x1conv=True, strides=2)
blk(X).shape
# Out:
# torch.Size([4, 6, 3, 3])
```

- å®šä¹‰ Residual block

```python
#ç…§æ¬GoogleNet b1
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2),
                   nn.BatchNorm2d(64), nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

def resnet_block(input_channels, num_channels, num_residuals,
                 first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            #ç¬¬ä¸€å—é«˜å®½å‡åŠ
            blk.append(
                Residual(input_channels, num_channels, use_1x1conv=True,
                         strides=2))
        else:
            blk.append(Residual(num_channels, num_channels))
            #é«˜å®½ä¸å˜
    return blk
```

- æ„å»º Stage

```python
b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))
b3 = nn.Sequential(*resnet_block(64, 128, 2))
b4 = nn.Sequential(*resnet_block(128, 256, 2))
b5 = nn.Sequential(*resnet_block(256, 512, 2))

net = nn.Sequential(b1, b2, b3, b4, b5,
                    nn.AdaptiveAvgPool2d((1,1)),
                    nn.Flatten(), nn.Linear(512, 10))
```

- æµ‹è¯•ç½‘ç»œè¾“å‡º

```python
X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
# Out:
# Sequential output shape:     torch.Size([1, 64, 56, 56])
# Sequential output shape:     torch.Size([1, 64, 56, 56])
# Sequential output shape:     torch.Size([1, 128, 28, 28])
# Sequential output shape:     torch.Size([1, 256, 14, 14])
# Sequential output shape:     torch.Size([1, 512, 7, 7])
# AdaptiveAvgPool2d output shape:      torch.Size([1, 512, 1, 1])
# Flatten output shape:        torch.Size([1, 512])
# Linear output shape:         torch.Size([1, 10])
```

- æ¨¡å‹è®­ç»ƒ

```python
lr, num_epochs, batch_size = 0.05, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
# Out:
# loss 0.009, train acc 0.998, test acc 0.922
# 4702.7 examples/sec on cuda:0
```

![output_resnet](https://zh.d2l.ai/_images/output_resnet_46beba_102_1.svg)

> é€šè¿‡ä¸ä¹‹å‰æ¨¡å‹ç»“æœçš„å¯¹æ¯”ï¼Œå¯ä»¥çœ‹å‡º ResNet å¾—ç›Šäºæ®‹å·®è®¾è®¡ï¼Œä½¿å¾—æ¢¯åº¦ä¼ æ’­æ›´å¿«ï¼Œæ¨¡å‹æ”¶æ•›æ›´å¿«ã€è®­ç»ƒç²¾åº¦æ›´é«˜ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹ç‰¹å¾æå–èƒ½åŠ›æ›´å¼ºï¼Œé€Ÿåº¦ä¹Ÿæ›´å¿«ã€‚ï¼ˆæ¯” Alexnet ç¨å¿«ã€æ¯” VGG å¿«å°†è¿‘ 100%ã€æ¯” NiN å¿«å°†è¿‘ 50%ã€æ¯” GoogLeNet å¿«å°†è¿‘ 35%ï¼‰

## ä» ResNet åˆ° DenseNet

**ç¨ å¯†è¿æ¥ç½‘ç»œï¼ˆDenseNetï¼‰** [[Huang et al., 2017]](https://arxiv.org/abs/1608.06993)åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯ ResNet çš„é€»è¾‘æ‰©å±•ã€‚è®©æˆ‘ä»¬å…ˆä»æ•°å­¦ä¸Šäº†è§£å®ƒï¼ŒResNet å°†å‡½æ•°å±•å¼€ä¸º:

$$
f(\mathbf{x}) = \mathbf{x} + g(\mathbf{x})
$$

ä¹Ÿå°±æ˜¯è¯´ï¼ŒResNet å°† $f$ åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†ï¼š

- ä¸€ä¸ªç®€å•çš„çº¿æ€§é¡¹ $\mathbf{x}$
- ä¸€ä¸ªå¤æ‚çš„éçº¿æ€§é¡¹ $g(\mathbf{x})$ã€‚

é‚£ä¹ˆå†å‘å‰æ‹“å±•ä¸€æ­¥ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å°† $f$ æ‹“å±•æˆè¶…è¿‡ä¸¤éƒ¨åˆ†çš„ä¿¡æ¯å‘¢ï¼Ÿ ä¸€ç§æ–¹æ¡ˆä¾¿æ˜¯ DenseNetğŸ‘‡

![densenet-block](https://zh.d2l.ai/_images/densenet-block.svg)

> ResNetï¼ˆå·¦ï¼‰ä¸ DenseNetï¼ˆå³ï¼‰åœ¨è·¨å±‚è¿æ¥ä¸Šçš„ä¸»è¦åŒºåˆ«ï¼šä½¿ç”¨ç›¸åŠ (+)å’Œä½¿ç”¨**è¿ç»“(concat)**

ResNet å’Œ DenseNet çš„å…³é”®åŒºåˆ«åœ¨äºï¼ŒDenseNet è¾“å‡ºæ˜¯**è¿æ¥**ï¼ˆç”¨å›¾ä¸­çš„ [,] è¡¨ç¤ºï¼‰è€Œä¸æ˜¯å¦‚ ResNet çš„ç®€å•ç›¸åŠ ã€‚ å› æ­¤ï¼Œåœ¨åº”ç”¨è¶Šæ¥è¶Šå¤æ‚çš„å‡½æ•°åºåˆ—åï¼Œæˆ‘ä»¬æ‰§è¡Œä» x åˆ°å…¶å±•å¼€å¼çš„æ˜ å°„ï¼š

$$
\mathbf{x} \to \left[
\mathbf{x},
f_1(\mathbf{x}),
f_2([\mathbf{x}, f_1(\mathbf{x})]), f_3([\mathbf{x}, f_1(\mathbf{x}), f_2([\mathbf{x}, f_1(\mathbf{x})])]), \ldots\right]
$$

æœ€åï¼Œå°†è¿™äº›å±•å¼€å¼ç»“åˆåˆ°å¤šå±‚æ„ŸçŸ¥æœºä¸­ï¼Œå†æ¬¡å‡å°‘ç‰¹å¾çš„æ•°é‡ã€‚ å®ç°èµ·æ¥éå¸¸ç®€å•ï¼šæˆ‘ä»¬ä¸éœ€è¦æ·»åŠ æœ¯è¯­ï¼Œè€Œæ˜¯å°†å®ƒä»¬è¿æ¥èµ·æ¥ã€‚ DenseNet è¿™ä¸ªåå­—ç”±å˜é‡ä¹‹é—´çš„â€œç¨ å¯†è¿æ¥â€è€Œå¾—æ¥ï¼Œæœ€åä¸€å±‚ä¸ä¹‹å‰çš„æ‰€æœ‰å±‚ç´§å¯†ç›¸è¿ã€‚ ç¨ å¯†è¿æ¥å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![densenet](https://zh.d2l.ai/_images/densenet.svg)

ç¨ å¯†ç½‘ç»œä¸»è¦ç”± 2 éƒ¨åˆ†æ„æˆï¼š**ç¨ å¯†å—**ï¼ˆdense blockï¼‰å’Œ**è¿‡æ¸¡å±‚**ï¼ˆtransition layerï¼‰ã€‚ å‰è€…å®šä¹‰å¦‚ä½•è¿æ¥è¾“å…¥å’Œè¾“å‡ºï¼Œè€Œåè€…åˆ™æ§åˆ¶é€šé“æ•°é‡ï¼Œä½¿å…¶ä¸ä¼šå¤ªå¤æ‚ã€‚

## ä»£ç å®ç°

- å®šä¹‰å·ç§¯å—

```python
import torch
from torch import nn
from d2l import torch as d2l


def conv_block(input_channels, num_channels):
    return nn.Sequential(
        nn.BatchNorm2d(input_channels), nn.ReLU(),
        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))
```

> ä¸€ä¸ªç¨ å¯†å—ç”±å¤šä¸ªå·ç§¯å—ç»„æˆï¼Œæ¯ä¸ªå·ç§¯å—ä½¿ç”¨ç›¸åŒæ•°é‡çš„è¾“å‡ºé€šé“ã€‚åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå·ç§¯å—çš„è¾“å…¥å’Œè¾“å‡ºåœ¨é€šé“ç»´ä¸Šè¿ç»“ï¼ˆconcatï¼‰

- å®šä¹‰ dense block

```python
class DenseBlock(nn.Module):
    def __init__(self, num_convs, input_channels, num_channels):
        super(DenseBlock, self).__init__()
        layer = []
        for i in range(num_convs):
            layer.append(conv_block(
                num_channels * i + input_channels, num_channels))
        self.net = nn.Sequential(*layer)

    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            # è¿æ¥é€šé“ç»´åº¦ä¸Šæ¯ä¸ªå—çš„è¾“å…¥å’Œè¾“å‡º
            X = torch.cat((X, Y), dim=1)
        return X
```

> æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæœ‰ 2 ä¸ªè¾“å‡ºé€šé“æ•°ä¸º 10 çš„ DenseBlockã€‚ ä½¿ç”¨é€šé“æ•°ä¸º 3 çš„è¾“å…¥æ—¶ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°é€šé“æ•°ä¸º $3+2Ã—10=23$ çš„è¾“å‡ºã€‚ å·ç§¯å—çš„é€šé“æ•°æ§åˆ¶äº†è¾“å‡ºé€šé“æ•°ç›¸å¯¹äºè¾“å…¥é€šé“æ•°çš„å¢é•¿ï¼Œå› æ­¤ä¹Ÿè¢«ç§°ä¸ºå¢é•¿ç‡ï¼ˆgrowth rateï¼‰ã€‚

```python
blk = DenseBlock(2, 3, 10)
X = torch.randn(4, 3, 8, 8)
Y = blk(X)
Y.shape
# Out:
# torch.Size([4, 23, 8, 8])
```

> ç”±äºæ¯ä¸ªç¨ å¯†å—éƒ½ä¼šå¸¦æ¥é€šé“æ•°çš„å¢åŠ ï¼Œä½¿ç”¨è¿‡å¤šåˆ™ä¼šè¿‡äºå¤æ‚åŒ–æ¨¡å‹ã€‚ è€Œè¿‡æ¸¡å±‚å¯ä»¥ç”¨æ¥æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ã€‚ å®ƒé€šè¿‡ $1Ã—1$ å·ç§¯å±‚æ¥å‡å°é€šé“æ•°ï¼Œå¹¶ä½¿ç”¨æ­¥å¹… stride=2 çš„å¹³å‡æ±‡èšå±‚å‡åŠé«˜å’Œå®½ï¼Œä»è€Œè¿›ä¸€æ­¥é™ä½æ¨¡å‹å¤æ‚åº¦ã€‚

- å®šä¹‰è¿‡æ¸¡å±‚

```python
def transition_block(input_channels, num_channels):
    return nn.Sequential(
        nn.BatchNorm2d(input_channels), nn.ReLU(),
        nn.Conv2d(input_channels, num_channels, kernel_size=1),
        nn.AvgPool2d(kernel_size=2, stride=2))
```

> å¯¹ä¸Šä¸€ä¸ªä¾‹å­ä¸­ç¨ å¯†å—çš„è¾“å‡ºä½¿ç”¨é€šé“æ•°ä¸º 10 çš„è¿‡æ¸¡å±‚ã€‚ æ­¤æ—¶è¾“å‡ºçš„é€šé“æ•°å‡ä¸º 10ï¼Œé«˜å’Œå®½å‡å‡åŠã€‚

```python
blk = transition_block(23, 10)
blk(Y).shape
# Out:
# torch.Size([4, 10, 4, 4])
```

- æ„é€  DenseNet

```python
b1 = nn.Sequential(
    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
    nn.BatchNorm2d(64), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

# num_channelsä¸ºå½“å‰çš„é€šé“æ•°
num_channels, growth_rate = 64, 32
num_convs_in_dense_blocks = [4, 4, 4, 4]
blks = []
for i, num_convs in enumerate(num_convs_in_dense_blocks):
    blks.append(DenseBlock(num_convs, num_channels, growth_rate))
    # ä¸Šä¸€ä¸ªç¨ å¯†å—çš„è¾“å‡ºé€šé“æ•°
    num_channels += num_convs * growth_rate
    # åœ¨ç¨ å¯†å—ä¹‹é—´æ·»åŠ ä¸€ä¸ªè½¬æ¢å±‚ï¼Œä½¿é€šé“æ•°é‡å‡åŠ
    if i != len(num_convs_in_dense_blocks) - 1:
        blks.append(transition_block(num_channels, num_channels // 2))
        num_channels = num_channels // 2

net = nn.Sequential(
    b1, *blks,
    nn.BatchNorm2d(num_channels), nn.ReLU(),
    nn.AdaptiveMaxPool2d((1, 1)),
    nn.Flatten(),
    nn.Linear(num_channels, 10))
```

- è®­ç»ƒæ¨¡å‹

```python
lr, num_epochs, batch_size = 0.1, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
# Out:
# loss 0.151, train acc 0.944, test acc 0.822
# 5507.0 examples/sec on cuda:0
```

![output_densenet](https://zh.d2l.ai/_images/output_densenet_e82156_102_1.svg)

---

## Q&AğŸ¤“

**Qï¼šä¸ºä»€ä¹ˆ$f(x)=x+g(x)$å°±èƒ½ä¿è¯ç»“æœè‡³å°‘ä¸ä¼šå˜å·®ï¼Ÿå‡å¦‚$g(x)$å˜å¾—æ›´å·®å‘¢ï¼Ÿ**

**ğŸ™‹â€â™‚ï¸**ï¼šåœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ï¼Œå¦‚æœåå‘ä¼ æ’­æ—¶ç®—æ³•å‘ç°$g(x)$å¯¹æ¨¡å‹`loss`æŸå¤±å‡½æ•°æ²¡æœ‰è´¡çŒ®ï¼ˆæˆ–è€…æœ‰è´Ÿè´¡çŒ®ï¼‰ï¼Œå°±ä¼šé€æ¸å°†$g(x)$çš„æ¢¯åº¦ç½®é›¶ï¼ˆæˆ–è€…åæ–¹å‘é™ä½$g(x)$çš„å½±å“ç›´åˆ°æƒé‡ä¸ºé›¶ï¼‰ï¼Œæœ€å$g(x)$å°±å¾—ä¸åˆ°æ¢¯åº¦æ›´æ–°ï¼Œç½‘ç»œæœ€ç»ˆç»“æœ$f(x)$ä¹Ÿä¼šå¿½ç•¥$g(x)$è€Œå‘$x$é è¿‘ã€‚

**Qï¼šæ˜¯ä¸æ˜¯è®­ç»ƒç²¾åº¦æ€»æ˜¯ä¼šæ¯”æµ‹è¯•ç²¾åº¦é«˜ï¼Ÿ**

**ğŸ™‹â€â™‚ï¸**ï¼šä¹Ÿä¸ä¸€å®šï¼Œåœ¨è®¸å¤šæœ‰ Data Argumentï¼ˆæ•°æ®å¢å¼ºï¼‰çš„ä»»åŠ¡ä¸­ï¼Œæ¯”å¦‚å›¾ç‰‡è¯†åˆ«ï¼Œæµ‹è¯•ç²¾åº¦æ˜¯å¯èƒ½é«˜äºè®­ç»ƒç²¾åº¦ï¼ˆå› ä¸ºè®­ç»ƒå›¾ç‰‡æœ‰æ·»åŠ å™ªå£°ç­‰å¹²æ‰°ï¼Œè€Œæµ‹è¯•å›¾ç‰‡æ²¡æœ‰ï¼‰ã€‚
