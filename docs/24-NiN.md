# 24 - ç½‘ç»œä¸­çš„ç½‘ç»œ NiN

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‡

[![Bilibil](https://i2.hdslb.com/bfs/archive/d20ed4243d80ed7d2c047bc421254b5ff2797c8e.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1Uv411G71b)

## å…¨è¿æ¥å±‚çš„é—®é¢˜

LeNetã€AlexNet å’Œ VGG éƒ½æœ‰ä¸€ä¸ªå…±åŒçš„è®¾è®¡æ¨¡å¼ï¼šé€šè¿‡ä¸€ç³»åˆ—çš„å·ç§¯å±‚ä¸æ± åŒ–å±‚æ¥æå–ç©ºé—´ç»“æ„ç‰¹å¾ï¼›ç„¶åé€šè¿‡å…¨è¿æ¥å±‚å¯¹ç‰¹å¾çš„è¡¨å¾è¿›è¡Œå¤„ç†ã€‚ AlexNet å’Œ VGG å¯¹ LeNet çš„æ”¹è¿›ä¸»è¦åœ¨äºå¦‚ä½•æ‰©å¤§å’ŒåŠ æ·±è¿™ä¸¤ä¸ªæ¨¡å—ã€‚è€Œå…¨è¿æ¥å±‚å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

1. ç›¸æ¯”å·ç§¯å±‚ï¼Œå…¨è¿æ¥å±‚**å‚æ•°å­˜å‚¨ç©ºé—´å¤§å¾—å¤š**ï¼ŒåŒæ—¶å ç”¨å¾ˆå¤§çš„è®¡ç®—å¸¦å®½

   - å·ç§¯å±‚éœ€è¦è¾ƒå°‘çš„å‚æ•°ï¼š$c_i\times c_o\times k^2$

   > $c_i$è¡¨ç¤ºè¾“å…¥é€šé“æ•°ï¼Œ$c_o$è¡¨ç¤ºè¾“å‡ºé€šé“æ•°ï¼Œ$k$è¡¨ç¤ºå·ç§¯æ ¸å°ºå¯¸ï¼Œ

   - ä»¥å·ç§¯å±‚åçš„ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„å‚æ•°å¤§å°ä¸ºä¾‹
     - LeNetï¼š$16\times5\times5\times120=48k$
     - AlexNetï¼š$256\times5\times5\times4096=26M$
     - VGGï¼š$512\times7\times7\times4096=102M$

2. å¤§å°ºå¯¸çš„å…¨è¿æ¥å±‚å¾ˆå®¹æ˜“å¼•èµ·**è¿‡æ‹Ÿåˆ**é—®é¢˜
3. ä½¿ç”¨äº†å…¨è¿æ¥å±‚ï¼Œç›¸å½“äºæ”¾å¼ƒç‰¹å¾çš„**ç©ºé—´ç»“æ„**

## NiN

ç½‘ç»œä¸­çš„ç½‘ç»œï¼ˆNiNï¼‰æä¾›äº†ä¸€ä¸ªéå¸¸ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼šåœ¨æ¯ä¸ªåƒç´ çš„é€šé“ä¸Šåˆ†åˆ«ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºæ¥å–ä»£å…¨è¿æ¥å±‚

### NiN å—

```mermaid
graph BT
  subgraph NiN Block
    A[Convolution] --> B[1x1 Convolution]
    B --> C[1x1 Convolution]
  end
```

ä¸€ä¸ªå·ç§¯å±‚åè·Ÿä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼ˆå³$(1 \times 1)$å·ç§¯å±‚ï¼Œç”¨äºæ··åˆé€šé“ï¼Œå¯å‚è€ƒ[å·ç§¯å±‚é€šé“](19-å·ç§¯å±‚é€šé“.md)ä¸€èŠ‚å†…å®¹ï¼‰ï¼Œæ¯ä¸ª$(1 \times 1)$å·ç§¯å±‚ï¼š

- æ­¥å¹… stride=1ï¼Œæ— å¡«å……ï¼Œé€šé“æ•°ç­‰äºå·ç§¯å±‚é€šé“æ•°ï¼Œè¾“å‡ºå½¢çŠ¶è·Ÿå·ç§¯å±‚è¾“å‡ºä¸€æ ·ï¼Œä¸æ”¹å˜è¾“å‡ºå°ºå¯¸ä¸é€šé“æ•°
- èµ·åˆ°å…¨è¿æ¥å±‚çš„ä½œç”¨
- å¯¹æ¯ä¸ªåƒç´ å¢åŠ äº†éçº¿æ€§æ€§

![conv-1x1](https://zh.d2l.ai/_images/conv-1x1.svg)

> ä¸Šå›¾ä¸º$(1 \times 1)$å·ç§¯å±‚è€ç¤ºä¾‹å›¾ä»…ä¾›å‚è€ƒã€‚å¯¹åº” NiN å—ï¼Œè¾“å…¥ä¸‰é€šé“æ—¶ï¼Œåˆ™å·ç§¯å±‚ä¹Ÿéœ€è¦è¾“å‡º 3 é€šé“è€Œä¸æ˜¯ä¸Šå›¾çš„çš„ 2 é€šé“

### NiN æ¶æ„

![nin](https://zh.d2l.ai/_images/nin.svg)

- æ— å…¨è¿æ¥å±‚
- äº¤æ›¿ä½¿ç”¨ **NiN å—**å’Œæ­¥å¹…ä¸º 2 çš„**æœ€å¤§æ± åŒ–å±‚**
  - é€æ­¥å‡å°é«˜å®½å’Œå¢å¤§é€šé“æ•°
- æœ€åä½¿ç”¨**å…¨å±€å¹³å‡æ± åŒ–å±‚**å¾—åˆ°è¾“å‡ºæ›¿ä»£ AlexNetã€VGG çš„å…¨è¿æ¥å±‚
  - å…¶è¾“å…¥é€šé“æ•°æ˜¯ç±»åˆ«æ•°
  - ä»æ¯ä¸ªé€šé“æ‹¿å‡ºä¸€ä¸ªå€¼ï¼Œä½œä¸ºå¯¹å…¶ç±»æ¯”çš„é¢„æµ‹ï¼Œå†æ±‚ softmax
  - å‡å°å…¨è¿æ¥å±‚è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå‡å°‘å‚æ•°ä¸ªæ•°ï¼Œé™ä½å­˜å‚¨ç©ºé—´ä½¿ç”¨

## ä»£ç å®ç°

- å®ç° NiN Block

```python
def nin_block(in_channels, out_channels, kernel_size,
             strides, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1),nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1),nn.ReLU())
```

- å®šä¹‰ NiN ç½‘ç»œï¼ˆä»¥ AlexNet ä¸ºæ¨¡æ¿ï¼‰

```python
net = nn.Sequential(
    nin_block(1, 96, kernel_size=11, strides=4, padding=0),
    nn.MaxPool2d(3, stride=2),
    nin_block(96, 256, kernel_size=5, strides=1, padding=2),
    nn.MaxPool2d(3, stride=2),
    nin_block(256, 384, kernel_size=3, strides=1, padding=1),
    nn.MaxPool2d(3, stride=2), nn.Dropout(0.5),
    # æœ€åå°†è¾“å‡ºé€šé“é™åˆ°10
    nin_block(384, 10, kernel_size=3, strides=1, padding=1),
    # nn.AdaptiveAvgPool2dä½¿å¾—è¾“å‡ºå°ºå¯¸ä¸º(1x10x1x1)
    # ç›¸å½“äºåœ¨æ¯ä¸ªé€šé“å†…åšå¹³å‡æ± åŒ–
    nn.AdaptiveAvgPool2d((1, 1)),
    nn.Flatten())
```

- æµ‹è¯•å„å±‚è¾“å‡ºå°ºå¯¸

```python
X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__, 'output shape\t', X.shape)

# Out:
# Sequential output shape:     torch.Size([1, 96, 54, 54])
# MaxPool2d output shape:      torch.Size([1, 96, 26, 26])
# Sequential output shape:     torch.Size([1, 256, 26, 26])
# MaxPool2d output shape:      torch.Size([1, 256, 12, 12])
# Sequential output shape:     torch.Size([1, 384, 12, 12])
# MaxPool2d output shape:      torch.Size([1, 384, 5, 5])
# Sequential output shape:     torch.Size([1, 10, 5, 5])
# AdaptiveAvgPool2d output shape:      torch.Size([1, 10, 1, 1])
# Flatten output shape:        torch.Size([1, 10])
```

- è®­ç»ƒ

```python
lr, num_epochs, batch_size = 0.1, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())

# Out:
# loss 0.563, train acc 0.783, test acc 0.772
# 3192.9 examples/sec on cuda:0
```

> æ ¹æ®ç»“æœå¯¹æ¯” AlexNet å¯çŸ¥ï¼Œç”±äºæœ‰å¤šä¸ª 1x1 å·ç§¯å­˜åœ¨ï¼Œè®­ç»ƒé€Ÿåº¦è¦æ¯” AlexNet æ…¢ï¼Œè€Œä¸”ç²¾åº¦æ²¡æœ‰åè€…é«˜ï¼ˆå¯èƒ½æ•°æ®é›†è¿‡å°ï¼‰

![output_nin](https://zh.d2l.ai/_images/output_nin_8ad4f3_42_1.svg)

## Pytorch æ¨¡å—å‚è€ƒæ–‡æ¡£

- `torch.nn.AdaptiveAvgPool2d(output_size)` Pytorch 2 ç»´çš„è‡ªé€‚åº”æœ€å¤§æ± åŒ–æ“ä½œ ğŸ§[ä¸­æ–‡](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#_2) | [å®˜æ–¹è‹±æ–‡](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d)

---

## Q&AğŸ¤“

**Qï¼šä¸ºä»€ä¹ˆæœ€è¿‘å‡ ä¸ªæ¨¡å‹åšåˆ†ç±»é—®é¢˜ï¼Œåœ¨ç½‘ç»œå®šä¹‰ä¸­éƒ½æ²¡çœ‹åˆ° Softmax å±‚ï¼Ÿ**

**ğŸ™‹â€â™‚ï¸**ï¼šä¸€èˆ¬ç°æœ‰æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸ºé¿å…åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å¯èƒ½ä¼šå›°æ‰°æˆ‘ä»¬çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼Œå¤§éƒ½å°† softmax å’Œäº¤å‰ç†µæŸå¤±å‡½æ•°ç»“åˆåœ¨ä¸€èµ·ã€‚è¯¦ç»†å†…å®¹å¯å‚è€ƒ ğŸ‘‰[è¿™é‡Œ](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-concise.html#subsec-softmax-implementation-revisited)
