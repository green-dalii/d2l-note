# ç•ªå¤–04-Kaggleç«žèµ›å®žè·µç»éªŒ

## ç«žèµ›1ï¼šCalifornia-House-Price

### æ•°æ®è½½å…¥å¯ä½¿ç”¨Kaggle APIè¿›è¡Œä¸‹è½½

å°¤å…¶é€‚åˆåœ¨äº‘è®¡ç®—æˆ–å…¶ä»–æ— GUIç•Œé¢çš„Linuxä¸»æœºä¸Šä½¿ç”¨ã€‚

```bash
# å®‰è£…
pip install kaggle

# ä¸‹è½½æŒ‡å®šæ•°æ®é›†
kaggle competitions download -c california-house-prices

# è§£åŽ‹
unzip california-house-prices-xxx.zip && rm california-house-prices-xxx.zip
```

### Kaggleæ¯ä¸ªé¡¹ç›®çš„æ•°æ®æ¦‚è§ˆé¡µé¢è‡³å°‘è¦çž¥ä¸€çœ¼

Kaggleçš„æ¯ä¸ªé¡¹ç›®ä¸»é¡µï¼Œéƒ½è®¾æœ‰æ•°æ®çš„æ¦‚è§ˆé¡µé¢ï¼Œç‚¹å‡»`Data`ï¼Œå¯çœ‹åˆ°æ¯ä¸ªç‰¹å¾çš„**æ•°æ®åˆ†å¸ƒ**ã€**æ•°æ®ç±»åž‹**ã€**ç®€è¦ç»Ÿè®¡**ç­‰ç­‰ï¼Œååˆ†æ–¹ä¾¿å¯¹æ•°æ®æ•´ä½“è¿›è¡Œä¸€ä¸ªåˆæ­¥çš„æŠŠæ¡

![kaggle_data](../Images/kaggle_data.jpg)

### æŽ¢ç´¢æ€§æ•°æ®åˆ†æžï¼ˆEDAï¼‰

æ‹¿åˆ°æ•°æ®åŽï¼Œç¬¬ä¸€æ­¥å¯ä»¥å…ˆè¿›è¡ŒEDAï¼Œä½¿ç”¨Pandasã€Seabornã€PlotlyåŒ…å¯ä»¥é«˜æ•ˆåœ°å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå¤„ç†ã€‚ä»¥ä¸‹ä¸ºå¸¸ç”¨å‘½ä»¤ï¼š

```python
# å¯¼å…¥ç›¸å…³åŒ…
import seaborn as sns 
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# è¯»å–CSVæ ¼å¼æ–‡ä»¶å¹¶è½¬æ¢ä¸ºDataframeæ ¼å¼
data = pd.read_csv('train_data.csv')

# å±•ç¤ºæ•°æ®å½¢çŠ¶
data.shape

# å±•ç¤ºå„åˆ—æ•°æ®ç±»åž‹
data.info()

# å±•ç¤ºæ•°å€¼åˆ—ç®€è¦ç»Ÿè®¡ä¿¡æ¯
data.describe()

# å–æ ·
data.sample()

# å–å‡ºå‰nè¡Œã€åŽnè¡Œ
data.head(n)
data.tail(n)

# å–å‡ºæŒ‡å®šç´¢å¼•åœ°è¡Œæˆ–åˆ—ï¼ˆå¯ç”¨åˆ‡ç‰‡ï¼‰
data.iloc[:,[1:3]]
```

- å¯¹äºŽæ•°å€¼åˆ—

```python
# å¯¹æŸåˆ—è¿›è¡Œæ•°æ®æ˜ å°„
data['gender'] = data['gender'].map({"male":0,"femal":1})

# å¯¹æŸåˆ—è¿›è¡Œæ•°æ®å¤„ç†æ“ä½œ
data['Bedrooms'] = data['Bedrooms'].map(lambda x: float(x))

# å¦‚æžœå¤„ç†å‡½æ•°æ¯”è¾ƒå¤æ‚ï¼Œæœ‰ä¸æ­¢ä¸€ä¸ªå‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨`apply()`
def apply_age(x,bias):
    return x+bias
#ä»¥å…ƒç»„çš„æ–¹å¼ä¼ å…¥é¢å¤–çš„å‚æ•°
data["age"] = data["age"].apply(apply_age,args=(-3,))
# å¤šåˆ—è®¡ç®—
data['col3'] = df.apply(lambda x: x['col1'] + 2 * x['col2'], axis=1) 

# æŒ‰è¡Œæ¨ªå‘apply
def BMI(series):
    weight = series["weight"]
    height = series["height"]/100
    BMI = weight/height**2
    return BMI
data["BMI"] = data.apply(BMI,axis=1)

# å¾…è¡¥å……`.filter() .groupby() .agg . `

# è®¡ç®—å„æ•°å€¼åˆ—åœ°ç›¸å…³ç³»æ•°ï¼Œå¯é€‰{'pearson', 'kendall', 'spearman'}
data_corr = data.corr(method='pearson')
# åšå‡ºç›´è§‚çš„ç›¸å…³ç³»æ•°çƒ­åŠ›å›¾
plt.subplots(figsize=(20,20))
mask = np.zeros_like(data_corr,dtype=np.bool)
mask[np.triu_indices_from(mask)] = True # é®æŽ‰é‡å¤çš„ä¸ŠåŠå›¾
sns.heatmap(data_corr,mask=mask,annot=True) # annotæ˜¾ç¤ºæ•°å€¼
plt.show()

# ä½¿ç”¨pairplotå±•çŽ°ä¸¤ä¸¤å˜é‡åˆ—çš„å…³ç³»å’Œæ•°æ®åˆ†å¸ƒ
# ä¼ å…¥æ•°å€¼åˆ—æ•°æ®ï¼Œ'corner'ï¼šæ˜¯å¦å±•ç¤ºåŠè§’ï¼›
# 'kind'={"scatter","reg"}éžå¯¹è§’çº¿å­å›¾ç”¨æ•£ç‚¹æˆ–å›žå½’æ‹Ÿåˆ:
# 'diag_kind'={"hist","kde"}å¯¹è§’çº¿å­å›¾ä½¿ç”¨ç›´æ–¹å›¾æˆ–æ ¸å¯†åº¦å‡½æ•°
sns.pairplot(data,vars=var,corner=True, diag_kind='hist',kind='reg',dropna=True)
```

- å¯¹äºŽç¦»æ•£ç±»åˆ«åˆ—

```python
# è®¡ç®—æ‰€åŒ…å«å„ç±»åž‹æ•°é‡
data['Type'].value_counts()

# ç­›é€‰å‡ºå±žäºŽæŸé›†åˆçš„è¡Œæ•°æ®
data[data['Type'].isin(['apple','pear'])]

# å¯¹ç±»åˆ«åˆ—åšå‡ºç®±å›¾ã€å°æç´å›¾
sns.catplot(y="Type",x="Sold Price",kind="violin",data=type_df,height=5,aspect=3)
```

### æ•°æ®é¢„å¤„ç†

ä¸€èˆ¬æ•°æ®åŒ…å«çš„ç±»åž‹æœ‰è¿žç»­åž‹æ•°å€¼ï¼ˆå¦‚ä»·æ ¼ã€æ¸©åº¦ï¼‰ã€ç¦»æ•£åž‹æ•°å€¼ï¼ˆå¦‚å¹´ä»½ã€é‚®ç¼–ï¼‰ã€æè¿°åž‹çŸ­å­—ç¬¦ä¸²ï¼ˆå¦‚æ€§åˆ«ã€åœ°åŒºï¼‰ã€å™è¿°åž‹é•¿æ–‡æœ¬ï¼ˆå¦‚æ¦‚è¿°ã€æ–°é—»ã€å¯¹è¯ï¼‰ï¼Œé¢„å¤„ç†æ—¶è¦åŒºåˆ«å¯¹å¾…ã€‚

#### åŒºåˆ†æ•°å€¼ç±»å’Œéžæ•°å€¼ç±»ï¼Œå¹¶å¯¹æ•°å€¼ç±»è¿›è¡Œå½’ä¸€åŒ–

```python
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index
all_features[numeric_features] = all_features[numeric_features].apply(
    lambda x: (x - x.mean()) / (x.std()))
all_features[numeric_features] = all_features[numeric_features].fillna(0)
```

#### å¯¹äºŽç¦»æ•£åž‹æ•°å€¼ã€æè¿°åž‹çŸ­å­—ç¬¦ï¼Œå¦‚æžœå„åˆ—çš„ç§ç±»ä¸å¤šï¼Œå¯é‡‡ç”¨ç‹¬çƒ­ç¼–ç ï¼ˆä½†å¦‚æžœç§ç±»å¾ˆå¤šï¼Œåˆ™ä¸èƒ½ç›²ç›®ä½¿ç”¨ï¼Œå¦åˆ™ä¼šæ’‘çˆ†å†…å­˜ï¼‰

```python
no_numeric_features = all_features.dtypes[all_features.dtypes == 'object'].index

all_features[no_numeric_features] = pd.get_dummies(all_features[no_numeric_features], dummy_na=True)
```

#### å¯¹äºŽç§ç±»å¤šã€ä¸èƒ½é‡‡ç”¨ç‹¬çƒ­ç¼–ç çš„æ•°æ®ï¼Œå¯ä»¥é‡‡ç”¨`scikit-learn.preprocessing`çš„ä¸€ç³»åˆ—ç‰¹å¾å·¥ç¨‹çš„æ–¹æ³•ã€‚å‚è€ƒæ–‡æ¡£ðŸ‘‰[API](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)

### ç¥žç»ç½‘ç»œç›¸å…³

#### ç”±æµ…å…¥æ·±ï¼Œå¥¥å¡å§†å®šç†

æœ€å¼€å§‹çš„ç½‘ç»œæ¨¡åž‹å¯ä»¥å…ˆä»Žæœ€ç®€å•çš„å•å±‚æˆ–æµ…å±‚çº¿æ€§æ¨¡åž‹é€æ­¥è¿‡æ¸¡åˆ°å¤æ‚ç½‘ç»œï¼Œæ²¡å¿…è¦ä¸€å¼€å§‹å°±ä½¿ç”¨é«˜é˜¶ç¥žç»ç½‘ç»œã€‚

é¦–å…ˆå°è¯•æµ…å±‚ç½‘ç»œèƒ½å¦workï¼Ÿworkåˆ°ä»€ä¹ˆç¨‹åº¦ï¼Ÿæ ¹æ®ç»“æžœå†é€æ­¥è¿›è¡Œæ”¹è¿›ã€‚

#### ä¸èƒ½ç›´æŽ¥å°†BNå±‚ä¸ŽDropoutå±‚æ··ç”¨

BatchNormå’ŒDropoutä¸¤ç§æ–¹æ³•ä¸èƒ½æ··ç”¨ï¼Œå®žé™…ä½¿ç”¨ä¸­å‘çŽ°ä¼šå¯¼è‡´ç²¾åº¦ä¸‹é™ä¸”è®­ç»ƒä¸ç¨³å®šçš„çŽ°è±¡ã€‚

è®ºæ–‡å‚è€ƒðŸ‘‰[Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift](https://arxiv.org/abs/1801.05134)

#### è®°å¾—åœ¨æ¨¡åž‹é¢„æµ‹æ—¶å†™ä¸Š`net.eval()`å¼€å¯è¯„ä¼°æ¨¡å¼ï¼Œä»¥å…å‰åŠŸå°½å¼ƒ

#### Adamä¼˜åŒ–å™¨ä¸èƒ½ä¸ŽWeight Deceyä¸€èµ·ä½¿ç”¨ï¼Œå¯ä»¥ç”¨AdamWä¼˜åŒ–å™¨æ›¿ä»£

åœ¨Adamä¼˜åŒ–å™¨ä¸­ï¼Œweight decayä¸ŽL2æ­£åˆ™å¹¶ä¸ç­‰ä»·ï¼Œé™¤æ­¤ä¹‹å¤–ï¼ŒAdam+L2çš„æ–¹æ¡ˆä¼šå¯¼è‡´ä¸ç†æƒ³çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚è®ºæ–‡ã€ŠDecoupled Weight Decay Regularizationã€‹æŒ‡å‡ºäº†è¿™ä¸€ç‚¹

> Just adding the square of the weights to the loss function is *not* the correct way of using L2 regularization/weight decay with Adam, since that will interact with the m and v parameters in strange ways.