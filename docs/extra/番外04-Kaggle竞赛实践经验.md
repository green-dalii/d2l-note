# ç•ªå¤– 04-Kaggle ç«èµ›å®è·µç»éªŒ

## ç«èµ› 1ï¼šCalifornia-House-Price

### æ•°æ®è½½å…¥å¯ä½¿ç”¨ Kaggle API è¿›è¡Œä¸‹è½½

å°¤å…¶é€‚åˆåœ¨äº‘è®¡ç®—æˆ–å…¶ä»–æ—  GUI ç•Œé¢çš„ Linux ä¸»æœºä¸Šä½¿ç”¨ã€‚

```bash
# å®‰è£…
pip install kaggle

# ä¸‹è½½æŒ‡å®šæ•°æ®é›†
kaggle competitions download -c california-house-prices

# è§£å‹
unzip california-house-prices-xxx.zip && rm california-house-prices-xxx.zip
```

### Kaggle æ¯ä¸ªé¡¹ç›®çš„æ•°æ®æ¦‚è§ˆé¡µé¢è‡³å°‘è¦ç¥ä¸€çœ¼

Kaggle çš„æ¯ä¸ªé¡¹ç›®ä¸»é¡µï¼Œéƒ½è®¾æœ‰æ•°æ®çš„æ¦‚è§ˆé¡µé¢ï¼Œç‚¹å‡»`Data`ï¼Œå¯çœ‹åˆ°æ¯ä¸ªç‰¹å¾çš„**æ•°æ®åˆ†å¸ƒ**ã€**æ•°æ®ç±»å‹**ã€**ç®€è¦ç»Ÿè®¡**ç­‰ç­‰ï¼Œååˆ†æ–¹ä¾¿å¯¹æ•°æ®æ•´ä½“è¿›è¡Œä¸€ä¸ªåˆæ­¥çš„æŠŠæ¡

![kaggle_data](../Images/kaggle_data.jpg)

### æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰

æ‹¿åˆ°æ•°æ®åï¼Œç¬¬ä¸€æ­¥å¯ä»¥å…ˆè¿›è¡Œ EDAï¼Œä½¿ç”¨ Pandasã€Seabornã€Plotly åŒ…å¯ä»¥é«˜æ•ˆåœ°å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå¤„ç†ã€‚ä»¥ä¸‹ä¸ºå¸¸ç”¨å‘½ä»¤ï¼š

```python
# å¯¼å…¥ç›¸å…³åŒ…
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# è¯»å–CSVæ ¼å¼æ–‡ä»¶å¹¶è½¬æ¢ä¸ºDataframeæ ¼å¼
data = pd.read_csv('train_data.csv')

# å±•ç¤ºæ•°æ®å½¢çŠ¶
data.shape

# å±•ç¤ºå„åˆ—æ•°æ®ç±»å‹
data.info()

# å±•ç¤ºæ•°å€¼åˆ—ç®€è¦ç»Ÿè®¡ä¿¡æ¯
data.describe()

# å–æ ·
data.sample()

# å–å‡ºå‰nè¡Œã€ånè¡Œ
data.head(n)
data.tail(n)

# å–å‡ºæŒ‡å®šç´¢å¼•åœ°è¡Œæˆ–åˆ—ï¼ˆå¯ç”¨åˆ‡ç‰‡ï¼‰
data.iloc[:,[1:3]]
```

- å¯¹äºæ•°å€¼åˆ—

```python
# å¯¹æŸåˆ—è¿›è¡Œæ•°æ®æ˜ å°„
data['gender'] = data['gender'].map({"male":0,"femal":1})

# å¯¹æŸåˆ—è¿›è¡Œæ•°æ®å¤„ç†æ“ä½œ
data['Bedrooms'] = data['Bedrooms'].map(lambda x: float(x))

# å¦‚æœå¤„ç†å‡½æ•°æ¯”è¾ƒå¤æ‚ï¼Œæœ‰ä¸æ­¢ä¸€ä¸ªå‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨`apply()`
def apply_age(x,bias):
    return x+bias
#ä»¥å…ƒç»„çš„æ–¹å¼ä¼ å…¥é¢å¤–çš„å‚æ•°
data["age"] = data["age"].apply(apply_age,args=(-3,))
# å¤šåˆ—è®¡ç®—
data['col3'] = df.apply(lambda x: x['col1'] + 2 * x['col2'], axis=1)

# æŒ‰è¡Œæ¨ªå‘apply
def BMI(series):
    weight = series["weight"]
    height = series["height"]/100
    BMI = weight/height**2
    return BMI
data["BMI"] = data.apply(BMI,axis=1)

# å¾…è¡¥å……`.filter() .groupby() .agg . `

# è®¡ç®—å„æ•°å€¼åˆ—åœ°ç›¸å…³ç³»æ•°ï¼Œå¯é€‰{'pearson', 'kendall', 'spearman'}
data_corr = data.corr(method='pearson')
# åšå‡ºç›´è§‚çš„ç›¸å…³ç³»æ•°çƒ­åŠ›å›¾
plt.subplots(figsize=(20,20))
mask = np.zeros_like(data_corr,dtype=np.bool)
mask[np.triu_indices_from(mask)] = True # é®æ‰é‡å¤çš„ä¸ŠåŠå›¾
sns.heatmap(data_corr,mask=mask,annot=True) # annotæ˜¾ç¤ºæ•°å€¼
plt.show()

# ä½¿ç”¨pairplotå±•ç°ä¸¤ä¸¤å˜é‡åˆ—çš„å…³ç³»å’Œæ•°æ®åˆ†å¸ƒ
# ä¼ å…¥æ•°å€¼åˆ—æ•°æ®ï¼Œ'corner'ï¼šæ˜¯å¦å±•ç¤ºåŠè§’ï¼›
# 'kind'={"scatter","reg"}éå¯¹è§’çº¿å­å›¾ç”¨æ•£ç‚¹æˆ–å›å½’æ‹Ÿåˆ:
# 'diag_kind'={"hist","kde"}å¯¹è§’çº¿å­å›¾ä½¿ç”¨ç›´æ–¹å›¾æˆ–æ ¸å¯†åº¦å‡½æ•°
sns.pairplot(data,vars=var,corner=True, diag_kind='hist',kind='reg',dropna=True)
```

- å¯¹äºç¦»æ•£ç±»åˆ«åˆ—

```python
# è®¡ç®—æ‰€åŒ…å«å„ç±»å‹æ•°é‡
data['Type'].value_counts()

# ç­›é€‰å‡ºå±äºæŸé›†åˆçš„è¡Œæ•°æ®
data[data['Type'].isin(['apple','pear'])]

# å¯¹ç±»åˆ«åˆ—åšå‡ºç®±å›¾ã€å°æç´å›¾
sns.catplot(y="Type",x="Sold Price",kind="violin",data=type_df,height=5,aspect=3)
```

### ä½¿ç”¨è¶…å¥½ç”¨çš„ Pandas-Profiling åº“è¿›è¡Œ EDAï¼Œè§£æ”¾ä¸Šè¿°ç¹ççš„æ“ä½œï¼Œå¥½ç”¨åˆ°å“­ï¼

```python
# å®‰è£…
%pip install pandas-profiling[notebook]
# å°†Dataframeä¼ å…¥åšEDAåˆ†æ
profile = ProfileReport(df, title="Pandas Profiling Report", explorative=True)
```

### æ•°æ®é¢„å¤„ç†

ä¸€èˆ¬æ•°æ®åŒ…å«çš„ç±»å‹æœ‰è¿ç»­å‹æ•°å€¼ï¼ˆå¦‚ä»·æ ¼ã€æ¸©åº¦ï¼‰ã€ç¦»æ•£å‹æ•°å€¼ï¼ˆå¦‚å¹´ä»½ã€é‚®ç¼–ï¼‰ã€æè¿°å‹çŸ­å­—ç¬¦ä¸²ï¼ˆå¦‚æ€§åˆ«ã€åœ°åŒºï¼‰ã€å™è¿°å‹é•¿æ–‡æœ¬ï¼ˆå¦‚æ¦‚è¿°ã€æ–°é—»ã€å¯¹è¯ï¼‰ï¼Œé¢„å¤„ç†æ—¶è¦åŒºåˆ«å¯¹å¾…ã€‚

#### åŒºåˆ†æ•°å€¼ç±»å’Œéæ•°å€¼ç±»ï¼Œå¹¶å¯¹æ•°å€¼ç±»è¿›è¡Œå½’ä¸€åŒ–

```python
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index
all_features[numeric_features] = all_features[numeric_features].apply(
    lambda x: (x - x.mean()) / (x.std()))
all_features[numeric_features] = all_features[numeric_features].fillna(0)
```

#### å¯¹äºç¦»æ•£å‹æ•°å€¼ã€æè¿°å‹çŸ­å­—ç¬¦ï¼Œå¦‚æœå„åˆ—çš„ç§ç±»ä¸å¤šï¼Œå¯é‡‡ç”¨ç‹¬çƒ­ç¼–ç ï¼ˆä½†å¦‚æœç§ç±»å¾ˆå¤šï¼Œåˆ™ä¸èƒ½ç›²ç›®ä½¿ç”¨ï¼Œå¦åˆ™ä¼šæ’‘çˆ†å†…å­˜ï¼‰

```python
no_numeric_features = all_features.dtypes[all_features.dtypes == 'object'].index

all_features[no_numeric_features] = pd.get_dummies(all_features[no_numeric_features], dummy_na=True)
```

#### å¯¹äºç§ç±»å¤šã€ä¸èƒ½é‡‡ç”¨ç‹¬çƒ­ç¼–ç çš„æ•°æ®ï¼Œå¯ä»¥é‡‡ç”¨`scikit-learn.preprocessing`çš„ä¸€ç³»åˆ—ç‰¹å¾å·¥ç¨‹çš„æ–¹æ³•ã€‚å‚è€ƒæ–‡æ¡£ ğŸ‘‰[API](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)

#### å¯¹äºæ–‡æœ¬ç‰¹å¾ï¼Œå¦‚æœä¸ºä¸€äº›é•¿åº¦çŸ­çš„æ–‡æœ¬æˆ–ç”±ä¸“æœ‰é¢†åŸŸè¯è¯­æ„æˆï¼Œå¯ä»¥ä¸ºæ¯ä¸ªç‰¹å¾å•ç‹¬æ„å»ºè¯æ±‡è¡¨`Vocabl`ï¼Œå†è¿›è¡Œè¯åµŒå…¥ç”Ÿæˆç‰¹å¾å‘é‡è¿›è¡Œè®­ç»ƒ

```python
# å®šä¹‰å‡½æ•°ç»Ÿä¸€å°å†™ã€ç©ºæ ¼æ›¿æ¢ç‰¹æ®Šç¬¦å·å¹¶ä»¥ç©ºæ ¼åˆ†è¯çš„å‡½æ•°
import re
def str2tok_list(string):
    return re.sub('[^A-Za-z]+', ' ', str(string)).strip().lower().split()

# åœ¨æ–‡æœ¬åˆ—ä¸Šä»¥mapæ–¹æ³•å¯¹æ¯è¡Œæ–‡å­—è¿›è¡Œå¤„ç†å¹¶è½¬æ¢ä¸ºäºŒç»´åµŒå¥—åˆ—è¡¨
summary_list=all_features['Summary'].map(str2tok_list).tolist()
# å°†åµŒå¥—åˆ—è¡¨å±•å¹³ä¸ºä¸€ç»´list
summary_plain=[token for line in summary_list for token in line]
# ç”Ÿæˆè¯¥åˆ—çš„è¯å…¸
v=d2l.Vocab(summary_plain)
# å¯¹è¯­æ–™è¿›è¡Œä¸‹é‡‡æ ·
subsampled, counter = d2l.subsample(summary_list, v)
# ä½¿ç”¨è¯å…¸ç´¢å¼•æ›¿æ¢åŸæœ‰æ–‡æœ¬
all_features['Summary']=[v[line] for line in subsampled]
# ä¸€èˆ¬æ¨¡å‹è®­ç»ƒè¦æ±‚ä¸ºå®šé•¿åºåˆ—ï¼Œå¯ä»¥ä½¿ç”¨truncate_padæ–¹æ³•æŒ‡å®špad_lené•¿åº¦
all_features['Summary']=[d2l.truncate_pad(v[line],pad_len,v['<pad>']) for line in subsampled]
```

### ç¥ç»ç½‘ç»œç›¸å…³

#### ç”±æµ…å…¥æ·±ï¼Œå¥¥å¡å§†å®šç†

æœ€å¼€å§‹çš„ç½‘ç»œæ¨¡å‹å¯ä»¥å…ˆä»æœ€ç®€å•çš„å•å±‚æˆ–æµ…å±‚çº¿æ€§æ¨¡å‹é€æ­¥è¿‡æ¸¡åˆ°å¤æ‚ç½‘ç»œï¼Œæ²¡å¿…è¦ä¸€å¼€å§‹å°±ä½¿ç”¨é«˜é˜¶ç¥ç»ç½‘ç»œã€‚é¦–å…ˆå°è¯•æµ…å±‚ç½‘ç»œèƒ½å¦ workï¼Ÿwork åˆ°ä»€ä¹ˆç¨‹åº¦ï¼Ÿæ ¹æ®ç»“æœå†é€æ­¥è¿›è¡Œæ”¹è¿›ã€‚

#### å¯¹æ•°å€¼åˆ—è¿›è¡Œ MLP æ“ä½œ

ç²¾åº¦æœ€é«˜ä¸º Public Scoreï¼š0.38412ã€Private Scoreï¼š0.40706

#### å¯¹æ•°æ® Vector åŒ–ï¼Œå¹¶é‡‡ç”¨ä»¥ä¸‹æ··åˆç»“æ„

å°†æ•°æ®åˆ†ä¸ºæ•°å€¼ç±»ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ï¼Œé¦–å…ˆå¯¹æ–‡æœ¬ç‰¹å¾åˆ†åˆ«æŒ‰åˆ—è¿›è¡Œåˆ†è¯å¹¶ä½¿ç”¨ Vocab è½¬æ¢ä¸ºå¯¹åº”çš„ç´¢å¼•ï¼Œç„¶åç»Ÿè®¡å„åˆ—åˆ†è¯é•¿åº¦ï¼ŒæŒ‰å¹³å‡é•¿åº¦è¿›è¡Œæˆªæ–­æˆ– Paddingã€‚

å†å°†æ•°å€¼ç±»å’Œæ–‡æœ¬ç±»ç‰¹å¾ concat åˆ°ä¸€èµ·ï¼Œä½œä¸ºè®­ç»ƒæ•°æ®ã€‚

```mermaid
flowchart BT
    subgraph MixMLP
    direction TB
        A[vec_data] --> B[numeric_data] & C[token_data]
        subgraph MLP x n
            direction BT
        B --> D[Dense]
        D --> E[BatchNorm]
        E --> F[ReLu]
        end
        subgraph TextCNN
            direction BT
            C --> G[fragment 1] & H[fragment 2] & SS[...] & I[fragment n]
            G --> K[Embedding1]
            H --> L[Embedding2]
            SS --> SSS[...]
            I --> M[Embedding n]
            K --> N[Conv1d]
            L --> O[Conv1d]
            SSS --> SSSS[...]
            M --> P[Conv1d]
            N & O & SSSS & P --> Q[text_features]
        end
        F --> R[numeric_features]
        Q & R --> S[concat_features]
        S --> T[MLP]
        T --> U[Sold Price]
    end
```

#### Embedding å±‚è¾“å…¥æ•°æ®è¦æ±‚å¿…é¡»ä¸º`Int`æˆ–`Long`æ•°æ®ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤`FloatTensor`æ•°æ®ç±»å‹ä¼šæŠ¥é”™

> å¯é€šè¿‡`X.int()` or `X.long()`è¿›è¡Œè½¬æ¢

#### ä½¿ç”¨åµŒå…¥å±‚å¾—åˆ°è¯å‘é‡ï¼Œåœ¨é€å¾€ Conv1d å·ç§¯å±‚å‰ï¼Œéœ€è¦å¯¹ Tensor ç»´åº¦ä½¿ç”¨æ–¹æ³•`.permute()`è¿›è¡Œè°ƒæ¢ï¼Œä»¥ç¬¦åˆå·ç§¯è¾“å…¥æ ¼å¼

åµŒå…¥å±‚çš„è¾“å‡ºå½¢çŠ¶æ˜¯`(æ‰¹é‡å¤§å°, è¯å…ƒæ•°é‡, è¯å…ƒå‘é‡ç»´åº¦)`ï¼Œè€Œå·ç§¯å±‚è¦æ±‚è¾“å…¥å½¢çŠ¶ä¸º`(æ‰¹é‡å¤§å°ï¼Œè¯å…ƒç‰¹å¾ç»´åº¦, è¯å…ƒæ•°é‡)`ï¼Œå¯ä»¥ä½¿ç”¨`embeddings.permute(0, 2, 1)`è¿›è¡Œè°ƒæ¢ã€‚

### æ¨¡å‹è®­ç»ƒç›¸å…³

#### ä¸èƒ½ç›´æ¥å°† BN å±‚ä¸ Dropout å±‚æ··ç”¨

BatchNorm å’Œ Dropout ä¸¤ç§æ–¹æ³•ä¸èƒ½æ··ç”¨ï¼Œå®é™…ä½¿ç”¨ä¸­å‘ç°ä¼šå¯¼è‡´ç²¾åº¦ä¸‹é™ä¸”è®­ç»ƒä¸ç¨³å®šçš„ç°è±¡ã€‚

è®ºæ–‡å‚è€ƒ ğŸ‘‰[Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift](https://arxiv.org/abs/1801.05134)

#### è®°å¾—åœ¨æ¨¡å‹é¢„æµ‹æ—¶å†™ä¸Š`net.eval()`å¼€å¯è¯„ä¼°æ¨¡å¼ï¼Œä»¥å…å‰åŠŸå°½å¼ƒ

#### åœ¨ä½¿ç”¨ GPU è®­ç»ƒæ—¶ï¼Œè‹¥æ•°æ®é›†å°äºæ˜¾å­˜å®¹é‡ï¼Œå¯ç›´æ¥å°†æ•´ä¸ªè®­ç»ƒé›†æ”¾ç½®äºæ˜¾å¡ä¸Šï¼Œæ¥æé«˜è¿ç®—æ•ˆç‡

æœ¬æ¬¡Kaggleé¡¹ç›®æ‰€æ¶‰åŠæ•°æ®ç»å‰æœŸç‰¹å¾åŒ–é¢„å¤„ç†ï¼Œæ•°æ®è§„æ¨¡å·²ç¼©å°å¾ˆå¤šï¼Œå¦‚æœæŒ‰ç…§å¸¸ç”¨æ–¹æ¡ˆåœ¨è¯»å–æ¯ä¸ªbatchæ•°æ®åç§»è‡³GPUæ˜¾å­˜è¿›è¡Œè®¡ç®—ï¼Œä¼šé€ æˆé¢‘ç¹è·¨è®¾å¤‡çš„æ•°æ®ç§»åŠ¨ï¼Œäº§ç”Ÿå¾ˆå¤§çš„è®¡ç®—å¼€é”€ã€‚å¦‚æœæ˜¾å­˜è¶³å¤Ÿå¤§ï¼ˆå£•ï¼‰ï¼Œå°±ç›´æ¥å°†æ•°æ®é›†æ”¾ç½®äºGPUä¸Šå³å¯ã€‚å…³äºä¸åŒç¡¬ä»¶çš„æ—¶å»¶å¯¹æ¯”ï¼Œå¯ä»¥å‚è€ƒğŸ‘‰[æœ¬ç« ](../28-æ·±åº¦å­¦ä¹ ç¡¬ä»¶ï¼ˆCPUå’ŒGPUï¼‰.md)

#### Adam ä¼˜åŒ–å™¨ä¸èƒ½ä¸ Weight Decey ä¸€èµ·ä½¿ç”¨ï¼Œå¯ä»¥ç”¨ AdamW ä¼˜åŒ–å™¨æ›¿ä»£

åœ¨ Adam ä¼˜åŒ–å™¨ä¸­ï¼Œweight decay ä¸ L2 æ­£åˆ™å¹¶ä¸ç­‰ä»·ï¼Œé™¤æ­¤ä¹‹å¤–ï¼ŒAdam+L2 çš„æ–¹æ¡ˆä¼šå¯¼è‡´ä¸ç†æƒ³çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚è®ºæ–‡ã€ŠDecoupled Weight Decay Regularizationã€‹æŒ‡å‡ºäº†è¿™ä¸€ç‚¹

> Just adding the square of the weights to the loss function is _not_ the correct way of using L2 regularization/weight decay with Adam, since that will interact with the m and v parameters in strange ways.
