# 25 - å«å¹¶è¡Œè¿æ¥çš„ç½‘ç»œ GoogLeNet

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‡

[![Bilibil](https://i2.hdslb.com/bfs/archive/537f10ce693bd7113b4eba116e64ec4ab443039d.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1b5411g7Xo)

## é—®é¢˜çš„å¼•å‡º

åœ¨ 2014 å¹´çš„ ImageNet å›¾åƒè¯†åˆ«æŒ‘æˆ˜èµ›ä¸­ï¼Œä¸€ä¸ªåå« GoogLeNet [[Szegedy et al., 2015]](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)çš„ç½‘ç»œæ¶æ„å¤§æ”¾å¼‚å½©ã€‚ GoogLeNet å¸æ”¶äº† NiN ä¸­**ä¸²è”ç½‘ç»œ**çš„æ€æƒ³ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šåšäº†æ”¹è¿›ã€‚ è¿™ç¯‡è®ºæ–‡çš„ä¸€ä¸ªé‡ç‚¹æ˜¯**è§£å†³äº†ä»€ä¹ˆæ ·å¤§å°çš„å·ç§¯æ ¸æœ€åˆé€‚çš„é—®é¢˜**ã€‚ æ¯•ç«Ÿï¼Œä»¥å‰æµè¡Œçš„ç½‘ç»œä½¿ç”¨å°åˆ° 1Ã—1 ï¼Œå¤§åˆ° 11Ã—11 çš„å·ç§¯æ ¸ã€‚ è¯¥æ–‡çš„ä¸€ä¸ªè§‚ç‚¹æ˜¯ï¼Œæœ‰æ—¶ä½¿ç”¨ä¸åŒå¤§å°çš„å·ç§¯æ ¸ç»„åˆæ˜¯æœ‰åˆ©çš„ã€‚

## Inception å—

åœ¨ GoogLeNet ä¸­ï¼ŒåŸºæœ¬çš„å·ç§¯å—è¢«ç§°ä¸º**Inception å—**ï¼ˆInception blockï¼‰ã€‚è¿™å¾ˆå¯èƒ½å¾—åäºç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹ï¼ˆInceptionï¼‰ï¼Œå› ä¸ºç”µå½±ä¸­çš„ä¸€å¥è¯â€œæˆ‘ä»¬éœ€è¦èµ°å¾—æ›´æ·±â€ï¼ˆâ€œWe need to go deeperâ€ï¼‰ã€‚

![](Images/inception.jpg)

Inception å—ä½¿ç”¨ç”¨ 4 ä¸ªè·¯å¾„ä»ä¸åŒå±‚é¢æŠ½å–ä¿¡æ¯ï¼Œç„¶ååœ¨è¾“å‡ºé€šé“ç»´åˆå¹¶ã€‚

![inception](https://zh.d2l.ai/_images/inception.svg)

- è¾“å‡ºå’Œè¾“å…¥ç­‰é«˜å®½ï¼Œå°† 4 æ¡è·¯å¾„çš„ç»“æœæŒ‰é€šé“åˆå¹¶
- 1x1 å·ç§¯å±‚ï¼šè¿›è¡Œé€šé“èåˆï¼Œé™ä½é€šé“æ•°æ¥æ§åˆ¶æ¨¡å‹å¤æ‚åº¦
- é 1x1 çš„å·ç§¯å±‚ã€MaxPoolingï¼šæå–ç©ºé—´ç‰¹æ€§ï¼Œå¢åŠ é²æ£’æ€§
- æ¯æ¡è·¯å¾„çš„é€šé“æ•°å¯èƒ½ä¸ä¸€æ ·

### Inception çš„ä¼˜åŠ¿

è·Ÿå• 3x3 æˆ– 5x5 å·ç§¯å±‚ç›¸æ¯”ï¼Œè¾“å‡ºç›¸åŒé€šé“æ•°ï¼ŒInception å—åªéœ€è¦æ›´å°‘çš„å‚æ•°ä¸ªæ•°å’Œè®¡ç®—å¤æ‚åº¦.

|           | #parameters | FLOPS |
| --------- | ----------- | ----- |
| Inception | 0.16M       | 128M  |
| 3x3 Conv  | 0.44M       | 346M  |
| 5x5 Conv  | 1.22M       | 963M  |

> FLOPSï¼ŒFloating-point Operations Per Second

## GoogLeNet

åŒ…å« 5 Stages, 9 Inception å—

- æ¶æ„å›¾

![nception-full](Images/inception-full.svg)

- å„å±‚ä¿¡æ¯

![table](Images/1_WfKerFhMvUGti7MWVQ81XQ.png)

### Inception å˜ç§

- Inception-BN(V2)ï¼šä½¿ç”¨äº† batch normalization
- Inception-V3ï¼šä¿®æ”¹äº† Inception(åŒ…å«è¯¡å¼‚çš„ 1x3ã€3x1ã€1x7ã€7x1Conv)
- Inception-V4ï¼šä½¿ç”¨æ®‹å·®è¿æ¥

## æ€»ç»“

- Inception å—ä½¿ç”¨ 4 æ¡æœ‰ä¸åŒè¶…å‚æ•°çš„å·ç§¯å±‚å’Œæ± åŒ–å±‚çš„é€šè·¯æ¥æŠ½å–ä¸åŒçš„ä¿¡æ¯
  - ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯æ¨¡å‹å‚æ•°å°ï¼Œè®¡ç®—å¤æ‚åº¦ä½
- GooLeNet ç”¨äº† 9 ä¸ª Inception å—ï¼Œæ˜¯ç¬¬ä¸€ä¸ªè¾¾åˆ°ä¸Šç™¾å±‚çš„ç½‘ç»œ
  - åç»­æœ‰ä¸€ç³»åˆ—æ”¹è¿›å˜ç§

## ä»£ç å®ç°

- å®ç° Inception å—

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

class Inception(nn.Module):
    # c1~c4è¡¨ç¤ºå››æ¡è·¯å¾„çš„é€šé“æ•°
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs) #ç»§æ‰¿çˆ¶ç±»å±æ€§

        #pass1 1x1conv
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1) #Conv2d, default_stride=1, default_padding=0
        #pass2 1x1+3x3conv
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        #pass3 1x1+5x5conv
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        #pass4 3x3MaxPool+1x1conv
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)

    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        # .cat(dim=1)åœ¨é€šé“ç»´dim=1åˆå¹¶ç»“æœ
        return torch.cat((p1, p2, p3, p4), dim=1)
```

- å®šä¹‰ Stage1 ç»“æ„ï¼šä½¿ç”¨ 64 ä¸ªé€šé“ã€ 7Ã—7 å·ç§¯å±‚

```python
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

- å®šä¹‰ Stage2 ç»“æ„ï¼šç¬¬ä¸€ä¸ªå·ç§¯å±‚æ˜¯ 64 ä¸ªé€šé“ã€ 1Ã—1 å·ç§¯å±‚ï¼›ç¬¬äºŒä¸ªå·ç§¯å±‚ä½¿ç”¨å°†é€šé“æ•°é‡å¢åŠ ä¸‰å€çš„ 3Ã—3 å·ç§¯å±‚

```python
b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),nn.ReLU(),
                   nn.Conv2d(64, 192, kernel_size=3, padding=1),nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

- å®šä¹‰ Stage3 ç»“æ„ï¼šç¬¬ä¸€ä¸ª Inception å—çš„è¾“å‡ºé€šé“æ•°ä¸º 64+128+32+32=256ã€‚ç¬¬äºŒä¸ª Inception å—çš„è¾“å‡ºé€šé“æ•°å¢åŠ åˆ° 128+192+96+64=480

```python
b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),
                   Inception(256, 128, (128, 192), (32, 96), 64),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

- å®šä¹‰ Stage4 ç»“æ„ï¼šä¸²è” 5 ä¸ª Inception å—

```python
b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),
                   Inception(512, 160, (112, 224), (24, 64), 64),
                   Inception(512, 128, (128, 256), (24, 64), 64),
                   Inception(512, 112, (144, 288), (32, 64), 64),
                   Inception(528, 256, (160, 320), (32, 128), 128),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

- å®šä¹‰ Stage5 ç»“æ„ï¼šä¸¤ä¸ª Inception å—åé¢ç´§è·Ÿè¾“å‡ºå±‚

```python
b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),
                   Inception(832, 384, (192, 384), (48, 128), 128),
                   nn.AdaptiveAvgPool2d((1,1)),
                   nn.Flatten())

net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))
```

- æµ‹è¯•å„å±‚è¾“å‡º

```python
X = torch.rand(size=(1, 1, 96, 96))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)

# Out:
# Sequential output shape:     torch.Size([1, 64, 24, 24])
# Sequential output shape:     torch.Size([1, 192, 12, 12])
# Sequential output shape:     torch.Size([1, 480, 6, 6])
# Sequential output shape:     torch.Size([1, 832, 3, 3])
# Sequential output shape:     torch.Size([1, 1024])
# Linear output shape:         torch.Size([1, 10])
```

- è®­ç»ƒ

```python
lr, num_epochs, batch_size = 0.1, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())

# Out:
# loss 0.244, train acc 0.908, test acc 0.896
# 3490.2 examples/sec on cuda:0
```

![output_googlenet](https://zh.d2l.ai/_images/output_googlenet_83a8b4_90_1.svg)
