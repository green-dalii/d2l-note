# 32 - ç‰©ä½“æ£€æµ‹å’Œæ•°æ®é›†

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‡

[![Bilibil](https://i1.hdslb.com/bfs/archive/74c42a4b752084a4f20f3e0ec318b59f009679ae.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1Lh411Y7LX)

## ç›®æ ‡æ£€æµ‹ï¼ˆObject Detectionï¼‰

### å›¾ç‰‡åˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²

![Object Detection](Images/1_Hz6t-tokG1niaUfmcysusw.jpeg)

å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å‡è®¾å›¾åƒä¸­åªæœ‰ä¸€ä¸ªä¸»è¦ç‰©ä½“å¯¹è±¡ï¼Œæˆ‘ä»¬åªå…³æ³¨å¦‚ä½•è¯†åˆ«å…¶ç±»åˆ«ã€‚ ç„¶è€Œï¼Œå¾ˆå¤šæ—¶å€™å›¾åƒé‡Œæœ‰å¤šä¸ªæˆ‘ä»¬æ„Ÿå…´è¶£çš„ç›®æ ‡ï¼Œæˆ‘ä»¬ä¸ä»…æƒ³çŸ¥é“å®ƒä»¬çš„ç±»åˆ«ï¼Œè¿˜æƒ³å¾—åˆ°å®ƒä»¬åœ¨å›¾åƒä¸­çš„å…·ä½“ä½ç½®ã€‚ åœ¨è®¡ç®—æœºè§†è§‰é‡Œï¼Œæˆ‘ä»¬å°†è¿™ç±»ä»»åŠ¡ç§°ä¸ºç›®æ ‡æ£€æµ‹ï¼ˆobject detectionï¼‰æˆ–ç›®æ ‡è¯†åˆ«ï¼ˆobject recognitionï¼‰ã€‚

ç›®æ ‡æ£€æµ‹åœ¨å¤šä¸ªé¢†åŸŸä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚ ä¾‹å¦‚ï¼Œåœ¨**æ— äººé©¾é©¶**é‡Œï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡è¯†åˆ«æ‹æ‘„åˆ°çš„è§†é¢‘å›¾åƒé‡Œçš„è½¦è¾†ã€è¡Œäººã€é“è·¯å’Œéšœç¢ç‰©çš„ä½ç½®æ¥è§„åˆ’è¡Œè¿›çº¿è·¯ã€‚ **æœºå™¨äºº**ä¹Ÿå¸¸é€šè¿‡è¯¥ä»»åŠ¡æ¥æ£€æµ‹æ„Ÿå…´è¶£çš„ç›®æ ‡ã€‚**å®‰é˜²é¢†åŸŸ**åˆ™éœ€è¦æ£€æµ‹å¼‚å¸¸ç›®æ ‡ï¼Œå¦‚æ­¹å¾’æˆ–è€…ç‚¸å¼¹ã€‚

### è¾¹ç•Œæ¡†ï¼ˆbounding boxï¼‰

ä¸€ä¸ªè¾¹ç•Œæ¡†å¯ä»¥ç”¨ 4 ä¸ªæ•°å­—å®šä¹‰

- å·¦ä¸Š xï¼Œå·¦ä¸Š yï¼Œå³ä¸‹ xï¼Œå³ä¸‹ y
- å·¦ä¸Š xï¼Œå·¦ä¸Š yï¼Œå®½ï¼Œé«˜

ç›®æ ‡è¯†åˆ«çš„æ•°æ®é›†é€šå¸¸æ¯”å›¾ç‰‡åˆ†ç±»çš„æ•°æ®é›†å°å¾ˆå¤šã€‚

### ç›®æ ‡æ£€æµ‹æ•°æ®é›†

- æ¯è¡Œè¡¨ç¤ºä¸€ä¸ªç‰©ä½“ï¼Œè‹¥ä¸€å¼ å›¾é‡Œæœ‰ n ä¸ªç‰©ä½“ï¼Œåˆ™é‡å¤ n è¡Œ

  > å¦‚ï¼š`å›¾ç‰‡æ–‡ä»¶å,ç‰©ä½“ç±»åˆ«,è¾¹ç¼˜æ¡†(x1,y1,x2,y2)`

- [COCO æ•°æ®é›†](cocodataset.org)

  > 80 ç±»åˆ«ï¼Œ330K å›¾ç‰‡ï¼Œ1.5M ç‰©ä½“

## ä»£ç å®ç°

```python
%matplotlib inline
import torch
from d2l import torch as d2l

d2l.set_figsize()
img = d2l.plt.imread('../Image/Lions.png')
d2l.plt.imshow(img)
```

- å®šä¹‰ä¸¤ç§è¾¹ç•Œæ¡†è¡¨ç¤ºå‡½æ•°

```python
def box_corner_to_center(boxes):
    """ä»ï¼ˆå·¦ä¸Šï¼Œå³ä¸‹ï¼‰è½¬æ¢åˆ°ï¼ˆä¸­é—´ï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼‰"""
    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    #è¡Œæ•°>1,è¡¨ç¤ºå¤šä¸ªæ¡†
    cx = (x1 + x2) / 2
    cy = (y1 + y2) / 2
    w = x2 - x1
    h = y2 - y1
    boxes = torch.stack((cx, cy, w, h), axis=-1)
    #torch.stack(),tensorçš„concatenate
    #Concatenates a sequence of tensors along a new dimension.
    return boxes

def box_center_to_corner(boxes):
    """ä»ï¼ˆä¸­é—´ï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼‰è½¬æ¢åˆ°ï¼ˆå·¦ä¸Šï¼Œå³ä¸‹ï¼‰"""
    cx, cy, w, h =boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    x1 = cx - 0.5 * w
    y1 = cy - 0.5 * h
    x2 = cx + 0.5 * w
    y2 = cy + 0.5 * h
    boxes = torch.stack((x1, y1, x2, y2), axis=-1)
    return boxes
```

æˆ‘ä»¬å°†æ ¹æ®åæ ‡ä¿¡æ¯å®šä¹‰å›¾åƒä¸­ç‹—å’ŒçŒ«çš„è¾¹ç•Œæ¡†ã€‚ å›¾åƒä¸­åæ ‡çš„åŸç‚¹æ˜¯å›¾åƒçš„å·¦ä¸Šè§’ï¼Œå‘å³çš„æ–¹å‘ä¸º x è½´çš„æ­£æ–¹å‘ï¼Œå‘ä¸‹çš„æ–¹å‘ä¸º y è½´çš„æ­£æ–¹å‘ã€‚

```python
# bboxæ˜¯è¾¹ç•Œæ¡†çš„è‹±æ–‡ç¼©å†™
dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]

boxes = torch.tensor((dog_bbox, cat_bbox))
box_center_to_corner(box_corner_to_center(boxes)) == boxes
# Out:
# tensor([[True, True, True, True],
#         [True, True, True, True]])
```

- ç”»å‡ºè¾¹ç•Œæ¡†

```python
def bbox_to_rect(bbox, color):
    return d2l.plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2] - bbox[0],
                             height=bbox[3] - bbox[1], fill=False, edgecolor=color, linewidth=2)

fig = d2l.plt.imshow(img)
fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))
fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'))
```

![output_bounding-box](https://zh.d2l.ai/_images/output_bounding-box_d6b70e_58_0.svg)

- ç›®æ ‡æ£€æµ‹æ•°æ®é›†

```python
%matplotlib inline
import os
import pandas as pd
import torch
import torchvision
from d2l import torch as d2l

d2l.DATA_HUB['banana-dection'] = (
    d2l.DATA_URL + 'banana-detection.zip',
    '5de26c8fce5ccdea9f91267273464dc968d20d72')

def read_data_bananas(is_train=True):
    data_dir = d2l.download_extract('banana-detection', folder='../data')
    csv_fname = os.path.join('../data/banana-detection', 'bananas_train' if is_train else 'bananas_val', 'label.csv')
    csv_data = pd.read_csv(csv_fname)
    csv_data = csv_data.set_index('img_name')
    images, targets = [], []
    for img_name, target in csv_data.iterrows():
    #iterrows()è¿›è¡Œè¡Œç´¢å¼•ï¼Œè¿”å›ç´¢å¼•å’Œå†…å®¹
        images.append(
            torchvision.io.read_image(
                os.path.join('../data/banana-detection', 'bananas_train' if is_train else
                             'bananas_val', 'images', f'{img_name}')))
            #Reads a JPEG or PNG image into a 3 dimensional RGB Tensor.
            #æŠŠå›¾ç‰‡å¼ é‡åŠ å…¥imagesï¼Œä¹Ÿå°±æ˜¯è¯´æŠŠæ‰€æœ‰å›¾ç‰‡éƒ½è¯»åˆ°å†…å­˜é‡Œ
            #å› ä¸ºæ•°æ®é›†å°
        targets.append(list(target))
        #æŠŠæ‰€è¯†åˆ«å†…å®¹åŠ å…¥targets
    return images, torch.tensor(targets).unsqueeze(1) / 256
    #unsqueeze(1)ï¼Œåœ¨ç¬¬ä¸€ç»´å¢åŠ ç»´åº¦
    #Returns a new tensor with a dimension of size one inserted at the specified position.
    #åŸæ¥æ˜¯dfçš„(1000, 5)ï¼Œç°åœ¨æ˜¯(1000, 1, 5)
    #targetæ˜¯é¦™è•‰çš„æ¡†
    #tensor(target)/256ç›¸å½“äºæŠŠæ¡†æ­£åˆ™åŒ–ï¼Œå˜æˆå…³äºä¸€ä¸ªåƒç´ çš„ä½ç½®
```

- åˆ›å»ºè‡ªå®šä¹‰ Dataset å®ä¾‹

```python
class BananasDataset(torch.utils.data.Dataset):
    def __init__(self, is_train):
        self.features, self.labels = read_data_bananas(is_train)
        print('read' + str(len(self.features)) + (f' training examples' if is_train else f' validation examples'))
        print(self.labels.shape)

    def __getitem__(self, idx):
    # __getitem__(self, idx)å®šä¹‰äº†ç±»ä¼¼äºç´¢å¼•ï¼Œå»å®ç°å¯ç´¢å¼•çš„åŠŸèƒ½
    # éœ€è¦æ›´å¤æ‚çš„é€»è¾‘å»å®šä¹‰åˆ‡ç‰‡ã€é¡ºåºã€æ­¥é•¿ç­‰
        return (self.features[idx].float(), self.labels[idx])

    def __len__(self):
        return len(self.features)
```

- å®šä¹‰æ•°æ®è¿­ä»£å™¨

```python
def load_data_bananas(batch_size):
    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),
                                             batch_size, shuffle=True)
    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),
                                           batch_size)

    return train_iter, val_iter
```

- æµ‹è¯•

```python
batch_size, edge_size = 32, 256 #å›¾ç‰‡å¤§å°256
train_iter, _ = load_data_bananas(batch_size)
batch = next(iter(train_iter))
batch[0].shape, batch[1].shape
# Out:
# (torch.Size([32, 3, 256, 256]), torch.Size([32, 1, 5]))
# è®­ç»ƒiter(batch_size, RBG_channals, height, width)
# æ ‡å·iter(batch_size, æ¯å¼ å›¾ç‰‡æœ€å¤šç‰©ä½“æ•°, æ ‡å·+è¾¹ç•Œæ¡†4ä¸ªåæ ‡)

imgs = (batch[0][0:10].permute(0, 2, 3, 1)) / 255
#batch[0]æ˜¯imgs
#permute()å‡½æ•°æ”¹å˜tensorçš„ç»´åº¦é¡ºåº
print(imgs.shape)
axes = d2l.show_images(imgs, 2, 5, scale=2)
#Plot a list of images.

for ax, label in zip(axes, batch[1][0:10]):
    #batch[1]æ˜¯boxes,tensor.shape = (32, 1, 5)
    #bacth[1][0:10]å°±æ˜¯å‰åä¸ªboxes
    #label.shape=(1, 5)
    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])
```

## Python æ¨¡å—å‚è€ƒæ–‡æ¡£

- `matplotlib.axes.Axes.add_patch` Matplotlib æ·»åŠ  Patch åˆ°è½´çš„è¡¥ä¸ ğŸ§[ä¸­æ–‡](https://www.osgeo.cn/matplotlib/api/_as_gen/matplotlib.axes.Axes.add_patch.html) | [å®˜æ–¹è‹±æ–‡](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.add_patch.html)
- `torch.unsqueeze` Pytorch Tensor åœ¨æŒ‡å®š dim å‰æ’å…¥ç»´åº¦ ğŸ§[ä¸­æ–‡](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch/) | [å®˜æ–¹è‹±æ–‡](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze)
- `torch.utils.data.DataLoader` Pytorch Dataloader è¿­ä»£å™¨ ğŸ§[ä¸­æ–‡](https://pytorch-cn.readthedocs.io/zh/latest/package_references/data/) | [å®˜æ–¹è‹±æ–‡](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)

---

## Q&AğŸ¤“

**Qï¼šæ€æ ·æ ‡æ³¨æ–°çš„æ•°æ®é›†å¹¶ä½¿ç”¨ï¼Ÿ**

**ğŸ™‹â€â™‚ï¸**ï¼šä¸€èˆ¬æ¥è¯´ä¸ç”¨å¤§è§„æ¨¡çš„ä»å¤´æ ‡æ³¨ä¸€ä¸ªå¤§æ•°æ®é›†ï¼Œé€šå¸¸ä¼šä½¿ç”¨è¿ç§»æ¨¡å‹ï¼Œæ ‡æ³¨å°‘é‡çš„ç›®æ ‡æ•°æ®é›†åï¼Œåœ¨å·²æœ‰çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸Šåš fine-tuningï¼Œç„¶åé€‰å‡ºç»“æœç½®ä¿¡åº¦ä¸é«˜çš„æ ·æœ¬ï¼Œå†è¿›è¡Œæ ‡æ³¨ã€å¾®è°ƒï¼Œä½¿ç”¨è¿™ç§æ–¹å¼è¿›è¡Œè¿­ä»£ç›´åˆ°æ•ˆæœæ»¡æ„ä¸ºæ­¢ã€‚
