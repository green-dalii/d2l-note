# 31 - Kaggle å®æˆ˜ - CIFAR-10

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‡

[![Bilibil](https://i2.hdslb.com/bfs/archive/1060d9d14c8d840fefaf6972f8b539d05655aa5d.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1Gy4y1M7Cu)

## CIFAR-10æ•°æ®é›†

ä¹‹å‰å‡ èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶çš„é«˜çº§ API ç›´æ¥è·å–å¼ é‡æ ¼å¼çš„å›¾åƒæ•°æ®é›†ã€‚ ä½†æ˜¯åœ¨å®è·µä¸­ï¼Œå›¾åƒæ•°æ®é›†é€šå¸¸ä»¥å›¾åƒæ–‡ä»¶çš„å½¢å¼å‡ºç°ã€‚ åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»åŸå§‹å›¾åƒæ–‡ä»¶å¼€å§‹ï¼Œç„¶åé€æ­¥ç»„ç»‡ã€è¯»å–å¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå¼ é‡æ ¼å¼ã€‚

æˆ‘ä»¬å°†æ•°æ®åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œä¸€æ—¦æ‰¾åˆ°çš„æœ€ä½³çš„å‚æ•°ï¼Œå°±åœ¨æµ‹è¯•é›†ä¸Šæœ€åæµ‹è¯•ä¸€æ¬¡ï¼Œæµ‹è¯•é›†ä¸Šçš„è¯¯å·®ä½œä¸ºæ³›åŒ–è¯¯å·®çš„è¿‘ä¼¼ã€‚

æ¯”èµ›æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­è®­ç»ƒé›†åŒ…å« 50000 å¼ ã€æµ‹è¯•é›†åŒ…å« 300000 å¼ å›¾åƒã€‚ åœ¨æµ‹è¯•é›†ä¸­ï¼Œ10000 å¼ å›¾åƒå°†è¢«ç”¨äºè¯„ä¼°ï¼Œè€Œå‰©ä¸‹çš„ 290000 å¼ å›¾åƒå°†ä¸ä¼šè¢«è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…å«å®ƒä»¬åªæ˜¯ä¸ºäº†é˜²æ­¢æ‰‹åŠ¨æ ‡è®°æµ‹è¯•é›†å¹¶æäº¤æ ‡è®°ç»“æœã€‚ ä¸¤ä¸ªæ•°æ®é›†ä¸­çš„å›¾åƒéƒ½æ˜¯ png æ ¼å¼ï¼Œé«˜åº¦å’Œå®½åº¦å‡ä¸º 32 åƒç´ å¹¶æœ‰ä¸‰ä¸ªé¢œè‰²é€šé“ï¼ˆRGBï¼‰ã€‚ è¿™äº›å›¾ç‰‡å…±æ¶µç›– 10 ä¸ªç±»åˆ«ï¼šé£æœºã€æ±½è½¦ã€é¸Ÿç±»ã€çŒ«ã€é¹¿ã€ç‹—ã€é’è›™ã€é©¬ã€èˆ¹å’Œå¡è½¦

## ä»£ç å®ç°

- å¼•å…¥åŒ…&ä¸‹è½½æ•°æ®

```python
import collections
import math
import os
# shutil æ¨¡å—æä¾›äº†ä¸€ç³»åˆ—å¯¹æ–‡ä»¶å’Œæ–‡ä»¶é›†åˆçš„é«˜é˜¶æ“ä½œã€‚
import shutil
import pandas as pd
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip','2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')
#æ­¤æ•°æ®é›†æ˜¯ä¿å­˜åœ¨AWSçš„demoè€Œéå®Œæ•´çš„cifar-10

demo = True

if demo:
    data_dir = d2l.download_extract('cifar10_tiny')
else:
    data_dir = '../code/data/cifar-10/'
```

æ•°æ®é›†ç»“æ„ï¼š

- ../data/cifar-10/train/[1-50000].png
- ../data/cifar-10/test/[1-300000].png
- ../data/cifar-10/trainLabels.csv
- ../data/cifar-10/sampleSubmission.csv

> train å’Œ test æ–‡ä»¶å¤¹åˆ†åˆ«åŒ…å«è®­ç»ƒå’Œæµ‹è¯•å›¾åƒï¼ŒtrainLabels.csv å«æœ‰è®­ç»ƒå›¾åƒçš„æ ‡ç­¾ï¼Œ sample_submission.csv æ˜¯æäº¤æ–‡ä»¶çš„èŒƒä¾‹ã€‚

- æ•´ç†æ•°æ®é›†

```python
def read_csv_labels(fname):
    #labelså­˜åœ¨csvæ–‡ä»¶é‡Œ
    with open(fname, 'r') as f:
        lines = f.readlines()[1:]
        #readlines() æŒ‰è¡Œè¯»å–æ•´ä¸ªæ–‡ä»¶ï¼Œã€‚
    tokens = [l.rstrip().split(',') for l in lines]
    #strip()åˆ é™¤å­—ç¬¦ä¸²å¤´å°¾ç‰¹å®šçš„å­—ç¬¦ï¼Œé»˜è®¤æ˜¯ç©ºæ ¼å’Œæ¢è¡Œç¬¦
    #rstrip()åœ¨æ­¤åˆ å»å³ä¾§æ¢è¡Œç¬¦
    #split()ä¸­é—´æŒ‰ç…§é€—å·åˆ†éš”
    return dict(((name, label) for name, label in tokens))
    #æŠŠ[]æ¢æˆ()çš„åˆ—è¡¨ç”Ÿæˆå¼ï¼Œå°±æ˜¯ä¸€ä¸ªgenerator
    #å¯ä»¥ç”¨list/tuple(generator)æ–¹å¼è½¬æ¢
    #å¦‚æœæ˜¯æˆå¯¹å…ƒç´ ï¼Œå°±å¯ä»¥dict(generator)è½¬æ¢æˆå­—å…¸ï¼Œç›¸å½“äºdict(list)çš„æ–¹å¼å®šä¹‰å­—å…¸
    #ä»¥ä¸Šçš„æ–¹å¼éƒ½ä¼šéå†generator
    #å½“åŒä¸€ä¸ªgeneratorè¢«éå†è¿‡ä¸€éæ—¶ï¼Œå†æ¬¡è°ƒç”¨ä¼šè¿”å›None
    #æ¯”å¦‚ç”Ÿæˆå™¨g,å…ˆlist(g)ï¼Œä¼šå¾—åˆ°æ‰€æœ‰å…ƒç´ 
    #å†tuple(g),å¾—åˆ°ç©ºå…ƒç»„ï¼Œæ‰€ä»¥éœ€è¦é‡æ–°å®šä¹‰g

labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))
```

- å°†éªŒè¯é›†ä»åŸå§‹è®­ç»ƒé›†ä¸­æ‹†åˆ†

```python
def copyfile(filename, target_dir):
    # æ–°å»ºæ–‡ä»¶å¤¹
    # exist_ok(default)=Falseï¼Œå¦‚æœç›®æ ‡æ–‡ä»¶å¤¹å·²å­˜åœ¨ä¼šæŠ¥é”™
    #å¯¹äºå·²å­˜åœ¨çš„æ–‡ä»¶å¤¹ä¸ä¼šè¦†ç›–
    os.makedirs(target_dir, exist_ok=True)
    #copyæ–‡ä»¶åˆ°ç›®å½•
    shutil.copy(filename, target_dir)

def reorg_train_valid(data_dir, labels, valid_ratio):
    # æ•´ä½“æ˜¯æŠŠæ–‡ä»¶å…¨éƒ¨å¤åˆ¶åˆ°train_validå¹¶ä¸”åˆ†ä¸ºtrainå’Œvalidï¼Œæ¯ä¸ªä¸‹é¢å†ä»¥ç±»åå»ºç«‹æ–‡ä»¶å¤¹ç»†åˆ†
    # collections.Counteræ˜¯è®¡æ•°å™¨
    # å…ƒç´ ä»¥å­—å…¸å½¢å¼ä¿å­˜
    # most_common(num)è¿”å›æœ€é¢‘ç¹çš„ç±»å‹å’Œé¢‘ç‡ï¼Œå¦‚æœä¸æŒ‡å®šä¸ªæ•°ï¼Œdefaultæ‰€æœ‰ç±»
    # è®­ç»ƒæ•°æ®é›†ä¸­æ ·æœ¬æœ€å°‘çš„ç±»åˆ«ä¸­çš„æ ·æœ¬æ•°
    n = collections.Counter(labels.values()).most_common()[-1][1]
    # math.floorå‘ä¸‹å–æ•´
    # valid_ratioå†³å®šäº†ä¸€ä¸ªä¸Šé™
    # éªŒè¯é›†ä¸­æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
    n_valid_per_label = max(1, math.floor(n * valid_ratio))
    print(n_valid_per_label)
    label_count = {}
    for train_file in os.listdir(os.path.join(data_dir, 'train')):
        #åˆ†ç¦»æ–‡ä»¶ï¼ˆå›¾ç‰‡ï¼‰åä¸åç¼€ï¼Œåªæå–æ–‡ä»¶åï¼Œä¹Ÿå°±æ˜¯å›¾ç‰‡çš„åºå·
        #å†åœ¨lablesè¿™ä¸ªå­—å…¸é‡Œæ‹¿å‡ºå¯¹åº”æ ‡å·çš„ç±»å‹å
        label = labels[train_file.split('.')[0]]
        # æ–‡ä»¶å
        fname = os.path.join(data_dir, 'train', train_file)
        # å¤åˆ¶åˆ°è®­ç»ƒæ–‡ä»¶å¤¹
        copyfile(fname,os.path.join(data_dir, 'train_valid_test', 'train_valid', label))
        # å¦‚æœlabelä¸åœ¨label_counté‡Œæˆ–è¯¥labelæ•°é‡æœªè¾¾åˆ°æ¯ç±»éªŒè¯é›†è¦æ±‚
        if label not in label_count or label_count[label] < n_valid_per_label:
            # å¤åˆ¶åˆ°éªŒè¯é›†æ–‡ä»¶å¤¹
            copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'valid', label))
            #ç”¨å­—å…¸è®¡æ•°
            #dict.get(key, default=None),æŸ¥æ‰¾ç±»å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›é»˜è®¤å€¼
            #å€Ÿæ­¤èµ‹å€¼dict[key]
            #é€šè¿‡+1è®¡æ•°ï¼Œå¹¶é‡æ–°èµ‹å€¼
            label_count[label] = label_count.get(label, 0) + 1
        # å¦‚æœlabelåŒ…å«åœ¨label_count
        else:
            # å¤åˆ¶åˆ°è®­ç»ƒé›†æ–‡ä»¶å¤¹
            copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'train', label))
    return n_valid_per_label
```

- åœ¨é¢„æµ‹æœŸé—´æ•´ç†æµ‹è¯•é›†ï¼Œæ–¹ä¾¿è¯»å–

```python
# åˆ›å»ºæµ‹è¯•é›†ï¼Œå¹¶ä¸”åªæœ‰ä¸€ä¸ªunknownæ–‡ä»¶å¤¹ï¼Œè¡¨ç¤ºæ‰€æœ‰ç±»å‹å‡ä¸ºä¸å¯çŸ¥
def reorg_test(data_dir):
    for test_file in os.listdir(os.path.join(data_dir, 'test')):
        copyfile(
            os.path.join(data_dir, 'test', test_file),
            os.path.join(data_dir, 'train_valid_test','test','unkonwn'))
```

- è°ƒç”¨å‰é¢å®šä¹‰çš„å‡½æ•°

```python
def reorg_cifar10_data(data_dir, valid_ratio):
    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))
    reorg_train_valid(data_dir, labels, valid_ratio)
    reorg_test(data_dir)

batch_size = 32 if demo else 128
valid_ratio = 0.1
reorg_cifar10_data(data_dir, valid_ratio)
```

- å›¾åƒå¢å¹¿

```python
# å¯¹è®­ç»ƒé›†åšå¢å¹¿ï¼šresize -> éšæœºå¤§å°ç¼©æ”¾å’Œè£å‰ª -> æ°´å¹³ç¿»è½¬ -> å¼ é‡åŒ– -> å½’ä¸€åŒ–
transform_train = torchvision.transforms.Compose([
    torchvision.transforms.Resize(40),
    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),
                                                ratio=(1.0, 1.)),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],
                                     [0.2023, 0.1994, 0.2010])])

# å¯¹æµ‹è¯•é›†åšå¢å¹¿ï¼šå¼ é‡åŒ– -> å½’ä¸€åŒ–
transform_test = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],
                                     [0.2023, 0.1994, 0.2010])])
```

- è¯»å–ç”±åŸå§‹å›¾åƒç»„æˆçš„æ•°æ®é›†

```python
# ImageFolderæ˜¯ä¸€ä¸ªé€šç”¨çš„æ•°æ®é›†åŠ è½½å™¨ï¼Œä¹Ÿå°±æ˜¯å¯¹è‡ªå®šä¹‰çš„å›¾ç‰‡æ–‡ä»¶å¤¹
# æŠŠdata_dir/rain_valid_test/trainä½œä¸ºè®­ç»ƒé›†
# æŠŠdata_dir/rain_valid_test/train_validä½œä¸ºè®­ç»ƒéªŒè¯é›†
train_ds, train_valid_ds = [
    torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'train_valid_test', folder),
        transform=transform_train) for folder in ['train', 'train_valid']]

# æŠŠdata_dir/rain_valid_test/validä½œä¸ºéªŒè¯é›†
# æŠŠdata_dir/rain_valid_test/test/unknownä½œä¸ºæµ‹è¯•é›†
valid_ds, test_ds = [
    torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'train_valid_test', folder),
        transform=transform_test) for folder in ['valid', 'test']]
```

- å®šä¹‰æ•°æ®é›†è¿­ä»£å™¨

```python
# :drop_last=Trueè¡¨ç¤ºè£å»æœ€åä¸æ»¡batch_sizeçš„ä¸€æ‰¹æ•°æ®ï¼Œdefault=False
# ä¸ºå®ç°éšæœºSGDï¼Œå¿…é¡»shuffle=Trueï¼Œå¦åˆ™æ•°æ®é›†é¡ºåºä¹Ÿä¼šè¢«è¯¯å­¦
train_iter, train_valid_iter = [
    torch.utils.data.DataLoader(
        dataset, batch_size, shuffle=True, drop_last=True)
    for dataset in (train_ds, train_valid_ds)]

valid_iter = torch.utils.data.DataLoader(
    valid_ds, batch_size, shuffle=False, drop_last=True)

test_iter =  torch.utils.data.DataLoader(test_ds, batch_size, shuffle=True,
                                         drop_last=False)
```

- å®šä¹‰æ¨¡å‹&æŸå¤±å‡½æ•°

```python
def get_net():
    num_classes = 10
    # :num_classes ç±»åˆ«æ•°ï¼Œ3ä¸ºRGBé€šé“æ•°
    net = d2l.resnet18(num_classes, 3)
    return net

loss = nn.CrossEntropyLoss(reduction='none')
```

- å®šä¹‰è®­ç»ƒè¿‡ç¨‹

```python
def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices,
          lr_period, lr_decay):
    # momentumä½¿ç”¨æŒ‡æ•°åŠ æƒå¹³å‡ä¹‹åçš„æ¢¯åº¦ä»£æ›¿åŸæ¢¯åº¦è¿›è¡Œå‚æ•°æ›´æ–°ã€‚
    # å› ä¸ºæ¯ä¸ªæŒ‡æ•°åŠ æƒå¹³å‡åçš„æ¢¯åº¦å«æœ‰ä¹‹å‰æ¢¯åº¦çš„ä¿¡æ¯ï¼ŒåŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•å› æ­¤å¾—åã€‚
    trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,
                              weight_decay=wd)
    # ä½¿ç”¨è°ƒåº¦å™¨æ¥å¯ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡
    # æ¯éš”lr_periodä¸ªepoch, å­¦ä¹ ç‡ä¸‹é™lr * lr_decay
    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)
    # ä½œå›¾ç›¸å…³
    num_batches, timer = len(train_iter), d2l.Timer()
    legend = ['train loss', 'train acc']
    if valid_iter is not None:
        legend.append('valid acc')
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=legend)
    # æ•°æ®å¹¶è¡Œ
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])
    for epoch in range(num_epochs):
        net.train()
        metric = d2l.Accumulator(3) # loss, accuracy, numel
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            l, acc = d2l.train_batch_ch13(net, features, labels, loss, trainer, devices)
            metric.add(l, acc, labels.shape[0])
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches, (metric[0] / metric[2], metric[1] / metric[2], None))
        if valid_iter is not None:
            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)
            animator.add(epoch + 1, (None, None, valid_acc))
        # ä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡åï¼ŒåŸå…ˆçš„`optimizer.step()`å†™æ³•
        # éœ€æ›¿æ¢ä¸º`scheduler.step()`
        scheduler.step()

    measures = (f'train loss {metric[0] / metric[2]:.3f}, '
                f'train acc {metric[1] / metric[2]:.3f}')
    if valid_iter is not None:
        measures += f', valid acc {valid_acc:.3f}'
    print(measures + f'\n{metric[2] * num_epochs / timer.sum():.1f}'
          f' examples/sec on {str(devices)}')
```

- è®­ç»ƒå’ŒéªŒè¯æ¨¡å‹

```python
devices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 2e-4, 5e-4
lr_period, lr_decay, net = 4, 0.9, get_net()
train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
      lr_decay)
# Out:
# train loss 0.685, train acc 0.751, valid acc 0.359
# 804.0 examples/sec on [device(type='cuda', index=0), device(type='cuda', index=1)]
```

![output_kaggle-cifar10](https://zh.d2l.ai/_images/output_kaggle-cifar10_42a34e_129_1.svg)

- åœ¨ Kaggle ä¸Šå¯¹æµ‹è¯•é›†è¿›è¡Œåˆ†ç±»å¹¶æäº¤ç»“æœ

```python
net, preds = get_net(), []
train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,
      lr_decay)

for X, _ in test_iter:
    y_hat = net(X.to(devices[0]))
    preds.extend(y_hat.argmax(dim=1).type(torch.int32).cpu().numpy())
sorted_ids = list(range(1, len(test_ds) + 1))
sorted_ids.sort(key=lambda x: str(x))
df = pd.DataFrame({'id': sorted_ids, 'label': preds})
df['label'] = df['label'].apply(lambda x: train_valid_ds.classes[x])
df.to_csv('submission.csv', index=False)
# Out:
# train loss 0.684, train acc 0.759
# 1052.1 examples/sec on [device(type='cuda', index=0), device(type='cuda', index=1)]
```

![output_kaggle-cifar10](https://zh.d2l.ai/_images/output_kaggle-cifar10_42a34e_138_1.svg)

## Python æ¨¡å—å‚è€ƒæ–‡æ¡£

- `shutil` Python é«˜é˜¶æ–‡ä»¶æ“ä½œåŒ… ğŸ§[å®˜æ–¹ä¸­æ–‡](https://docs.python.org/zh-cn/3/library/shutil.html)
- `torchvision.datasets.ImageFolder` torchvison ä»æ–‡ä»¶å¤¹å†…æ„é€ æ•°æ®é›†æ–¹æ³• ğŸ§[ä¸­æ–‡](https://pytorch-cn.readthedocs.io/zh/latest/torchvision/torchvision-datasets/#imagefolder) | [å®˜æ–¹è‹±æ–‡](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.DatasetFolder)
- `torch.optim.lr_scheduler` Pytorch è‡ªé€‚åº”å­¦ä¹ ç‡ç›¸å…³æ–‡æ¡£ ğŸ§[å®˜æ–¹è‹±æ–‡](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)

---

## Q&AğŸ¤“

**Qï¼šæ·±åº¦å­¦ä¹ çš„æŸå¤±å‡½æ•°ä¸€èˆ¬æ˜¯éå‡¸çš„å—ï¼Ÿ**

**ğŸ™‹â€â™‚ï¸**ï¼šä¸€èˆ¬æŸå¤±å‡½æ•°çš„æ•°å­¦å½¢å¼ï¼ˆå¦‚äº¤å‰ç†µæŸå¤±å‡½æ•°ã€çº¿æ€§å›å½’çš„æœ€å°äºŒä¹˜æ³•ç­‰ï¼‰æ˜¯å‡¸å‡½æ•°ï¼Œä½†æœ‰éšè—å±‚å’Œæ¿€æ´»å‡½æ•°çš„ç¥ç»ç½‘ç»œçš„æ•°å­¦å½¢å¼éƒ½æ˜¯éå‡¸çš„ï¼Œå¸¦æ¥å…¶æŸå¤±å‡½æ•°çš„æ±‚è§£å°±æ˜¯éå‡¸ä¼˜åŒ–é—®é¢˜ã€‚ä½†åªè¿½æ±‚å‡¸å‡½æ•°æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œå‡¸å‡½æ•°çš„è¡¨ç¤ºèƒ½åŠ›æœ‰é™ï¼Œä¸èƒ½æ‹Ÿåˆå¤æ‚é—®é¢˜ã€‚
