# 08 - 线性回归

---

### 🎦 本节课程视频地址 👉[Bilibil](https://www.bilibili.com/video/BV1PX4y1g7KC)

## 1.回归（Regression） 
在机器学习领域，大致可分为**回归**与**分类**两类问题。

回归是指一类为一个或多个自变量与因变量之间关系建模的方法。在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系；在现实生活中，回归常用于解决**预测**问题。

**线性回归**（Linear Regression）是最简单的一种回归模型，在对结果精度要求不高、现实情况相对不复杂的情况（假设满足线性关系），使用线性回归可以简便快速地得到可接受的结果。

线性回归模型可以等价看作是激活函数为线性函数$\sigma=wx$的**单层神经网络**模型。其中输入层**节点数量**等于数据**特征个数**：

![单层神经网络](http://zh.d2l.ai/_images/singleneuron.svg)

**Ep: 房价预测**
- 预测模型
输出 = 输入的加权和 + 标量偏差
$$
y = <{\bf{w},\bf{x}}>+b=
w_1x_1+w_2x_2+...+w_nx_n+b
$$
线性模型可以看作单层神经网络，带权重的层为1。
- 衡量预估质量——通过比较真实值和预估值。
常用平方损失：
$$
l(y,\hat{y})={1\over2}{(y-\hat{y})^2}
$$
式中的$1\over2$方便求导。
- 训练数据
收集一些数据点来决定参数（权重、偏差），称之为训练数据，多多益善。
$$
{\bf{X}}=[{\bf{x_1}},{\bf{x_2}},{\bf{x_3}},...,{\bf{x_n}}]^T
$$
$$
{\bf{y}}=[y_1,y_2,y_3,...,y_n]^T
$$
- 参数学习
训练损失
$$
l({\bf{X}},{\bf{y}},{\bf{w}},b)={1\over2n}\sum_{i=1}^n(y_i-<{\bf{w}},{\bf{x_i}}>-b)^2={1\over2n}||{\bf{y}}-{\bf{X}}{\bf{w}}-b||^2
$$
最小化损失来学习参数
$$
{\bf{w}}^*,{\bf{b}}^*=arg\,\min_{{\bf{w}},b}l({\bf{X}},{\bf{y}},{\bf{w}},b)
$$
- 显示解
  将偏差加入权重
  $$
  {\bf{X}}\leftarrow[{\bf{X}},{\bf{1}}]
  $$
  $$
  {\bf{w}}\leftarrow\begin{bmatrix} {\bf{w}}\\ b\\ \end{bmatrix}
  $$
  $$
  l({\bf{X}},{\bf{y}},{\bf{w}})={1\over2n}||{\bf{y}}-{\bf{X}}{\bf{w}}||^2
  $$
  $$
  {\partial\over\partial{\bf{w}}}l({\bf{X}},{\bf{y}},{\bf{w}})={1\over n}({\bf{y}}-{\bf{X}}{\bf{w}})^T{\bf{X}}
  $$
  损失是凸函数，最优解满足
$$
{\partial\over\partial{\bf{w}}}l({\bf{X}},{\bf{y}},{\bf{w}})=0
$$
$$
{1\over n}({\bf{y}}-{\bf{X}}{\bf{w}})^T{\bf{X}}=0
$$
$$
{\bf{w}}^*=({\bf{X}}^T{\bf{X}})^{-1}{\bf{X}}{\bf{y}}
$$
线性回归是对n维数据的加权，用平方损失来预测实值的误差，有显示解，可以看作单层神经网络。
## 2.优化方法
1. 梯度下降
如果一个模型没有显示解，就需要借助数值方法。
挑选一个损失值${\bf{w_0}}$，重复迭代$t=1,2,3...$
$${\bf{w_t}}={\bf{w_t}}-\eta{\partial\over\partial{\bf{w_{t-1}}}}l$$
学习率（$\eta$）：步长的超参数（hyperparameter）,不能太小（花费高）也不能太大（震荡）。
沿梯度方向将增加损失函数值。
1. 小批量随机梯度下降 **(SGD)**
在整个训练集上算梯度实在太贵
可以随机采样$b$个样本$i_1,i_2,...,i_b$来近似损失
$${1\over b}\sum_{i\in I_b}^nl({\bf{x_i}},y_i,{\bf{w}})$$
$b$为批量大小，另外一个重要的超参数。
梯度下降就是不断延着反梯度方向更新求解，不需要了解显示解的形式，只要可导。
##3. 线性回归的代码实现
Eg：以线性噪声为例
- 构造数据集
假设初始 ${\bf{w}}=[2,-.3.4]^T$，$b=4.2$，定义线性回归函数。
$y={\bf{X}}{\bf{w}}+b+\epsilon$
```
def synthetic_data(w, b, num_examples):
    x = torch.normal(0, 1, (num_examples, len(w)))
    ## 从平均值为0，方差为1的正态分布随机创造num_examples个（这里是1000）张量，其长度与w相同，输出是一个1000×2的矩阵。
    y = torch.matmul(X, w) + b
    ## mutmul()是矩阵乘法。
    y += torch.normal(0, 0.01, y.shape)
    ## 添加个随机扰动
    return X, y.reshape(-1, 1)

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
## features特征，表示已有的数据集，labels是真值。
```
- 每次读取一个小批量

```
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    ## 得到一个与特征等长的序列
    random.shuffle(indices)
    ## 打乱该序列
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples])
        yield features[batch_indices], labels[batch_indices]
    ## 确定批次量作为步长，这里取10，生成器每运行一次，就会创造出一个长度为10的张量。

batch_size = 10

for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
    ## break 用于中断生成器，要么就会生成100组。
```
- 定义模型
```
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
## 设置初值。

def linreg(X, w, b):
    return torch.matmul(X, w) + b
## 定义模型
```
- 定义损失函数
```
def squared_loss(y_hat, y):
    return(y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```
- 定义优化算法
```
def sgd(params, lr, batch_size):
    with torch.no_grad():
    ## 求导不在此处。
         for param in params:
         ## 传入w, b的列表。
            param -= lr * param.grad / batch_size
            param.grad.zero_()
            ## pyTorch需要手动梯度清零
    ## 对于列表对象，函数中改变就会改变全局变量，故不须return
```
- 训练
  ```
  lr = 0.03
  num_epochs = 3
  net = linreg
  loss = squared_loss
  ## 定义超参数

  for epoch in range(num_epochs):
  ## 打乱一遍，出100组，一共打乱三遍。
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)
        l.sum().backward()
        ## 前面定义了b, w的grad_requires=True
        sgd([w, b], lr, batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        ## 显示每遍历完一遍后的最终损失。
        print('output:')

  ```
##线性回归的简单实现
```
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l

## 创建数据集
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

## 创建批量
def load_array(data_arrays, batch_size, is_train=True):
    dataset = data.TensorDataset(*data_arrays)
    ## 把X,y连成列表作为数据集
    return data.DataLoader(dataset, batch_size, shuffle=is_train)
    ## DataLoader()函数表示提取一个批次。

batch_size = 10
data_iter = load_array((features, labels), batch_size)

next(iter(data_iter))


from torch import nn
net = nn.Sequential(nn.Linear(2, 1))
## Linear()线性回归，输入输出的维度。Sequential()表示List of Layers

net[0].weight.data.normal_(0, 0.01)
## 设置权重参数w的初始值。
net[0].bias.data.fill_(0)

## 调入损失函数
loss = nn.MSELoss()

## 调入优化方法
trainer = torch.optim.SGD(net.parameters(), lr=0.03)
## parameters是network里所有的参数，包括w和b。

## 训练
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X), y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {1:f}')
```
Q1：怎么把图插入笔记，如果是本地图片是否会被远程调用？

