# 15 - KaggleåŠ å·æˆ¿ä»·é¢„æµ‹å®æˆ˜

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰
[![Bilibil](https://i0.hdslb.com/bfs/archive/d66e2524575703d66340b8fd486793523fc5c32f.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1NK4y1P7Tu?spm_id_from=333.999.0.0)

**å‡†å¤‡å·¥ä½œâ€”â€”å¯¼å…¥åŒ…/ä¸‹è½½æ•°æ®**
```
import hashlib 
#æŠŠä»»æ„é•¿åº¦çš„è¾“å…¥ï¼Œé€šè¿‡æŸç§hashç®—æ³•ï¼Œå˜æ¢æˆå›ºå®šé•¿åº¦çš„è¾“å‡ºï¼Œ
#ä¸»è¦ç”¨æ¥åŠ å¯†å’Œè§£å¯†ã€‚
import os
import tarfile #TarFileç±»å¯¹äºå°±æ˜¯tarå‹ç¼©åŒ…å®ä¾‹
import zipfile
import requests

DATA_HUB = dict()
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'

def download(name, cache_dir=os.path.join('..', 'data')):  
    #os.path.join()è·¯å¾„æ‹¼æ¥ï¼Œç›¸å½“äº../dataæ–‡ä»¶å¤¹ã€‚
    assert name in DATA_HUB, f"{name} ä¸å­˜åœ¨äº {DATA_HUB}"
    url, sha1_hash = DATA_HUB[name]
    os.makedirs(cache_dir, exist_ok=True)
    #os.makedirs()yç”¨äºåˆ›å»ºç›®å½•
    #exist_ok=Trueï¼Œé»˜è®¤ä¸ºFalse,ç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨ä¼šæŠ¥é”™
    fname = os.path.join(cache_dir, url.split('/')[-1])
    #æŠŠurlæŒ‰ç…§/æ‹†åˆ†ï¼Œæå–æœ€åä¸€ä¸ªå­—ç¬¦ä¸²è¿æ¥ç›®å½•ï¼Œä½œä¸ºæ–‡ä»¶å
    if os.path.exists(fname):
        #os.path.exists()æ£€æµ‹ç›®å½•/æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        sha1 = hashlib.sha1()
        #sha1ç®—æ³•åŠ å¯†
        with open(fname, 'rb') as f:
            #è¯»å–äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä¸€èˆ¬ç”¨äºéæ–‡æœ¬æ–‡ä»¶å¦‚å›¾ç‰‡ç­‰ã€‚
            while True:
                data = f.read(1048576)
                #file.read([size])æŒ‡å®šä¸€æ¬¡æœ€å¤šå¯è¯»å–çš„å­—ç¬¦ï¼ˆå­—èŠ‚ï¼‰ä¸ªæ•°
                #é»˜è®¤è¯»å–æ‰€æœ‰å†…å®¹ã€‚
                #size=1048576=1024*1024,æ¯æ¬¡çš„è¯»å–1Mbã€‚
                if not data:
                #å¦‚è¿‡è¿”å›ç©º
                    break
                sha1.update(data)
                #update()åˆå¹¶ä¸è¦†ç›–å½“å‰å†…å®¹
        if sha1.hexdigest() == sha1_hash:
            #æŠŠæ‰€æœ‰å†…å®¹åŠ å¯†åä¸æ–‡ä»¶å¯¹æ¯”ï¼Œ
            #å¦‚æœå¯†ç ç›¸åŒï¼Œåˆ™è¿”å›æ–‡ä»¶å
            return fname  # å‘½ä¸­ç¼“å­˜
    print(f'æ­£åœ¨ä»{url}ä¸‹è½½{fname}...')
    r = requests.get(url, stream=True, verify=True)
    #ä»é“¾æ¥è·å–å†…å®¹
    with open(fname, 'wb') as f:
        #å› ä¸ºfnameæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶ã€‚
        #å†™äºŒè¿›åˆ¶æ–‡ä»¶
        f.write(r.content)
        #å†™å…¥é“¾æ¥å†…å®¹
    return fname
```
```
def download_extract(name, folder=None):
    fname = download(name)
    base_dir = os.path.dirname(fname)
    #å»æ‰æ–‡ä»¶åï¼Œè¿”å›ç›®å½•
    data_dir, ext = os.path.splitext(fname)
    #åˆ†ç¦»æ–‡ä»¶åä¸æ‰©å±•å
    if ext == '.zip':
        fp = zipfile.ZipFile(fname, 'r')
        #è¯»å–å‹ç¼©æ–‡ä»¶
    elif ext in ('tar', '.gz'):
        fp = tarfile.open(fname, 'r')
    else:
        assert False
    fp.extractall(base_dir)
    #extractall()å³è§£å‹ç¼©ã€‚
    return os.path.join(base_dir, folder) if folder else data_dir
    #è¦ä¹ˆè¿”å›joinçš„è·¯å¾„ï¼Œè¦ä¹ˆè¿”å›baseçš„è·¯å¾„

def download_all():
    for name in DATA_HUB:
        download(name)
```
```
%matplotlib inline
import numpy as np
import pandas as pd
import torch
from torch import nn
from d2l import torch as d2l

DATA_HUB['kaggle_house_train'] = (  
    DATA_URL + 'kaggle_house_pred_train.csv',
    '585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (  
    DATA_URL + 'kaggle_house_pred_test.csv',
    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

train_data = pd.read_csv(download('kaggle_house_train'))
test_data = pd.read_csv(download('kaggle_house_test'))
```
```
'''
ç°åœ¨æ¥çœ‹ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ï¼š
DATA_HUB={'kaggle_house_train': ('http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_test.csv','fa19780a7b011d9b009e8bff8e99922a8ee2eb90')}
download(name = 'kaggle_house_train')
url, sha1_hash = ('http://d2l-data.s3-accelerate.amazonaws.com/kaggle_house_pred_test.csv','fa19780a7b011d9b009e8bff8e99922a8ee2eb90')
os.makedirs(cache_dir, exist_ok=True):åˆ›å»ºç›®å½•"../data"
fname = os.path.join(cache_dir, url.split('/')[-1])
fname = "../data/kaggle_house_pred_test.csv"
èµ·åˆå¹¶ä¸å­˜åœ¨ï¼Œæ‰€ä»¥è¦å…ˆä¸‹è½½ï¼š
print(f'æ­£åœ¨ä»{url}ä¸‹è½½{fname}...')
r = requests.get(url, stream=True, verify=True)
with open(fname, 'wb') as f:
        f.write(r.content)
åˆ›å»ºå¹¶å†™å…¥æ•°æ®ã€‚
å¦‚æœå·²ç»ä¸‹è½½è¿‡äº†ï¼šé€šè¿‡Whileå¾ªç¯ï¼ŒæŠŠæ–‡ä»¶çš„æ‰€æœ‰æ•°æ®åŠ å¯†ï¼Œè¿”å›Hash
if sha1.hexdigest() == sha1_hash=='fa19780a7b011d9b009e8bff8e99922a8ee2eb90':
è¡¨ç¤ºå·²å­˜åœ¨æ–‡ä»¶ä¸ç›®æ ‡æ–‡ä»¶å†…å®¹ä¸€è‡´ã€‚
æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„ï¼šåŠ å¯†è¿”å›å€¼çš„é•¿çŸ­å¹¶ä¸æ˜¯ä¸å­—ç¬¦ä¸²æˆæ­£æ¯”ï¼Œè€Œæ˜¯ä¸€ä¸ªå›ºå®šå€¼ã€‚
'''
```
**æ•°æ®é¢„å¤„ç†**

```
all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:-1]))
#é»˜è®¤æŒ‰åˆ—è¿æ¥
#åˆ¨å»ç¬¬ä¸€åˆ—ID,å’Œæœ€åä¸€åˆ—labels

#å¯¹æ•°å€¼åˆ—è¿›è¡Œnormalization
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index
#åœ¨pandasé‡Œï¼Œç”¨objectè¡¨ç¤ºstringã€‚
all_features[numeric_features] = all_features[numeric_features].apply(
lambda x: (x - x.mean()) / (x.std()))
all_features[numeric_features] = all_features[numeric_features].fillna(0)

all_features = pd.get_dummies(all_features, dummy_na=True)
all_features.shape
#get_dummies()ä½¿one-hotç¼–ç ï¼ŒæŠŠå­—ç¬¦ä¸²æ•°æ®è½¬æ¢ä¸º0-1ï¼›
#æ¯åˆ—æœ‰nä¸ªä¸åŒçš„å­—ç¬¦ä¸²ï¼Œé‚£ä¹ˆç‹¬çƒ­å‘é‡çš„é•¿åº¦å°±ä¸ºnï¼›
#å› æ­¤dfçš„åˆ—æ•°å¤§å¢
```
**ç½‘ç»œç®—æ³•**

```
#å®šä¹‰è¾“å…¥è¾“å‡º
n_train = train_data.shape[0] #1460
train_features = torch.tensor(all_features[:n_train].values,
                                         dtype=torch.float32)
#æ²¡æœ‰ilocçš„åˆ‡ç‰‡ï¼Œé»˜è®¤å¯¹è¡Œï¼›
#å–æ¯åˆ—å»æ ‡é¢˜è¡Œå‰n_train=1460è¡Œã€‚1460*331
test_features = torch.tensor(all_features[n_train:].values,
                                         dtype=torch.float32)
#1459*331
train_labels = torch.tensor(train_data.SalePrice.values.reshape(-1, 1),
                           dtype=torch.float32)
#1460*1

#æŸå¤±å‡½æ•°å’Œç¥ç»ç½‘ç»œ
loss = nn.MSELoss()
in_features = train_features.shape[1]

def get_net():
    net = nn.Sequential(nn.Linear(in_features, 1))
    return net

#normalizeæŸå¤±
def log_rmse(net, features, labels):
    clipped_preds = torch.clamp(net(features), 1, float('inf'))
    #clamp(x, min, max)ï¼Œé™åˆ¶çš„å€¼åœ¨[min, max]ä¹‹é—´ï¼Œè¶…å‡ºå–æœ€å¤–å€¼ã€‚
    #æŠŠå°äº1çš„å€¼å˜ä¸º1
    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))
    #Lossæ±‚å‡æ–¹æŸå¤±ï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡
    return rmse.item()
#tensor.item()=number
#Returns the value of this tensor as a standard Python number.
#This only works for tensors with one element. 

#è®­ç»ƒå‡½æ•°
def train(net, train_features, train_labels, test_features, test_labels,
          num_epochs, learning_rate, weight_decay, batch_size):
    train_ls, test_ls = [], []
    train_iter = d2l.load_array((train_features, train_labels), batch_size)
    optimizer = torch.optim.Adam(net.parameters(), 
                                 lr=learning_rate,
                                weight_decay=weight_decay)
    for epoch in range(num_epochs):
        for X, y in train_iter:
            optimizer.zero_grad()
            l = loss(net(X), y)
            l.backward()
            optimizer.step()
        train_ls.append(log_rmse(net, train_features, train_labels))
        #æ¯ä¸ªepochè®¡ç®—æ¯æ¬¡çš„lossså¹¶æ·»åŠ åˆ°åˆ—è¡¨
        if test_labels is not None:
            test_ls.append(log_rmse(net, test_features, test_labels))
        #æ¯ä¸ªepochè®¡ç®—æ¯æ¬¡çš„lossså¹¶æ·»åŠ åˆ°åˆ—è¡¨
    return train_ls, test_ls
    #train_lsï¼Œtest_lså‡ºå…¥çš„å°±æ˜¯ 1*n=epochs çš„åˆ—è¡¨

#K-æŠ˜äº¤å‰éªŒè¯æ•°æ®é›†çš„å®šä¹‰
def get_k_fold_data(k, i, X, y):
    assert k > 1
    fold_size = X.shape[0] // k
    # zæ•´é™¤
    X_train, y_train = None, None
    for j in range(k):
        idx = slice(j * fold_size, (j + 1) * fold_size)
        #slice()è¿”å›åˆ‡ç‰‡
        X_part, y_part = X[idx, :], y[idx]
        if j == i:
            X_valid, y_valid = X_part, y_part
            #validæ˜¯éªŒè¯é›†
        elif X_train is None:
            X_train, y_train = X_part, y_part
        else:
            X_train = torch.cat([X_train, X_part], 0)
            y_train = torch.cat([y_train, y_part], 0)
            #torch.cat(tensors, dim=0) 
    return X_train, y_train, X_valid, y_valid
#è¿”å›çš„æ˜¯ç¬¬iä¸ªæŠ˜çš„è®­ç»ƒã€æµ‹è¯•é›†

#å®šä¹‰è®­ç»ƒè¿‡ç¨‹
def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size):
    train_l_sum, valid_l_sum = 0, 0
    for i in range(k):
        data = get_k_fold_data(k, i, X_train, y_train)
        net = get_net()
        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,
                                  weight_decay, batch_size)
        #*dataè¡¨ç¤ºæŠŠå…¶è¿”å›å€¼å…ƒç»„(X_train, y_train, X_valid, y_valid)ä¼ å‚
        #iåœ¨epochä¹‹å‰ï¼Œè¡¨ç¤ºå¯¹äºæ¯ä¸€æŠ˜ï¼Œè¿›è¡Œepochsæ¬¡è®­ç»ƒ
        train_l_sum += train_ls[-1]
        valid_l_sum += valid_ls[-1]
        #æŠŠæœ€åä¸€ä¸ªepochçš„æŸå¤±æå‡ºæ¥ï¼Œä¹Ÿå°±æ˜¯æœ€ç²¾ç¡®çš„ä¸€ç»„
        if i == 0:
            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],
                    xlabel='epoch', ylabel='rmse', xlim=[1, num_epochs],
                    legend=['train', 'valid'], yscale='log')
            #åªç”»ç¬¬ä¸€æŠ˜çš„å›¾
        print(f'æŠ˜{i + 1}ï¼Œè®­ç»ƒlog rmse{float(train_ls[-1]):f},'
             f'éªŒè¯log rmse{float(valid_ls[-1]):f}')
    return train_l_sum /k, valid_l_sum / k
    #æ±‚æ‰€æœ‰æŠ˜çš„å¹³å‡æŸå¤±

k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64
train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,
                         weight_decay, batch_size)
#X_train: train_features.shape=(1459, 331), y_train: train_labels.shape=(1459, 1), k=5, batch_size=64
#data = get_k_fold_data(k, i, X_train, y_train)
#fold_size=1459//5=291
#i=0, X_part, y_part = X[(0:291), :], y[0:291]=X_valid, y_valid; X_train, y_train=X[(291:), :], y[291:]

print(f'{k}-æŠ˜éªŒè¯ï¼šå¹³å‡è®­ç»ƒlog rmse:{float(train_l):f},'
     f'å¹³å‡éªŒè¯log rmse: {float(valid_l):f}')#è¡¨ç¤ºå°æ•°å®šä¹‰ç²¾åº¦ï¼Œé»˜è®¤å…­ä½æµ®ç‚¹æ•°
#f'{}'æ˜¯{}.format()çš„ç®€æ˜“å†™æ³•
```
![](\Images/å¾®ä¿¡æˆªå›¾_20211221142847.png)

å®é™…ä¸Šåœ¨å‰æ–¹çš„K-æŠ˜äº¤å‰éªŒè¯é‡Œå¹¶æ²¡æœ‰ç”¨åˆ°æµ‹è¯•é›†ã€‚
**é¢„æµ‹æµ‹è¯•é›†å¹¶ä¿å­˜è¾“å‡º**
```
def train_and_pred(train_features, test_feature, train_labels, test_data,
                  num_epochs, lr, weight_decay, batch_size):
    net = get_net()
    train_ls, _ = train(net, train_features, train_labels, None, None,
                       num_epochs, lr, weight_decay, batch_size)
    #ä¼ å‡ºçš„æ˜¯æ¯ä¸ªepochçš„lossï¼Œè€Œtest_ls=None
    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel = 'epoch',
            ylabel='log rmse', xlim=[1, num_epochs], yscale='log')
    print(f'train log rmse {float(train_ls[-1]):f}')
    #è¾“å‡ºæœ€åä¸€ä¸ªepochçš„å€¼ã€‚
    preds = net(test_features).detach().numpy()
    #netè¿”å›é¢„æµ‹çš„test_labels
    #detach()ä½¿æ¢¯åº¦è®¡ç®—å°±æ­¤æ‰“ä½
    #numpy()è¿”å›æ•°ç»„
    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])
    #(1, -1)æ’åˆ—ä¸ºä¸€è¡Œï¼Œä½†æ˜¯è§„å®šæ˜¯äºŒç»´å¼ é‡ï¼Œå°½ç®¡è¡Œä¸º1.
    #[0]å–ç¬¬ä¸€è¡Œï¼Œä¸€ä¸ªä¸€ç»´è¡Œå‘é‡
    #pd.Series()è½¬åŒ–ä¸ºä¸€ä¸ªdataframe
    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)
    #æŠŠ['Id']åˆ—å’Œ['labels']ä»¥è¡Œç›¸è¿ã€‚
    submission.to_csv('submission.csv', index=False)
    #ä¿å­˜è·¯å¾„ä¸º./submission.csv
    #index=Falseï¼Œå°±ä¸ä¿å­˜ç´¢å¼•åˆ—
train_and_pred(train_features, test_features, train_labels, test_data,
              num_epochs, lr, weight_decay, batch_size)
```
![](/Images/å¾®ä¿¡æˆªå›¾_20211221174519.png)