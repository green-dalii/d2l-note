# 42 - è¯­è¨€æ¨¡å‹

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰
[![Bilibil](https://i1.hdslb.com/bfs/archive/7831a88cef7a4f169648bd21e8b1cb7fe0ca104d.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1ZX4y1F7K3)
## è¯­è¨€æ¨¡å‹

- ç»™å®šæ–‡æœ¬åºåˆ— $x_1,...,x_T$ï¼Œè¯­è¨€æ¨¡å‹çš„ç›®æ ‡æ˜¯ä¼°è®¡è”åˆæ¦‚ç‡ $p(x_1,...,x_T)$
- å®ƒçš„åº”ç”¨åŒ…æ‹¬
  - åšé¢„è®­ç»ƒæ¨¡å‹(eg BERT, GPT-3)
  - ç”Ÿæˆæœ¬æ–‡ï¼Œç»™å®šå‰é¢å‡ ä¸ªè¯ï¼Œä¸æ–­åœ°ä½¿ç”¨ $x_t~p(x_t|x_1,...,x_{t-1})$ï¼Œç”Ÿæˆåç»­æ–‡æœ¬
  - åˆ¤æ–­å¤šä¸ªåºåˆ—ä¸­å“ªä¸ªæ›´å¸¸è§ï¼Œe.g. "to recognize a speech" vs "to wreck a nice beach"   
    - è¯­éŸ³æ¨¡å‹è¿”å›è¿™ä¸¤ä¸ªå¯èƒ½ï¼Œè¯­è¨€æ¨¡å‹é¢„æµ‹å…¶å¯èƒ½æ€§(åˆç†æ€§)ã€‚

### ä½¿ç”¨è®¡æ•°æ¥å»ºæ¨¡
- å‡è®¾åºåˆ—é•¿åº¦ä¸º2ï¼Œæˆ‘ä»¬é¢„æµ‹
$p(x,x')=p(x)p(x'|x)={n(x)\over n}{n(x,x')\over n(x)}$
  - $n$ æ˜¯æ€»è¯æ•°ï¼Œ $n(x),n(x,x')$ æ˜¯å•ä¸ªå•è¯å’Œè¿ç»­å•è¯å¯¹ï¼ˆä¸€å‰ä¸€åï¼‰çš„å‡ºç°æ¬¡æ•°
- å¾ˆå®¹æ˜“æ‹“å±•åˆ°é•¿ä¸º3çš„æƒ…å†µ
$p(x,x',x'')=p(x)p(x'|x)p(x''|x,x')={n(x)\over n}{n(x,x')\over n(x)}{n(x,x',x'')\over n(x,x')}$
- æ‰«ä¸€éæ•´ä¸ªæ–‡æœ¬ï¼Œçœ‹è¯¥åºåˆ—å‡ºç°çš„æ¬¡æ•°

**Nå…ƒè¯­æ³•**

- å½“åºåˆ—å¾ˆé•¿æ—¶ï¼Œå› ä¸ºæ–‡æœ¬é‡ä¸å¤Ÿå¤§ï¼Œå¾ˆå¯èƒ½ $n(x_1,...,x_T)\le1$
- ä½¿ç”¨é©¬å°”ç§‘å¤«å‡è®¾å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜
  - ä¸€å…ƒè¯­æ³•ï¼š$\tau=1$
  $$\begin{split}
  p(x_1,x_2,x_3,x_4)&=p(x_1)p(x_2)p(x_3)p(x_4)\\
  &={n(x_1)\over n}{n(x_2)\over n}{n(x_3)\over n}{n(x_4)\over n}
  \end{split}$$
  - äºŒå…ƒè¯­æ³•ï¼š$\tau=2$
  $$\begin{split}
  p(x_1,x_2,x_3,x_4)&=p(x_1)p(x_2|x_1)p(x_3|x_2)p(x_4|x_3)\\
  &={n(x_1)\over n}{n(x_1,x_2)\over n}{n(x_2,x_3)\over n}{n(x_3,x_4)\over n}
  \end{split}$$
  - ä¸‰å…ƒè¯­æ³•ï¼š $\tau=3$
  $$\begin{split}
  p(x_1,x_2,x_3,x_4)&=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)p(x_4|x_2,x_3)\\
  &={n(x_1)\over n}{n(x_1,x_2)\over n}{n(x_1,x_2,x_3)\over n}{n(x_2,x_3,x_4)\over n}
  \end{split}$$

  è¿™ä¸ªæ¨¡å‹çš„ä¼˜åŠ¿æ˜¯ä¸éœ€è¦æŠŠæ–‡æœ¬ä¸­ä»»ä½•é•¿åº¦çš„æ–‡æœ¬éƒ½æ‰«æå‡ºæ¥ï¼Œåªéœ€è¦çœ‹å­åºåˆ—ã€‚å¹¶ä¸”å¯ä»¥å­˜å‚¨ä¸€å®šé•¿åº¦åºåˆ—çš„æ¦‚ç‡ã€‚
  å¤æ‚åº¦ä¸ $n$ å…ƒè¯­æ³•æ˜¯æŒ‡æ•°å…³ç³»ã€‚
  eg: äºŒå…ƒï¼Œ$m$ ä¸ªtokenï¼Œ å°±éœ€è¦ $m+m^2$ä¸ªå­åºåˆ—

### æ€»ç»“

- è¯­è¨€æ¨¡å‹ä¼°è®¡æ–‡æœ¬åºåˆ—çš„è”åˆæ¦‚ç‡
- ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ—¶å¸¸ç”¨ $n$ å…ƒè¯­æ³•

### ä»£ç å®ç°

**stop words**

æŒ‡è‡ªç„¶è¯­è¨€å¤„ç†å½“ä¸­ä¼šè¢«è¿‡æ»¤æ‰çš„ä¸€äº›å•è¯ï¼Œä¸€èˆ¬æ˜¯æŒ‡æ— æ„ä¹‰çš„å®šå† è¯ï¼Œä¸å®šå† è¯ï¼ˆa,an,theï¼‰, è¿æ¥è¯ï¼ˆof,but...ï¼‰

**è§‚æµ‹æ•°æ®**

```
import random
import torch
from d2l import torch as d2l

tokens = d2l.tokenize(d2l.read_time_machine())
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
#æ’è¿‡åºçš„
vocab.token_freqs[:10]

freqs = [freq for token, freq in vocab.token_freqs]
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
         xscale='log', yscale='log')
#å•è¯æŒ‰æ ‡å·æ’åºï¼Œä½œä¸ºxè½´
#è¯­è¨€åºåˆ—ä¹Ÿç¬¦åˆäºŒå…«å®šå¾‹
#å‡ºç°åæ¬¡çš„è¯ä¸è¶…è¿‡äºŒä¸‰ç™¾ï¼Œå‰©ä¸‹çš„åƒä½™è¯å‡ºç°é¢‘ç‡å¾ˆå°‘
```
![](\Images/041-02.png)

```
#åšäºŒå…ƒè¯­æ³•
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
#æŠŠcorpusé‡Œï¼Œæ‹¿æ‰æœ€åä¸€ä¸ªå’Œæ‹¿æ‰ç¬¬ä¸€ä¸ªåšzip()
#è¿”å›å‰åä¸¤ä¸ªtokençš„åºåˆ—
bigram_vocab = d2l.Vocab(bigram_tokens)
#åˆ›å»ºä¸€ä¸ªäºŒå…ƒè¯å…¸
bigram_vocab.token_freqs[:10]

#åšä¸‰å…ƒ
trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]

bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
#å› ä¸ºè¿”å›çš„æ˜¯å­—å…¸key-valueå…ƒç»„ï¼Œæ‰€ä»¥å¿…é¡»ç”¨ä¸¤ä¸ªå˜é‡å»æ¥
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
```
![](\Images/041-03.png)

**å­åºåˆ—é‡‡æ ·**

![](\Images/041-01.png)
ç”¨ä¸€ç§éšæœºåˆ†å‰²çš„æ–¹å¼å†³å®šèµ·å§‹ä½ç½®

```
def seq_data_iter_random(corpus, batch_size, num_steps):  #@save
    """ä½¿ç”¨éšæœºæŠ½æ ·ç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡å­åºåˆ—"""
    # num_stepsæ—¶é—´å‡è®¾çš„t,æˆ–è€…é©¬å°”å¯å¤«å‡è®¾çš„tau
    corpus = corpus[random.randint(0, num_steps - 1):]
    # éšæœºåœ¨ä¸€ä¸ªåºåˆ—(é•¿åº¦tau)é‡Œé€‰ä¸€ä¸ªä½ç½®ï¼Œå»æ‰å‰é¢çš„éƒ¨åˆ†
    num_subseqs = (len(corpus) - 1) // num_steps
    # èƒ½å¤Ÿç”Ÿæˆå­åºåˆ—çš„ä¸ªæ•°ï¼Œ corpus-1ä¸ºäº†è¡¨ç¤ºçœŸå®ç´¢å¼•
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # num_subseqs * num_steps = (len(corpus) - 1)
    # ä»æå¤´çš„corpusçš„ç¬¬ä¸€ä¸ªtokenå¼€å§‹ï¼Œåˆ°num_subseqs * num_steps
    # æ¯tauä¸ªå–ä¸€ä¸ªä½œä¸ºç´¢å¼•ï¼Œæ‰€ä»¥æœ‰num_subseqsæ•°é‡çš„ç´¢å¼•
    # ä¹Ÿå°±æ˜¯æ¯ä¸ªå­åºåˆ—åœ¨corpusé‡Œå¼€å§‹çš„ä¸‹æ ‡
    random.shuffle(initial_indices)
    # æ‰“ä¹±è¿™äº›ä¸‹æ ‡ï¼Œæ‰€ä»¥æ¯æ¬¡éšæœºå–
    # å–å‡ºçš„å­åºåˆ—åœ¨æ—¶åºä¸Šä¸ä¸€å®šç›¸é‚»

    def data(pos):
        # è¿”å›ä»posä½ç½®å¼€å§‹çš„é•¿åº¦ä¸ºnum_stepsçš„åºåˆ—
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size
    #æ¯ä¸ªbatchçš„å­åºåˆ—æ•°é‡
    for i in range(0, batch_size * num_batches, batch_size):
        # æŠŠæ¯ä¸ªbatchå¼€å¤´çš„å­åºåˆ—ä¸‹æ ‡æ‰¾å‡ºæ¥éå†
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        # è¿™é‡Œçš„ä¸‹æ ‡å·²ç»è¢«æ‰“ä¹±äº†
        # æŠŠæ¯ä¸ªbatché‡Œå­åºåˆ—çš„ä¸‹æ ‡æ‹¿å‡ºæ¥
        X = [data(j) for j in initial_indices_per_batch]
        # æå–æ¯ä¸ªå­åºåˆ—é‡Œçš„token
        Y = [data(j + 1) for j in initial_indices_per_batch]
        # æå–æ¯ä¸ªå­åºåˆ—ç›¸é‚»çš„token
        yield torch.tensor(X), torch.tensor(Y)
        #æ¯æ¬¡è¿”å›ä¸€ä¸ªbatchçš„å­åºåˆ—
        #å¯¹äºä¸€ä¸ªä¸‰å…ƒåºåˆ—
        #ç”¨X[0]é¢„æµ‹Y[0];X[0],X[1]é¢„æµ‹Y[1];X[1],X[2]é¢„æµ‹Y[2];X[2],X[3]é¢„æµ‹Y[3];X[3],X[4]é¢„æµ‹Y[4];
        #Yå°±æ˜¯labels

my_seq = list(range(35))
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    # ç°åœ¨0-4é‡Œå–ä¸€ä¸ªæå¤´
    # å‰©ä¸‹35-[0~4]é‡Œä¸ªåˆ‡æˆ//5=6ä¸ª
    # æ‰“ä¹±åˆ†æˆ6/2=3ä¸ªbatchï¼Œæ¯ä¸ªbatchæœ‰ä¸¤ä¸ªå­åºåˆ—
    # å†æå–æ¯ä¸ªbatché‡Œçš„æ‰€æœ‰å…ƒç´ 
    print('X: ', X, '\nY:', Y)
```
```
def seq_data_iter_sequential(corpus, batch_size, num_steps):  
    """ä¿è¯æ¯ä¸ªå°æ‰¹é‡çš„ç¬¬iä¸ªå…ƒç´ å’Œä¸‹ä¸€ä¸ªå°æ‰¹é‡çš„ç¬¬iä¸ªå…ƒç´ åœ¨åŸå§‹æ–‡æœ¬ä¸Šçš„ç›¸é‚»"""
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    # ä¿è¯è¿™äº›num_tokenså¯ä»¥è¢«æ•´é™¤
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    #Xå’ŒYç›¸å·®ä¸€ä¸ªtoken
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    #ä¸€ä¸ªbatché‡Œæœ‰å¤šå°‘ä¸ªå­åºåˆ—
    for i in range(0, num_steps * num_batches, num_steps):
        #é€šè¿‡å®šä¹‰æ­¥é•¿å¯ä»¥è·³ç€ç´¢å¼•
        #å†³å®šæ¯ä¸ªbatchçš„èµ·å§‹ä½ç½®
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        #æŒ‰é¡ºåºå–token
        yield X, Y
        #æŠŠreturnæ”¹æˆyield,å°±æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡è¿”å›ä¸€ä¸ªbatchï¼Œæ¯ä¸ªbatchåªæœ‰ä¸€ä¸ªå­åºåˆ—

for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):
    #ç”Ÿæˆå™¨é‡Œçš„for-loopå·²ç»åˆ†å‰²å¥½äº†batchï¼Œè¿™é‡Œçš„for-loopæ˜¯è¿”å›æ‰€æœ‰çš„batch
    print('X: ', X, '\nY:', Y)
```

**é›†æˆåˆ°ç±»é‡Œ**

```
class SeqDataLoader:  
    """åŠ è½½åºåˆ—æ•°æ®çš„è¿­ä»£å™¨"""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        #__iter__()å¯ä¸€è®©å®ä¾‹æˆä¸ºå¯è¿­ä»£å¯¹è±¡
        #å®šä¹‰__next__()å¯ä»¥è·å¾—å…¶è¿”å›å€¼
        #è¿™é‡Œiterè¿”å›çš„æœ¬èº«å°±æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨self.data_iter_fn()ï¼Œæ‰€ä»¥ä¸éœ€è¦å®šä¹‰next
        #å¯ä»¥ç†è§£ä¸ºfor _ in self.data_iter_fn()
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
        # è¿”å›æ‰¹é‡
```
è¾“å…¥æ˜¯åŸå§‹çš„tokenå’Œæ­¥é•¿ã€æ‰¹é‡ç­‰ï¼Œè¿”å›æ‰¹é‡ï¼Œç±»å°±å¾ˆæ–¹ä¾¿åœ°å°è£…äº†ä¸­é—´çš„åŠŸèƒ½