## 语言模型

- 给定文本序列 $x_1,...,x_T$，语言模型的目标是估计联合概率 $p(x_1,...,x_T)$
- 它的应用包括
  - 做预训练模型(eg BERT, GPT-3)
  - 生成本文，给定前面几个词，不断地使用 $x_t~p(x_t|x_1,...,x_{t-1})$，生成后续文本
  - 判断多个序列中哪个更常见，e.g. "to recognize a speech" vs "to wreck a nice beach"   
    - 语音模型返回这两个可能，语言模型预测其可能性(合理性)。

### 使用计数来建模
- 假设序列长度为2，我们预测
$p(x,x')=p(x)p(x'|x)={n(x)\over n}{n(x,x')\over n(x)}$
  - $n$ 是总词数， $n(x),n(x,x')$ 是单个单词和连续单词对（一前一后）的出现次数
- 很容易拓展到长为3的情况
$p(x,x',x'')=p(x)p(x'|x)p(x''|x,x')={n(x)\over n}{n(x,x')\over n(x)}{n(x,x',x'')\over n(x,x')}$
- 扫一遍整个文本，看该序列出现的次数

**N元语法**

- 当序列很长时，因为文本量不够大，很可能 $n(x_1,...,x_T)\le1$
- 使用马尔科夫假设可以缓解这个问题
  - 一元语法：$\tau=1$
  $$\begin{split}
  p(x_1,x_2,x_3,x_4)&=p(x_1)p(x_2)p(x_3)p(x_4)\\
  &={n(x_1)\over n}{n(x_2)\over n}{n(x_3)\over n}{n(x_4)\over n}
  \end{split}$$
  - 二元语法：$\tau=2$
  $$\begin{split}
  p(x_1,x_2,x_3,x_4)&=p(x_1)p(x_2|x_1)p(x_3|x_2)p(x_4|x_3)\\
  &={n(x_1)\over n}{n(x_1,x_2)\over n}{n(x_2,x_3)\over n}{n(x_3,x_4)\over n}
  \end{split}$$
  - 三元语法： $\tau=3$
  $$\begin{split}
  p(x_1,x_2,x_3,x_4)&=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)p(x_4|x_2,x_3)\\
  &={n(x_1)\over n}{n(x_1,x_2)\over n}{n(x_1,x_2,x_3)\over n}{n(x_2,x_3,x_4)\over n}
  \end{split}$$

  这个模型的优势是不需要把文本中任何长度的文本都扫描出来，只需要看子序列。并且可以存储一定长度序列的概率。
  复杂度与 $n$ 元语法是指数关系。
  eg: 二元，$m$ 个token， 就需要 $m+m^2$个子序列

### 总结

- 语言模型估计文本序列的联合概率
- 使用统计方法时常用 $n$ 元语法

### 代码实现

**stop words**

指自然语言处理当中会被过滤掉的一些单词，一般是指无意义的定冠词，不定冠词（a,an,the）, 连接词（of,but...）

**观测数据**

```
import random
import torch
from d2l import torch as d2l

tokens = d2l.tokenize(d2l.read_time_machine())
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
#排过序的
vocab.token_freqs[:10]

freqs = [freq for token, freq in vocab.token_freqs]
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
         xscale='log', yscale='log')
#单词按标号排序，作为x轴
#语言序列也符合二八定律
#出现十次的词不超过二三百，剩下的千余词出现频率很少
```
![](\Images/041-02.png)

```
#做二元语法
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
#把corpus里，拿掉最后一个和拿掉第一个做zip()
#返回前后两个token的序列
bigram_vocab = d2l.Vocab(bigram_tokens)
#创建一个二元词典
bigram_vocab.token_freqs[:10]

#做三元
trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]

bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
#因为返回的是字典key-value元组，所以必须用两个变量去接
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
```
![](\Images/041-03.png)

**子序列采样**

![](\Images/041-01.png)
用一种随机分割的方式决定起始位置

```
def seq_data_iter_random(corpus, batch_size, num_steps):  #@save
    """使用随机抽样生成一个小批量子序列"""
    # num_steps时间假设的t,或者马尔可夫假设的tau
    corpus = corpus[random.randint(0, num_steps - 1):]
    # 随机在一个序列(长度tau)里选一个位置，去掉前面的部分
    num_subseqs = (len(corpus) - 1) // num_steps
    # 能够生成子序列的个数， corpus-1为了表示真实索引
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # num_subseqs * num_steps = (len(corpus) - 1)
    # 从掐头的corpus的第一个token开始，到num_subseqs * num_steps
    # 每tau个取一个作为索引，所以有num_subseqs数量的索引
    # 也就是每个子序列在corpus里开始的下标
    random.shuffle(initial_indices)
    # 打乱这些下标，所以每次随机取
    # 取出的子序列在时序上不一定相邻

    def data(pos):
        # 返回从pos位置开始的长度为num_steps的序列
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size
    #每个batch的子序列数量
    for i in range(0, batch_size * num_batches, batch_size):
        # 把每个batch开头的子序列下标找出来遍历
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        # 这里的下标已经被打乱了
        # 把每个batch里子序列的下标拿出来
        X = [data(j) for j in initial_indices_per_batch]
        # 提取每个子序列里的token
        Y = [data(j + 1) for j in initial_indices_per_batch]
        # 提取每个子序列相邻的token
        yield torch.tensor(X), torch.tensor(Y)
        #每次返回一个batch的子序列
        #对于一个三元序列
        #用X[0]预测Y[0];X[0],X[1]预测Y[1];X[1],X[2]预测Y[2];X[2],X[3]预测Y[3];X[3],X[4]预测Y[4];
        #Y就是labels

my_seq = list(range(35))
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    # 现在0-4里取一个掐头
    # 剩下35-[0~4]里个切成//5=6个
    # 打乱分成6/2=3个batch，每个batch有两个子序列
    # 再提取每个batch里的所有元素
    print('X: ', X, '\nY:', Y)
```
```
def seq_data_iter_sequential(corpus, batch_size, num_steps):  
    """保证每个小批量的第i个元素和下一个小批量的第i个元素在原始文本上的相邻"""
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    # 保证这些num_tokens可以被整除
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    #X和Y相差一个token
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    #一个batch里有多少个子序列
    for i in range(0, num_steps * num_batches, num_steps):
        #通过定义步长可以跳着索引
        #决定每个batch的起始位置
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        #按顺序取token
        yield X, Y
        #把return改成yield,就是一个生成器，每次返回一个batch，每个batch只有一个子序列

for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):
    #生成器里的for-loop已经分割好了batch，这里的for-loop是返回所有的batch
    print('X: ', X, '\nY:', Y)
```

**集成到类里**

```
class SeqDataLoader:  
    """加载序列数据的迭代器"""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        #__iter__()可一让实例成为可迭代对象
        #定义__next__()可以获得其返回值
        #这里iter返回的本身就是一个生成器self.data_iter_fn()，所以不需要定义next
        #可以理解为for _ in self.data_iter_fn()
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
        # 返回批量
```
输入是原始的token和步长、批量等，返回批量，类就很方便地封装了中间的功能