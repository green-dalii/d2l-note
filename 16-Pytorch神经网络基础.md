# 16 - Pytorchç¥žç»ç½‘ç»œåŸºç¡€

### ðŸŽ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ðŸ‘‰[![Bilibil](https://i0.hdslb.com/bfs/archive/9a09827a8220e688f6866c928f58f5a256788aab.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1AK4y1P7vs)

### å±‚å’Œå—

**å—**ï¼ˆblockï¼‰å¯ä»¥æè¿°å•ä¸ªå±‚ã€ç”±å¤šä¸ªå±‚ç»„æˆçš„ç»„ä»¶æˆ–æ•´ä¸ªæ¨¡åž‹æœ¬èº«ã€‚ä½¿ç”¨å—è¿›è¡ŒæŠ½è±¡çš„ä¸€ä¸ªå¥½å¤„æ˜¯å¯ä»¥å°†ä¸€äº›å—ç»„åˆæˆæ›´å¤§çš„ç»„ä»¶ï¼Œ è¿™ä¸€è¿‡ç¨‹é€šå¸¸æ˜¯é€’å½’çš„ã€‚ é€šè¿‡å®šä¹‰ä»£ç æ¥æŒ‰éœ€ç”Ÿæˆä»»æ„å¤æ‚åº¦çš„å—ï¼Œ æˆ‘ä»¬å¯ä»¥**é€šè¿‡ç®€æ´çš„ä»£ç å®žçŽ°å¤æ‚çš„ç¥žç»ç½‘ç»œ**ã€‚

ä»Žç¼–ç¨‹çš„è§’åº¦æ¥çœ‹ï¼Œå—ç”±**ç±»**ï¼ˆclassï¼‰è¡¨ç¤ºã€‚ å®ƒçš„ä»»ä½•å­ç±»éƒ½å¿…é¡»å®šä¹‰ä¸€ä¸ªå°†å…¶è¾“å…¥è½¬æ¢ä¸ºè¾“å‡ºçš„å‰å‘ä¼ æ’­å‡½æ•°ï¼Œ å¹¶ä¸”å¿…é¡»å­˜å‚¨ä»»ä½•å¿…éœ€çš„å‚æ•°ã€‚ 

ä»Žå¤šå±‚æ„ŸçŸ¥æœºå…¥æ‰‹ï¼š
```
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256,10))

X = torch.rand(2, 20)
net(X)

## åˆ©ç”¨Sequentialå®šä¹‰äº†ä¸€ä¸ªç‰¹æ®Šçš„Module
```
ä»»ä½•ä¸€ä¸ªå±‚ã€ç¥žç»ç½‘ç»œéƒ½å¯ä»¥çœ‹ä½œModuleçš„ä¸€ä¸ªå­ç±»ã€‚

æˆ‘ä»¬é€šè¿‡å®žä¾‹åŒ–nn.Sequentialæ¥æž„å»ºæˆ‘ä»¬çš„æ¨¡åž‹ï¼Œ å±‚çš„æ‰§è¡Œé¡ºåºæ˜¯ä½œä¸ºå‚æ•°ä¼ é€’çš„ã€‚

ä¹Ÿå¯ä»¥è‡ªå®šä¹‰å—ï¼š
```
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.out = nn.Linear(256, 10)
        
    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
    #æ­¤å¤„å¿…é¡»æ˜¯forward,ç›¸å½“äºŽå¯¹nn.Moduleé‡Œçš„__call__()ä¸‹çš„forwardé‡å®šä¹‰ï¼›
```
è¿˜å¯ä»¥è‡ªå·±å®žçŽ°Sequential:
```
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for block in args:
            #æŠŠargsçš„å‚æ•°ä¼ å…¥ä½œä¸ºself.modulesè¿™ä¸ªæœ‰åºå­—å…¸çš„é”®-å€¼å¯¹
            self._modules[block] = block
    
    def forward(self, X):
        for block in self._modules.values():
            #ä¾æ¬¡è°ƒç”¨æœ‰åºå­—å…¸é‡Œçš„æ¨¡å—
            X = block(X)
        return X
    
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
```
å¯ä»¥è‡ªå®šä¹‰å—æ¥ä½¿ç®—æ³•æ›´çµæ´»ï¼š
```
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        #éšæœºç”Ÿæˆ20*20ä¸å‚ä¸Žè®­ç»ƒçš„æƒé‡
        self.linear = nn.Linear(20, 20)
        
    def forward(self, X):
        X = self.linear(X)
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        ##torch.mm()æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œå‡è®¾æœ‰ä¸€ä¸ªåç§»
        X = self.linear(X)
        while X.abs().sum() > 1:
            X /= 2
            return X.sum()
    
net = FixedHiddenMLP()
net(X)
## é€šè¿‡ç»§æ‰¿nn.Moduleå¯ä»¥æ›´çµæ´»åœ°å®šä¹‰æ¨¡åž‹
```
å—ä¹‹é—´å¯ä»¥åµŒå¥—ï¼š
```
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)
        
    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

ç»¼ä¸Šï¼Œå—å¯ä»¥ç†è§£ä¸ºèƒ½å¤Ÿå®žçŽ°ä¸€ä¸ªæˆ–å¤šä¸ªå±‚çš„ç±»ï¼Œé€šè¿‡å®šä¹‰ç±»çš„å®žä¾‹åŒ–æ¥å®Œæˆç¥žç»ç½‘ç»œçš„è¿ç®—ã€‚

### åˆå§‹åŒ–å‚æ•°
> state_dict() #æŸ¥çœ‹å­—å…¸å½¢å¼çš„æ•°å€¼

```
print(net[2].state_dict())
#å¯ä»¥æŠŠSequentialçœ‹ä½œä¸€ä¸ªlistï¼Œå¯ä»¥åˆ‡ç‰‡æ‹¿å‡ºæ¯ä¸€å±‚çš„å‚æ•°ã€‚
#æ˜¯ä¸€ä¸ªå­—å…¸ã€‚
#module.state_dict().keys()=['weight','bias']
```

>nn.bias/.weight(.data/.grad)#ç›´æŽ¥æŸ¥çœ‹å‚æ•°
```
print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)
# .data/.grad
```

> .named_parameters()#è¿”å›žiteratorï¼Œç”¨äºŽå¾ªçŽ¯ï¼Œè¿”å›ž(å‚æ•°å, å‚æ•°æ•°å€¼)ã€‚
```
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
print(*[(name, param.shape) for name, param in net.named_parameters()])
# *ä»£è¡¨æŠŠlist/tupleé‡Œçš„å…ƒç´ åˆ†å¼€ï¼Œè€Œéžæ•´ä¸ªè¾“å‡º
```
>add_module(name, module)#åœ¨å½“å‰æ¨¡å—æ·»åŠ å­æ¨¡å—ï¼Œä»¥ï¼ˆå‘½åï¼Œæ¨¡å—ï¼‰çš„æ–¹å¼
```
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        net.add_module(f'block {i}', block1())
        #åµŒå¥—å››ä¸ªblock1
        #add_module(name, module)
        #The module can be accessed as an attribute using the given name.
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
rgnet(X)
```
> nn.init.normal_/zeros_/constant_/uniform_ #åˆå§‹åŒ–å‚æ•°
> nn.apply() #å¯¹æ¨¡å—åº”ç”¨æ–¹æ³•

```
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        # _è¡¨ç¤ºç›´æŽ¥æ›¿æ¢æŽ‰m.weightï¼Œè€Œéžè¿”å›žå€¼ï¼Œæˆ–è€…è¯´ç›´æŽ¥èµ‹å€¼
        nn.init.zeros_(m.bias)
    
net.apply(init_normal)
#ç›¸å½“äºŽä¸€ä¸ªfor loop
net[0].weight.data[0], net[0].bias.data[0]
--------------------------------------
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1)
        nn.init.zeros_(m.bias)

net.apply(init_constant)
net[0].weight.data[0], net[0].bias.data[0]
--------------------------------------
def xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
        #uniform distribution
-----------------------------------   
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)
        #nn.initå‡½æ•°è®¾ç½®æ¨¡å—åˆå§‹å€¼

net[0].apply(xavier)
net[2].apply(init_42)
print(net[0].weight.data[0])
print(net[2].weight.data)
```
>ç®€å•ç²—æš´çš„ç›´æŽ¥èµ‹å€¼æ–¹å¼
```
net[0].weight.data[:] += 1
net[0].weight.data[0, 0] = 42
net[0].weight.data[0] #é»˜è®¤å–è¡Œ
```
>å‚æ•°è”åŠ¨
```
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared,
                   nn.ReLU(), nn.Linear(8, 1))
net(X)
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] == 100
# ä¼šåŒæ—¶ä¿®æ”¹ä¸¤ä¸ªshared, ç›¸å½“äºŽåŒä¸€ä¸ªå®žä¾‹çš„èµ‹å€¼
print(net[2].weight.data[0] == net[4].weight.data[0])
```

### è‡ªå®šä¹‰å±‚
```
class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()
    
    def forward(self, X):
        return X - X.mean()

layer = CenteredLayer()
layer(torch.FloatTensor([1, 2, 3, 4, 5]))

net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())

Y = net(torch.rand(4, 8))
Y.mean()
```
> nn.Parameter(tensor, required_grad=True) #æŠŠä¼ å…¥å¼ é‡å½“ä½œæ¨¡å—å‚æ•°ï¼Œå¯ä»¥å¯¹å…¶æ±‚å¯¼çš„
```
#è‡ªå®šä¹‰å¸¦å‚æ•°çš„å±‚
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
        #ç†è®ºä¸Štorch.randn(units,)ä¸Žtorch.randn(units)æ²¡æœ‰åŒºåˆ«
        #é€—å·åŽçœç•¥è¡¨ç¤ºç»´åº¦åªæœ‰1
        #å¦‚æžœæ˜¯randn(2, 1)ï¼Œå°±æ˜¯ä¸€ä¸ªäºŒç»´å¼ é‡äº†ã€‚
        
    def forward(self, X):
        linear = torch.matmul(X, self.weight.dataï¼‰ + self.bias.data
        return F.relu(linear)
    
dense = MyLinear(5, 3)
dense.weight
```
### è¯»å†™æ–‡ä»¶

>torch.save(parameter, 'filename')
>torch.load('filename')

```
#å­˜å‚¨ä¸€ä¸ªtensor
X = torch.arange(4)
torch.save(X, 'x-file')

X2 = torch.load('x-file')
X2
```
```
#å­˜å‚¨é«˜ç»´åº¦
y = torch.zeros(4)
torch.save([X, y], 'x-files')
x2, y2 = torch.load('x-files')
(x2, y2)
```
```
#å­˜å‚¨å­—å…¸
mydict = {'x': X, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
mydict2
```
```
#å­˜å‚¨æ¨¡åž‹å‚æ•°
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)
    
    def forward(self, X):
        return self.output(F.relu(self.hidden(X)))
    
net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)

torch.save(net.state_dict(), 'mlp.params') #å­˜å‚¨çš„å®žé™…æ˜¯æ¨¡åž‹å‚æ•°è€Œéžæ¨¡åž‹æœ¬èº«

clone = MLP()
#å…ˆå…‹éš†åŽŸæ¨¡åž‹æœ¬èº«
clone.load_state_dict(torch.load('mlp.params'))
#å†è½½å…¥å¹¶é‡å†™å…‹éš†çš„æ¨¡åž‹
clone.eval()
#eval()è®¾ç½®ä¸€ä¸ªæ¨¡åž‹å‚æ•°ä¸ºä¸å¯å¯¼ï¼Œå¹¶è¿”å›žæ¨¡åž‹æœ¬èº«
Y_clone = clone(X)
Y_clone == Y  ===>  True
```