# 16 - Pytorchç¥žç»ç½‘ç»œåŸºç¡€

---

### ðŸŽ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ðŸ‘‰
[![Bilibil](https://i0.hdslb.com/bfs/archive/9a09827a8220e688f6866c928f58f5a256788aab.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1AK4y1P7vs)

## å±‚å’Œå—

**å—**ï¼ˆblockï¼‰å¯ä»¥æè¿°å•ä¸ªå±‚ã€ç”±å¤šä¸ªå±‚ç»„æˆçš„ç»„ä»¶æˆ–æ•´ä¸ªæ¨¡åž‹æœ¬èº«ã€‚ä½¿ç”¨å—è¿›è¡ŒæŠ½è±¡çš„ä¸€ä¸ªå¥½å¤„æ˜¯å¯ä»¥å°†ä¸€äº›å—ç»„åˆæˆæ›´å¤§çš„ç»„ä»¶ï¼Œ è¿™ä¸€è¿‡ç¨‹é€šå¸¸æ˜¯é€’å½’çš„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ é€šè¿‡å®šä¹‰ä»£ç æ¥æŒ‰éœ€ç”Ÿæˆä»»æ„å¤æ‚åº¦çš„å—ï¼Œ æˆ‘ä»¬å¯ä»¥**é€šè¿‡ç®€æ´çš„ä»£ç å®žçŽ°å¤æ‚çš„ç¥žç»ç½‘ç»œ**ã€‚

![block](https://zh.d2l.ai/_images/blocks.svg)

ä»Žç¼–ç¨‹çš„è§’åº¦æ¥çœ‹ï¼Œå—ç”±**ç±»**ï¼ˆclassï¼‰è¡¨ç¤ºã€‚ å®ƒçš„ä»»ä½•å­ç±»éƒ½å¿…é¡»å®šä¹‰ä¸€ä¸ªå°†å…¶è¾“å…¥è½¬æ¢ä¸ºè¾“å‡ºçš„å‰å‘ä¼ æ’­å‡½æ•°ï¼Œ å¹¶ä¸”å¿…é¡»å­˜å‚¨ä»»ä½•å¿…éœ€çš„å‚æ•°ã€‚ 

### ä½¿ç”¨Sequentialå®žçŽ°å±‚

`nn.Sequential`å®šä¹‰äº†ä¸€ç§ç‰¹æ®Šçš„Moduleï¼Œé€šè¿‡å®žä¾‹åŒ–nn.Sequentialæ¥æž„å»ºæˆ‘ä»¬çš„æ¨¡åž‹ï¼Œ å±‚çš„æ‰§è¡Œé¡ºåºæ˜¯ä½œä¸ºå‚æ•°ä¼ é€’çš„ã€‚

```python
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256,10))

X = torch.rand(2, 20)
net(X)
```

### ä½¿ç”¨Blockå®žçŽ°å—

ä»»ä½•ä¸€ä¸ªå±‚ã€ç¥žç»ç½‘ç»œéƒ½å¯ä»¥çœ‹ä½œModuleçš„ä¸€ä¸ªå­ç±»ã€‚

```python
class MLP(nn.Module):
    # å¿…é¡»å…ˆä½¿ç”¨çˆ¶ç±»çš„initåˆå§‹åŒ–ï¼ŒæŽ¥ä¸‹æ¥å¯ä»¥å®šä¹‰å„å±‚
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.out = nn.Linear(256, 10)
    # å¿…é¡»é‡æ–°å®šä¹‰å‰é¦ˆè¿‡ç¨‹   
    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
    
net = MLP()
net(X)
```

### è‡ªå®šä¹‰Sequentialå®žçŽ°

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # è¿™é‡Œï¼Œ`module`æ˜¯`Module`å­ç±»çš„ä¸€ä¸ªå®žä¾‹ã€‚æˆ‘ä»¬æŠŠå®ƒä¿å­˜åœ¨'Module'ç±»çš„æˆå‘˜
            # å˜é‡`_modules` ä¸­ã€‚`module`çš„ç±»åž‹æ˜¯OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDictä¿è¯äº†æŒ‰ç…§æˆå‘˜æ·»åŠ çš„é¡ºåºéåŽ†å®ƒä»¬
        for block in self._modules.values():
            X = block(X)
        return X
    
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
```

### è‡ªå®šä¹‰Blockå®žçŽ°

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        #éšæœºç”Ÿæˆ20*20ä¸å‚ä¸Žè®­ç»ƒçš„æƒé‡
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)
        
    def forward(self, X):
        X = self.linear(X)
        X = F.relu(torch.mm(X, self.rand_weight) + 1)   ##torch.mm()çŸ©é˜µä¹˜æ³•ï¼Œå‡è®¾æœ‰ä¸€ä¸ªåç§»1
        X = self.linear(X)
        # å¯ä»¥åœ¨å‰å‘è®¡ç®—ä¸­ä½¿ç”¨PythonæŽ§åˆ¶æµæ¥å®žçŽ°æ›´å¤æ‚çš„è¿‡ç¨‹
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
    
net = FixedHiddenMLP()
net(X)
```

> åœ¨è¿™ä¸ª`FixedHiddenMLP`æ¨¡åž‹ä¸­ï¼Œæˆ‘ä»¬å®žçŽ°äº†ä¸€ä¸ªéšè—å±‚ï¼Œ
å…¶æƒé‡ï¼ˆ`self.rand_weight`ï¼‰åœ¨å®žä¾‹åŒ–æ—¶è¢«éšæœºåˆå§‹åŒ–ï¼Œä¹‹åŽä¸ºå¸¸é‡ã€‚
è¿™ä¸ªæƒé‡ä¸æ˜¯ä¸€ä¸ªæ¨¡åž‹å‚æ•°ï¼Œå› æ­¤å®ƒæ°¸è¿œä¸ä¼šè¢«åå‘ä¼ æ’­æ›´æ–°ã€‚
ç„¶åŽï¼Œç¥žç»ç½‘ç»œå°†è¿™ä¸ªå›ºå®šå±‚çš„è¾“å‡ºé€šè¿‡ä¸€ä¸ªå…¨è¿žæŽ¥å±‚ã€‚
>
> æ³¨æ„ï¼Œåœ¨è¿”å›žè¾“å‡ºä¹‹å‰ï¼Œæ¨¡åž‹åšäº†ä¸€äº›ä¸å¯»å¸¸çš„äº‹æƒ…ï¼š
å®ƒè¿è¡Œäº†ä¸€ä¸ªwhileå¾ªçŽ¯ï¼Œåœ¨$L_1$èŒƒæ•°å¤§äºŽ$1$çš„æ¡ä»¶ä¸‹ï¼Œ
å°†è¾“å‡ºå‘é‡é™¤ä»¥$2$ï¼Œç›´åˆ°å®ƒæ»¡è¶³æ¡ä»¶ä¸ºæ­¢ã€‚
æœ€åŽï¼Œæ¨¡åž‹è¿”å›žäº†`X`ä¸­æ‰€æœ‰é¡¹çš„å’Œã€‚
æ³¨æ„ï¼Œæ­¤æ“ä½œå¯èƒ½ä¸ä¼šå¸¸ç”¨äºŽåœ¨ä»»ä½•å®žé™…ä»»åŠ¡ä¸­ï¼Œ
æˆ‘ä»¬åªæ˜¯å‘ä½ å±•ç¤ºå¦‚ä½•å°†ä»»æ„ä»£ç é›†æˆåˆ°ç¥žç»ç½‘ç»œè®¡ç®—çš„æµç¨‹ä¸­ã€‚


### æ··åˆSequentialå’ŒBlockä½¿ç”¨

```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)
        
    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

ç»¼ä¸Šï¼Œå—å¯ä»¥ç†è§£ä¸ºèƒ½å¤Ÿå®žçŽ°ä¸€ä¸ªæˆ–å¤šä¸ªå±‚çš„ç±»ï¼Œé€šè¿‡å®šä¹‰ç±»çš„å®žä¾‹åŒ–æ¥å®Œæˆç¥žç»ç½‘ç»œçš„è¿ç®—ã€‚

## å‚æ•°ç®¡ç†

### åˆå§‹åŒ–å‚æ•°

> state_dict() #æŸ¥çœ‹å­—å…¸å½¢å¼çš„æ•°å€¼

```python
print(net[2].state_dict())
#å¯ä»¥æŠŠSequentialçœ‹ä½œä¸€ä¸ªlistï¼Œå¯ä»¥åˆ‡ç‰‡æ‹¿å‡ºæ¯ä¸€å±‚çš„å‚æ•°ã€‚
#æ˜¯ä¸€ä¸ªå­—å…¸ã€‚
#module.state_dict().keys()=['weight','bias']
```

>nn.bias/.weight(.data/.grad)#ç›´æŽ¥æŸ¥çœ‹å‚æ•°

```python
print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)
# .data/.grad
```

> .named_parameters()#è¿”å›žiteratorï¼Œç”¨äºŽå¾ªçŽ¯ï¼Œè¿”å›ž(å‚æ•°å, å‚æ•°æ•°å€¼)ã€‚

```python
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
print(*[(name, param.shape) for name, param in net.named_parameters()])
# *ä»£è¡¨æŠŠlist/tupleé‡Œçš„å…ƒç´ åˆ†å¼€ï¼Œè€Œéžæ•´ä¸ªè¾“å‡º
```

>add_module(name, module)#åœ¨å½“å‰æ¨¡å—æ·»åŠ å­æ¨¡å—ï¼Œä»¥ï¼ˆå‘½åï¼Œæ¨¡å—ï¼‰çš„æ–¹å¼

```python
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        net.add_module(f'block {i}', block1())
        #åµŒå¥—å››ä¸ªblock1
        #add_module(name, module)
        #The module can be accessed as an attribute using the given name.
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
rgnet(X)
```

> nn.init.normal_/zeros_/constant_/uniform_ #åˆå§‹åŒ–å‚æ•°
> nn.apply() #å¯¹æ¨¡å—åº”ç”¨æ–¹æ³•

```python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        # _è¡¨ç¤ºç›´æŽ¥æ›¿æ¢æŽ‰m.weightï¼Œè€Œéžè¿”å›žå€¼ï¼Œæˆ–è€…è¯´ç›´æŽ¥èµ‹å€¼
        nn.init.zeros_(m.bias)
    
net.apply(init_normal)
#ç›¸å½“äºŽä¸€ä¸ªfor loop
net[0].weight.data[0], net[0].bias.data[0]
--------------------------------------
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1)
        nn.init.zeros_(m.bias)

net.apply(init_constant)
net[0].weight.data[0], net[0].bias.data[0]
--------------------------------------
def xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
        #uniform distribution
-----------------------------------   
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)
        #nn.initå‡½æ•°è®¾ç½®æ¨¡å—åˆå§‹å€¼

net[0].apply(xavier)
net[2].apply(init_42)
print(net[0].weight.data[0])
print(net[2].weight.data)
```

>ç®€å•ç²—æš´çš„ç›´æŽ¥èµ‹å€¼æ–¹å¼

```python
net[0].weight.data[:] += 1
net[0].weight.data[0, 0] = 42
net[0].weight.data[0] #é»˜è®¤å–è¡Œ
```

>å‚æ•°è”åŠ¨

```python
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared,
                   nn.ReLU(), nn.Linear(8, 1))
net(X)
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] == 100
# ä¼šåŒæ—¶ä¿®æ”¹ä¸¤ä¸ªshared, ç›¸å½“äºŽåŒä¸€ä¸ªå®žä¾‹çš„èµ‹å€¼
print(net[2].weight.data[0] == net[4].weight.data[0])
```

### è‡ªå®šä¹‰å±‚

```python
class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()
    
    def forward(self, X):
        return X - X.mean()

layer = CenteredLayer()
layer(torch.FloatTensor([1, 2, 3, 4, 5]))

net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())

Y = net(torch.rand(4, 8))
Y.mean()
```

> nn.Parameter(tensor, required_grad=True) #æŠŠä¼ å…¥å¼ é‡å½“ä½œæ¨¡å—å‚æ•°ï¼Œå¯ä»¥å¯¹å…¶æ±‚å¯¼çš„

```python
#è‡ªå®šä¹‰å¸¦å‚æ•°çš„å±‚
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
        #ç†è®ºä¸Štorch.randn(units,)ä¸Žtorch.randn(units)æ²¡æœ‰åŒºåˆ«
        #é€—å·åŽçœç•¥è¡¨ç¤ºç»´åº¦åªæœ‰1
        #å¦‚æžœæ˜¯randn(2, 1)ï¼Œå°±æ˜¯ä¸€ä¸ªäºŒç»´å¼ é‡äº†ã€‚
        
    def forward(self, X):
        linear = torch.matmul(X, self.weight.dataï¼‰ + self.bias.data
        return F.relu(linear)
    
dense = MyLinear(5, 3)
dense.weight
```
### è¯»å†™æ–‡ä»¶

>torch.save(parameter, 'filename')
>torch.load('filename')

```python
#å­˜å‚¨ä¸€ä¸ªtensor
X = torch.arange(4)
torch.save(X, 'x-file')

X2 = torch.load('x-file')
X2
```

```python
#å­˜å‚¨é«˜ç»´åº¦
y = torch.zeros(4)
torch.save([X, y], 'x-files')
x2, y2 = torch.load('x-files')
(x2, y2)
```

```python
#å­˜å‚¨å­—å…¸
mydict = {'x': X, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
mydict2
```

```python
#å­˜å‚¨æ¨¡åž‹å‚æ•°
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)
    
    def forward(self, X):
        return self.output(F.relu(self.hidden(X)))
    
net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)

torch.save(net.state_dict(), 'mlp.params') #å­˜å‚¨çš„å®žé™…æ˜¯æ¨¡åž‹å‚æ•°è€Œéžæ¨¡åž‹æœ¬èº«

clone = MLP()
#å…ˆå…‹éš†åŽŸæ¨¡åž‹æœ¬èº«
clone.load_state_dict(torch.load('mlp.params'))
#å†è½½å…¥å¹¶é‡å†™å…‹éš†çš„æ¨¡åž‹
clone.eval()
#eval()è®¾ç½®ä¸€ä¸ªæ¨¡åž‹å‚æ•°ä¸ºä¸å¯å¯¼ï¼Œå¹¶è¿”å›žæ¨¡åž‹æœ¬èº«
Y_clone = clone(X)
Y_clone == Y  ===>  True
```
