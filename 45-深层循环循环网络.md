# 45 - æ·±å±‚å¾ªç¯ç¥ç»ç½‘ç»œ

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰[![Bilibil](https://i0.hdslb.com/bfs/archive/d12bdc5f377444e73ba65e9ee2b27847535aff64.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1JM4y1T7N4)

é—®ï¼šå¦‚ä½•è·å¾—æ›´å¤šçš„éçº¿æ€§æ€§ï¼Ÿ

ç­”ï¼šå¤šéšè—å±‚

![](\Images/045-01.png)

$$\begin{split}
&{\bf H_t^1}=f_1(\bf H_{t-1}^1,X_t)\\
&{\bf H_t^2}=f_2(\bf H_{t-1}^2,H_t^1)\\
&...\\
&{\bf H_t^j}=f_j(\bf H_{t-1}^j,H_t^{j-1})\\
&...\\
&{\bf O_t}=g(\bf H_t^L)
\end{split}$$

- æ¯å±‚æ¨¡å‹é‡Œæ‰“åŒ…äº†éçº¿æ€§å‡½æ•°

**æ€»ç»“**

- æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œä½¿ç”¨å¤šä¸ªéšè—å±‚æ¥è·å¾—æ›´å¤šçš„éçº¿æ€§æ€§

### ä»£ç å®ç°

```
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
device = d2l.try_gpu()
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)
#æ‰€æœ‰æ¨¡å‹éƒ½æœ‰kw,num_layersï¼Œå› ä¸º hidden layers çš„å¤§å°æ•°é‡ä¸å˜ï¼Œæ‰€ä»¥ç›´æ¥è®¾ç½®å±‚æ•°ï¼ˆé»˜è®¤åˆå§‹åŒ–æƒé‡å…¨é›¶ï¼‰å°±å¯ä»¥
#æ¨¡å‹æ¯å±‚éƒ½ç”¨tanhï¼ˆï¼‰åšäº†éçº¿æ€§
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
```

```
num_epochs, lr = 500, 2
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)

#è®¡ç®—æ…¢äº†ï¼Œæ”¶æ•›å¿«äº†ï¼Œè®°ä½äº†æ–‡æœ¬ï¼Œperplexity=1
```
![](Images/045-02.png)