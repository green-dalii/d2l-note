# 26 - æ‰¹é‡å½’ä¸€åŒ–

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‡

[![Bilibil](https://i2.hdslb.com/bfs/archive/c52c4d88d8fe65f6d2ffac27b8ce6cb02dcdcacc.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1X44y1r77r)

## ç½‘ç»œè¶Šæ·±äº§ç”Ÿçš„é—®é¢˜

- åå‘ä¼ æ’­ï¼ŒæŸå¤±çš„æ¢¯åº¦ä»è¾“å‡ºå±‚å‘åä¼ ï¼Œé è¿‘è¾“å‡ºçš„å±‚è®­ç»ƒè¾ƒå¿«
  - æ¢¯åº¦è¶Šå¾€ä¸‹ä¼ é€’è¶Šå°ï¼ˆå°æ•°ç›¸ä¹˜ï¼‰
- æ•°æ®åœ¨æœ€åº•éƒ¨
  - é è¿‘æ•°æ®çš„åº•éƒ¨å±‚è®­ç»ƒè¾ƒæ…¢
  - åº•éƒ¨å±‚ä¸€å˜åŒ–ï¼Œæ‰€æœ‰éƒ½å¾—è·Ÿç€å˜ï¼Œç›¸å½“äºä½å±‚ç‰¹å¾æ”¹å˜ï¼Œä¸æ–­æŠ½è±¡å¾—åˆ°çš„é«˜å±‚ç‰¹å¾ä¹Ÿä¼šéšä¹‹æ”¹å˜
  - é¡¶éƒ¨çš„é‚£äº›å±‚éœ€è¦é‡æ–°å­¦ä¹ å¤šæ¬¡
  - å¯¼è‡´æ”¶æ•›å˜æ…¢



## å¦‚ä½•è§£å†³

å¯¹äºå…¸å‹çš„å¤šå±‚æ„ŸçŸ¥æœºæˆ–å·ç§¯ç¥ç»ç½‘ç»œã€‚å½“æˆ‘ä»¬è®­ç»ƒæ—¶ï¼Œä¸­é—´å±‚ä¸­çš„å˜é‡ï¼ˆä¾‹å¦‚ï¼Œå¤šå±‚æ„ŸçŸ¥æœºä¸­çš„ä»¿å°„å˜æ¢è¾“å‡ºï¼‰å¯èƒ½å…·æœ‰æ›´å¹¿çš„å˜åŒ–èŒƒå›´ï¼šä¸è®ºæ˜¯æ²¿ç€ä»è¾“å…¥åˆ°è¾“å‡ºçš„å±‚ï¼Œè·¨åŒä¸€å±‚ä¸­çš„å•å…ƒï¼Œæˆ–æ˜¯éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæ¨¡å‹å‚æ•°çš„éšç€è®­ç»ƒæ›´æ–°å˜å¹»è«æµ‹ã€‚ æ‰¹é‡è§„èŒƒåŒ–çš„å‘æ˜è€…éæ­£å¼åœ°å‡è®¾ï¼Œè¿™äº›å˜é‡åˆ†å¸ƒä¸­çš„è¿™ç§åç§»å¯èƒ½ä¼šé˜»ç¢ç½‘ç»œçš„æ”¶æ•›ã€‚ ç›´è§‚åœ°è¯´ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šçŒœæƒ³ï¼Œå¦‚æœä¸€ä¸ªå±‚çš„å¯å˜å€¼æ˜¯å¦ä¸€å±‚çš„100å€ï¼Œè¿™å¯èƒ½éœ€è¦å¯¹å­¦ä¹ ç‡è¿›è¡Œè¡¥å¿è°ƒæ•´ã€‚

åŒæ—¶ï¼Œæ›´æ·±å±‚çš„ç½‘ç»œå¾ˆå¤æ‚ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆã€‚ è¿™æ„å‘³ç€æ­£åˆ™åŒ–å˜å¾—æ›´åŠ é‡è¦ã€‚

æ‰¹é‡è§„èŒƒåŒ–åº”ç”¨äºå•ä¸ªå¯é€‰å±‚ï¼ˆä¹Ÿå¯ä»¥åº”ç”¨åˆ°æ‰€æœ‰å±‚ï¼‰ï¼Œå…¶åŸç†å¦‚ä¸‹ï¼šåœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè§„èŒƒåŒ–è¾“å…¥ï¼Œå³é€šè¿‡å‡å»å…¶å‡å€¼å¹¶é™¤ä»¥å…¶æ ‡å‡†å·®ï¼Œå…¶ä¸­ä¸¤è€…å‡åŸºäºå½“å‰å°æ‰¹é‡å¤„ç†ã€‚ æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åº”ç”¨æ¯”ä¾‹ç³»æ•°å’Œæ¯”ä¾‹åç§»ã€‚ æ­£æ˜¯ç”±äºè¿™ä¸ªåŸºäºæ‰¹é‡ç»Ÿè®¡çš„æ ‡å‡†åŒ–ï¼Œæ‰æœ‰äº†æ‰¹é‡å½’ä¸€åŒ–çš„åç§°ã€‚

![bn](Images/Batch_Normalization.gif)

å›ºå®šå°æ‰¹é‡ï¼ˆåœ¨ä¸åŒå±‚è¾“å‡ºï¼‰é‡Œé¢çš„å‡å€¼å’Œæ–¹å·®ï¼š

$$
\mu_B={1 \over |B|} \sum_{i \in B} x_i \\

\sigma_B^2={1 \over |B|} \sum_{i \in B} (x_i-\mu_B)^2 + \epsilon
$$

> å…¶ä¸­$B$æŒ‡ä¸€ä¸ªæ‰¹é‡ Batchï¼Œ$\epsilon$ä¸ºä¸€ä¸ªå¾ˆå°çš„æ•°ï¼Œé˜²æ­¢æ–¹å·®ä¸ºé›¶ï¼Œåœ¨ä¸‹æ–‡æ— æ³•è¿›è¡Œé™¤é›¶è¿ç®—

ç„¶åå†é€šè¿‡ä¸‹å¼å¯¹æ¯ä¸ªæ‰¹é‡åœ¨ä¸åŒå±‚çš„è¾“å‡ºå€¼æ•°æ®åšé¢å¤–çš„è°ƒæ•´ï¼Œå°†æ¯å±‚è¾“å‡ºå€¼å›ºå®šä¸ºå‡å€¼ä¸º${\beta}$ã€æ–¹å·®ä¸º${\gamma}$çš„åˆ†å¸ƒï¼š

$$
x_{i+1}=\gamma{x_i-\mu_B \over \sigma_B} + \beta
$$

- æ¯”ä¾‹${\gamma}$å’Œåç§»ç³»æ•°${\beta}$æ˜¯å­¦ä¹ å‡ºæ¥çš„
- æ‰¹é‡å½’ä¸€åŒ–æ˜¯ä¸€ä¸ªçº¿æ€§å˜æ¢
- ä½œç”¨ä½ç½®
  - å…¨è¿æ¥å±‚å’Œå·ç§¯å±‚è¾“å‡ºä¸Šï¼Œæ¿€æ´»å‡½æ•°ä¹‹å‰
    > å› ä¸ºä¸€èˆ¬æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ reluï¼‰ ä¼šå°†æ•°æ®æ˜ å°„ä¸ºæ­£æ•°ï¼Œæ‰€ä»¥ä¸èƒ½å†å¸¦å›æ­£è´Ÿå„å¼‚çš„çŠ¶æ€
  - å…¨è¿æ¥å±‚å’Œå·ç§¯å±‚è¾“å…¥ä¸Š
- å¯¹äºå…¨è¿æ¥å±‚ï¼Œä½œç”¨åœ¨**ç‰¹å¾ç»´**ï¼ˆç‹¬ç«‹æ”¹å˜æ¯ä¸ªç‰¹å¾çš„åˆ†å¸ƒï¼‰
- å¯¹äºå·ç§¯å±‚ï¼Œä½œç”¨äº**é€šé“ç»´**ï¼ˆå³ä¸€ä¸ªæ»‘åŠ¨çª—å£é‡Œåƒç´ çš„ç‰¹å¾ï¼‰
- åªæœ‰**æ‰¹é‡è¶³å¤Ÿå¤§**æ—¶æ‰¹é‡å½’ä¸€åŒ–æ•ˆæœæ‰èƒ½æœ‰æ•ˆä¸”ç¨³å®šï¼ˆæ¯”å¦‚æç«¯æƒ…å†µbatch_size=1æ—¶ï¼Œnormalizationåéšè—å±‚è¾“å‡ºå°†ä¸ºé›¶ï¼‰ï¼Œæ‰¹é‡å¤§å°è¶…å‚æ•°çš„é€‰æ‹©ç”šè‡³æ¯”æ²¡æœ‰æ‰¹é‡å½’ä¸€åŒ–æ›´é‡è¦

## æ‰¹é‡å½’ä¸€åŒ–çš„ä½œç”¨

- å¯ä»¥**åŠ é€Ÿæ”¶æ•›å¹¶è®©è®­ç»ƒæ›´ç¨³å®š**ï¼ˆå¯ä»¥ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ï¼Œè€Œé˜²æ­¢å­¦ä¹ ç‡è¿‡å¤§é€ æˆçš„æ— æ³•æ”¶æ•›æŠ–åŠ¨æˆ–è€…é è¿‘è¾“å‡ºå±‚æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ï¼‰
- ä¸€èˆ¬ä¸æ”¹å˜æ¨¡å‹çš„ç²¾åº¦

![bn](Images/batch_norm3.jpg)

> ä¸Šå›¾ä»¥ä½¿ç”¨VGGç½‘ç»œä¸ºä¾‹å±•ç¤ºBatchNormçš„æ•ˆæœï¼Œæ©™è‰²ä»£è¡¨æ ‡å‡†ç»“æ„ï¼Œè“è‰²ä»£è¡¨å¢åŠ äº†BatchNormçš„å¯¹æ¯”ç»“æ„ï¼Œå“çº¢è‰²ä»£è¡¨å¢åŠ äº†â€œNoisy BatchNormâ€çš„å¯¹æ¯”ç»“æ„ã€‚ä»å·¦ä¾§å›¾å¯çœ‹å‡ºåŠ å…¥BatchNormåï¼Œè®­ç»ƒç²¾è¯»æ”¶æ•›å¾—æ›´å¿«ï¼ŒåŒæ—¶æŠ–åŠ¨æ›´å°ï¼ˆä½†ä¸æ”¹å˜æœ€ç»ˆçš„ç²¾åº¦ï¼‰ï¼›ä»å³ä¾§å›¾å¯çœ‹å‡ºåŠ å…¥BatchNormåï¼Œå„å±‚è¾“å‡ºåˆ†å¸ƒæ›´åŠ â€œå‡è¡¡â€ã€‚

![bn2](Images/batch_norm.jpg)

> é€šè¿‡ä¸Šå›¾å¯¹æ¯”å®éªŒï¼Œå¯ä»¥çœ‹å‡ºä½¿ç”¨BNåï¼ŒæŸå¤±ä¸‹é™æ›´å¿«æ›´å¹³ç¨³ï¼Œæ¢¯åº¦æŠ–åŠ¨æ›´ç¨³å®šã€‚

## æ‰¹é‡å½’ä¸€åŒ–ä½œç”¨çš„åŸç†

- æœ€åˆçš„è®ºæ–‡è¡¨ç¤ºå¯ä»¥å‡å°‘å†…éƒ¨åå˜é‡è½¬ç§»
- åç»­è®ºæ–‡æŒ‡å‡ºbatch normalizationç›¸å½“äºåœ¨å°æ‰¹é‡é‡Œ**å¢åŠ å™ªéŸ³**$\mu,\sigma$ï¼Œå¯¹æ•°æ®è¿›è¡Œäº†éšæœºåç§»å’Œç¼©æ”¾ï¼ˆç›®å‰è¿˜æ²¡æœ‰ä¸€ä¸ªç»Ÿä¸€çš„ç»“è®ºï¼‰
- **æ²¡å¿…è¦å’Œä¸¢å¼ƒæ³•æ··åˆä½¿ç”¨**ï¼ˆåœ¨[ç•ªå¤–04-Kaggleç«èµ›å®è·µç»éªŒ](ç•ªå¤–ç¯‡/ç•ªå¤–04-Kaggleç«èµ›å®è·µç»éªŒ.md)ä¸€ç¯‡ä¸­æœ‰ç›¸å…³å®è·µè¯æ˜ï¼‰

## Batch Normalizationã€Layer Normalization

## ä»£ç å®ç°

åœ¨æµ‹è¯•çš„æ—¶å€™ï¼Œæ ·æœ¬ä¸€ä¸ªä¸€ä¸ªè¿›å…¥ç½‘ç»œï¼Œæ²¡æœ‰åŒºåŸŸå‡å€¼ã€æ–¹å·®å¯æ±‚ã€‚å¯ä»¥ç”¨æµ‹è¯•é›†å¾—åˆ°å‡å€¼ã€æ–¹å·®ä½œä¸ºåˆå€¼ã€‚

å…¨å±€æœŸæœ›çš„æ›´æ–°è¦å€ŸåŠ©**Karlman æ»¤æ³¢**

å¯¹åŒä¸€ä¸ªç›®æ ‡ï¼Œå¤šæ¬¡æµ‹é‡å€¼$z_1,z_2,z_3,...,z_k$ï¼Œå…¶æœŸæœ›$x_1, x_2,...,x_k$

$$
\hat x_k&={1\over k}(z_1+z_2+z_3+...+z_{k-1}+z_k)\\
&={1\over k}(z_1+z_2+z_3+...+z_{k-1})+{1\over k}z_k\\
&={1\over k}{k-1\over k-1}(z_1+z_2+z_3+...+z_{k-1})+{1\over k}z_k\\
&={k-1\over k}\hat x_{k-1}+{1\over k}z_k\\
&=\hat x_{k-1}-{1\over k}\hat x_{k-1}+{1\over k}z_k\\
&=\hat x_{k-1}+{1\over k}(z_k-\hat x_{k-1})
\end{split}\end{equation}$$
$$

éšç€$k$çš„å¢åŠ ï¼Œæµ‹é‡å€¼å°±ä¸å†é‡è¦

$$\hat x_k=\hat x_{k-1}+k_k(z_k-\hat x_{k-1})$$

$k_k$: Karlman Gain

å½“å‰ä¼°è®¡å€¼=ä¸Šä¸€æ¬¡çš„ä¼°è®¡å€¼+ç³»æ•° Ã—(å½“å‰æµ‹é‡å€¼-ä¸Šä¸€æ¬¡çš„ä¼°è®¡å€¼)

- åªä¸ä¸Šä¸€æ¬¡çš„ä¼°è®¡å€¼æœ‰å…³ï¼Œé€’å½’ç®—æ³•ã€‚

ä¼°è®¡è¯¯å·®ï¼š$e_e$
æµ‹é‡è¯¯å·®ï¼š$e_m$

$$k_k={e_e\over e_e+e_m}$$

- $e_e\gt\gt e_m: k_k\rightarrow1\quad \hat x_k=\hat x_{k-1}+z_k-\hat x_{k-1}=z_k$ï¼Œå½“ä¼°è®¡è¯¯å·®è¿œå¤§äºæµ‹é‡è¯¯å·®æ—¶ï¼Œä»¥æµ‹é‡å€¼ä¸ºå‡†
- $e_e\lt\lt e_m: k_k\rightarrow0\quad \hat x_k=\hat x_{k-1}$å½“ä¼°è®¡è¯¯å·®è¿œå°äºæµ‹é‡è¯¯å·®æ—¶ï¼Œä»¥ä¼°è®¡å€¼ä¸ºå‡†

**ä»£ç å®ç°**

```
import torch
from torch import nn
from d2l import torch as d2l
#å®šä¹‰batch_normç®—æ³•
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    #å®ç°batch_normåŠŸèƒ½
    #moving_mean/varå…¨å±€çš„æœŸæœ›å’Œæ–¹å·®ï¼Œè¿‘ä¼¼äºæ•´ä¸ªæ•°æ®é›†ä¸Šï¼Œåšæ¨ç†
    #epsé¿å…å‡ºé›¶ï¼Œå¾ˆé‡è¦
    #momentumï¼Œç”¨äºæ›´æ–°movingï¼Œé€šå¸¸å–å›ºå®šå€¼
    if not torch.is_grad_enabled():
    #åšinferenceè€Œétrainï¼Œæ‰€ä»¥ä¸æ±‚æ¢¯åº¦
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
        #epså¹²æ‰°é¡¹é¿å…äº†0å€¼
    else:
        assert len(X.shape) in (2, 4)
        #åˆ¤æ–­2-D/4-Dtensor
        if len(X.shape) == 2:
        #æŒ‰ç‰¹å¾æ±‚å‡å€¼å’Œæ–¹å·®
            mean = X.mean(dim=0)
            # å¯¹è¡Œæ±‚å‡å€¼,å„ä¸ªè¡ŒåŒä¸€åˆ—çš„å…ƒç´ 
            # æ¯ä¸ªåˆ—éƒ½æ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªè¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬çš„ä¸åŒç‰¹å¾
            var = ((X - mean)**2).mean(dim=0)
            # X-meanæ˜¯æŒ‰è¡Œçš„å¹¿æ’­
        else:
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            # å¯¹æ¯ä¸€ä¸ªé€šé“çš„å…¨éƒ¨å…ƒç´ æ±‚å‡å€¼
            var = ((X - mean)**2).mean(dim=(0, 2, 3), keepdim=True)
        X_hat = (X - mean) / torch.sqrt(var + eps)
        #å¯¹Xé‡Œæ¯ä¸ªé€šé“çš„å…¨éƒ¨å…ƒç´ å»æ±‚å¹³å‡
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta
    return Y, moving_mean.data, moving_var.data
    #å¾—åˆ°normalizeè¿‡çš„Yå’Œæ›´æ–°è¿‡çš„moving
```

```
#å®šä¹‰batchnormå—
class BatchNorm(nn.Module):
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
            #ä½œç”¨äºç‰¹å¾
        else:
            shape = (1, num_features, 1, 1)
            #ä½œç”¨äºé€šé“
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        #gammaå’Œbetaæ˜¯è¦å‚æ•°å­¦ä¹ çš„
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)
        #movingæ˜¯è¿­ä»£çš„

    def forward(self, X):
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)

        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.9)
        return Y
    # ç›¸å½“äºBatchnormå®šä¹‰äº†å¹¶åˆå§‹åŒ–ä¸¤ä¸ªè¶…å‚æ•°gammaå’Œbeta
    # åˆå§‹åŒ–äº†moving_mean & var

```

```
#åµŒå¥—è¿›ä¸€ä¸ªLeNetç¥ç»ç½‘ç»œ
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4),
    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4),
    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(), nn.Linear(16 * 4 * 4, 120),
    BatchNorm(120, num_dims=2), nn.Sigmoid(),
    nn.Linear(120, 84), BatchNorm(84, num_dims=2),
    nn.Sigmoid(), nn.Linear(84, 10))
#å¯¹çº¿æ€§å±‚åªæœ‰(256ï¼Œ16*4*4)çŸ©é˜µçš„norm
```

```
#è®­ç»ƒ
lr, num_epochs, batch_size = 1.0, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

```
net[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,))
#è‡ªåŠ¨è®¡ç®—è¡Œï¼Œæ‰€ä»¥é»˜è®¤å¡«å……æ‰€æœ‰åˆ—
```

```
#ç®€æ˜å®ç°
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6),
    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16),
    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(), nn.Linear(16 * 4 * 4, 120),
    nn.BatchNorm1d(120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.BatchNorm1d(84),
    nn.Sigmoid(), nn.Linear(84, 10))
#æ³¨æ„1dä¸2dçš„åŒºåˆ«ã€‚
```

```
nn.Flatten()
#é»˜è®¤ä»ç¬¬1ä¸ªç»´åº¦ï¼ˆè€Œä¸æ˜¯ç¬¬0ç»´ï¼‰å¼€å§‹æ‹‰å¹³ç›´è‡³æœ€åä¸€ä¸ªç»´åº¦
#æ‰€ä»¥Flatten()ä¸€ä¸ªäºŒç»´çŸ©é˜µä¸å‘ç”Ÿä»»ä½•å½¢çŠ¶å˜åŒ–ã€‚
```
