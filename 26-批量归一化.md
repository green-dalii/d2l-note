# 26 - æ‰¹é‡å½’ä¸€åŒ–

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰[![Bilibil](	https://i2.hdslb.com/bfs/archive/c52c4d88d8fe65f6d2ffac27b8ce6cb02dcdcacc.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1X44y1r77r)
## æ‰¹é‡å½’ä¸€åŒ–

- æŸå¤±å‡ºç°åœ¨æœ€åï¼Œåé¢çš„å±‚è®­ç»ƒè¾ƒå¿«
  - æ¢¯åº¦è¶Šå¾€ä¸‹ä¼ é€’è¶Šå°ï¼ˆå°æ•°ç›¸ä¹˜ï¼‰
- æ•°æ®åœ¨æœ€åº•éƒ¨
  - åº•éƒ¨çš„å±‚è®­ç»ƒè¾ƒæ…¢
  - åº•éƒ¨å±‚ä¸€å˜åŒ–ï¼Œæ‰€æœ‰éƒ½å¾—è·Ÿç€å˜
  - é¡¶éƒ¨çš„é‚£äº›å±‚éœ€è¦é‡æ–°å­¦ä¹ å¤šæ¬¡
    - ç›¸å½“äºè®­ç»ƒå¥½äº†å› ä¸ºåº•éƒ¨ç‰¹å¾å˜åŒ–é‡æ¥
  - å¯¼è‡´æ”¶æ•›å˜æ…¢
- æˆ‘ä»¬å¯ä»¥åœ¨å­¦ä¹ åº•éƒ¨å±‚çš„æ—¶å€™é¿å…å˜åŒ–é¡¶éƒ¨å±‚å—ï¼Ÿ

å›ºå®šå°æ‰¹é‡ï¼ˆåœ¨ä¸åŒå±‚è¾“å‡ºï¼‰é‡Œé¢çš„å‡å€¼å’Œæ–¹å·®ï¼š

$$\begin{array}{l}\mu_B={1\over|B|}\sum_{i\in B}x_i\\
\sigma_B^2={1\over|B|}\sum_{i\in B}
(x_i-\mu_B)^2\end{array}$$

ç„¶åå†åšé¢å¤–çš„è°ƒæ•´ï¼š

$$BN(x_i)=\gamma{x_i-\mu_B\over\sqrt{\sigma_B^2+\epsilon}}+\beta$$

- æ¯”ä¾‹${\gamma}$å’Œåç§»ç³»æ•°${\beta}$æ˜¯å­¦ä¹ å‡ºæ¥çš„ï¼Œæˆ–è€…è¯´æ˜¯å­¦åˆ°çš„å‡å€¼å’Œæ–¹å·®
- ä½œç”¨åœ¨
  - å…¨è¿æ¥å±‚å’Œå·ç§¯å±‚è¾“å‡ºä¸Šï¼Œæ¿€æ´»å‡½æ•°ä¹‹å‰
    - reluå˜æˆæ­£æ•°ï¼Œæ‰€ä»¥ä¸èƒ½å†å¸¦å›æ­£è´Ÿå„å¼‚çš„çŠ¶æ€
  - å…¨è¿æ¥å±‚å’Œå·ç§¯å±‚è¾“å…¥ä¸Š
- å¯¹äºå…¨è¿æ¥å±‚ï¼Œä½œç”¨åœ¨ç‰¹å¾ç»´
- å¯¹äºå·ç§¯å±‚ï¼Œä½œç”¨äºé€šé“ç»´ï¼ˆåŒä¸€ä¸ªé€šé“æ‰€æœ‰å…ƒç´ çš„BNï¼‰

**å®ƒçš„ä½œç”¨**

- æœ€åˆçš„è®ºæ–‡è¡¨ç¤ºå¯ä»¥å‡å°‘å†…éƒ¨åå˜é‡è½¬ç§»
- åç»­è®ºæ–‡æŒ‡å‡ºåœ¨å°æ‰¹é‡é‡Œå¢åŠ å™ªéŸ³$\mu,\sigma$ï¼Œéšæœºåç§»å’Œç¼©æ”¾
- æ²¡å¿…è¦å’Œä¸¢å¼ƒæ³•æ··åˆä½¿ç”¨

![](Images/Batch_Normalization.gif)
**æ€»ç»“**

- å›ºå®šä¸€ä¸ªå°æ‰¹é‡çš„å‡å€¼å’Œæ–¹å·®ï¼Œç„¶åå­¦ä¹ å‡ºé€‚åˆçš„åç§»å’Œç¼©æ”¾ï¼›
- å¯ä»¥åŠ é€Ÿæ”¶æ•›ï¼ˆæ›´å¤§çš„å­¦ä¹ ç‡ï¼‰ï¼Œä½†ä¸æ”¹å˜æ¨¡å‹çš„ç²¾åº¦

### ä»£ç å®ç°

åœ¨æµ‹è¯•çš„æ—¶å€™ï¼Œæ ·æœ¬ä¸€ä¸ªä¸€ä¸ªè¿›å…¥ç½‘ç»œï¼Œæ²¡æœ‰åŒºåŸŸå‡å€¼ã€æ–¹å·®å¯æ±‚ã€‚å¯ä»¥ç”¨æµ‹è¯•é›†å¾—åˆ°å‡å€¼ã€æ–¹å·®ä½œä¸ºåˆå€¼ã€‚

å…¨å±€æœŸæœ›çš„æ›´æ–°è¦å€ŸåŠ©**Karlmanæ»¤æ³¢**

å¯¹åŒä¸€ä¸ªç›®æ ‡ï¼Œå¤šæ¬¡æµ‹é‡å€¼$z_1,z_2,z_3,...,z_k$ï¼Œå…¶æœŸæœ›$x_1, x_2,...,x_k$

$$\begin{equation}\begin{split}
\hat x_k&={1\over k}(z_1+z_2+z_3+...+z_{k-1}+z_k)\\
&={1\over k}(z_1+z_2+z_3+...+z_{k-1})+{1\over k}z_k\\
&={1\over k}{k-1\over k-1}(z_1+z_2+z_3+...+z_{k-1})+{1\over k}z_k\\
&={k-1\over k}\hat x_{k-1}+{1\over k}z_k\\
&=\hat x_{k-1}-{1\over k}\hat x_{k-1}+{1\over k}z_k\\
&=\hat x_{k-1}+{1\over k}(z_k-\hat x_{k-1})
\end{split}\end{equation}$$

$$k\uparrow, {1\over k}\rightarrow0,\hat x_{k-1}\rightarrow\hat x_{k}$$

éšç€$k$çš„å¢åŠ ï¼Œæµ‹é‡å€¼å°±ä¸å†é‡è¦

$$\hat x_k=\hat x_{k-1}+k_k(z_k-\hat x_{k-1})$$

$k_k$: Karlman Gain

å½“å‰ä¼°è®¡å€¼=ä¸Šä¸€æ¬¡çš„ä¼°è®¡å€¼+ç³»æ•°Ã—(å½“å‰æµ‹é‡å€¼-ä¸Šä¸€æ¬¡çš„ä¼°è®¡å€¼)


- åªä¸ä¸Šä¸€æ¬¡çš„ä¼°è®¡å€¼æœ‰å…³ï¼Œé€’å½’ç®—æ³•ã€‚

ä¼°è®¡è¯¯å·®ï¼š$e_e$
æµ‹é‡è¯¯å·®ï¼š$e_m$

$$k_k={e_e\over e_e+e_m}$$

   - $e_e\gt\gt e_m: k_k\rightarrow1\quad \hat x_k=\hat x_{k-1}+z_k-\hat x_{k-1}=z_k$ï¼Œå½“ä¼°è®¡è¯¯å·®è¿œå¤§äºæµ‹é‡è¯¯å·®æ—¶ï¼Œä»¥æµ‹é‡å€¼ä¸ºå‡†
   - $e_e\lt\lt e_m: k_k\rightarrow0\quad \hat x_k=\hat x_{k-1}$å½“ä¼°è®¡è¯¯å·®è¿œå°äºæµ‹é‡è¯¯å·®æ—¶ï¼Œä»¥ä¼°è®¡å€¼ä¸ºå‡†

**ä»£ç å®ç°**

```
import torch
from torch import nn
from d2l import torch as d2l
#å®šä¹‰batch_normç®—æ³•
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    #å®ç°batch_normåŠŸèƒ½ 
    #moving_mean/varå…¨å±€çš„æœŸæœ›å’Œæ–¹å·®ï¼Œè¿‘ä¼¼äºæ•´ä¸ªæ•°æ®é›†ä¸Šï¼Œåšæ¨ç†
    #epsé¿å…å‡ºé›¶ï¼Œå¾ˆé‡è¦
    #momentumï¼Œç”¨äºæ›´æ–°movingï¼Œé€šå¸¸å–å›ºå®šå€¼
    if not torch.is_grad_enabled():
    #åšinferenceè€Œétrainï¼Œæ‰€ä»¥ä¸æ±‚æ¢¯åº¦
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
        #epså¹²æ‰°é¡¹é¿å…äº†0å€¼
    else:
        assert len(X.shape) in (2, 4)
        #åˆ¤æ–­2-D/4-Dtensor
        if len(X.shape) == 2:
        #æŒ‰ç‰¹å¾æ±‚å‡å€¼å’Œæ–¹å·®
            mean = X.mean(dim=0)
            # å¯¹è¡Œæ±‚å‡å€¼,å„ä¸ªè¡ŒåŒä¸€åˆ—çš„å…ƒç´ 
            # æ¯ä¸ªåˆ—éƒ½æ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªè¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬çš„ä¸åŒç‰¹å¾
            var = ((X - mean)**2).mean(dim=0)
            # X-meanæ˜¯æŒ‰è¡Œçš„å¹¿æ’­
        else:
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            # å¯¹æ¯ä¸€ä¸ªé€šé“çš„å…¨éƒ¨å…ƒç´ æ±‚å‡å€¼
            var = ((X - mean)**2).mean(dim=(0, 2, 3), keepdim=True)
        X_hat = (X - mean) / torch.sqrt(var + eps)
        #å¯¹Xé‡Œæ¯ä¸ªé€šé“çš„å…¨éƒ¨å…ƒç´ å»æ±‚å¹³å‡
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta
    return Y, moving_mean.data, moving_var.data
    #å¾—åˆ°normalizeè¿‡çš„Yå’Œæ›´æ–°è¿‡çš„moving
```
```
#å®šä¹‰batchnormå—
class BatchNorm(nn.Module):
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
            #ä½œç”¨äºç‰¹å¾
        else:
            shape = (1, num_features, 1, 1)
            #ä½œç”¨äºé€šé“
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        #gammaå’Œbetaæ˜¯è¦å‚æ•°å­¦ä¹ çš„
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)
        #movingæ˜¯è¿­ä»£çš„
        
    def forward(self, X):
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.9)
        return Y
    # ç›¸å½“äºBatchnormå®šä¹‰äº†å¹¶åˆå§‹åŒ–ä¸¤ä¸ªè¶…å‚æ•°gammaå’Œbeta
    # åˆå§‹åŒ–äº†moving_mean & var

```
```
#åµŒå¥—è¿›ä¸€ä¸ªLeNetç¥ç»ç½‘ç»œ
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4),
    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4),
    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(), nn.Linear(16 * 4 * 4, 120),
    BatchNorm(120, num_dims=2), nn.Sigmoid(),
    nn.Linear(120, 84), BatchNorm(84, num_dims=2),
    nn.Sigmoid(), nn.Linear(84, 10))
#å¯¹çº¿æ€§å±‚åªæœ‰(256ï¼Œ16*4*4)çŸ©é˜µçš„norm
```
```
#è®­ç»ƒ
lr, num_epochs, batch_size = 1.0, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```
```
net[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,))
#è‡ªåŠ¨è®¡ç®—è¡Œï¼Œæ‰€ä»¥é»˜è®¤å¡«å……æ‰€æœ‰åˆ—
```
```
#ç®€æ˜å®ç°
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6),
    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16),
    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(), nn.Linear(16 * 4 * 4, 120),
    nn.BatchNorm1d(120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.BatchNorm1d(84),
    nn.Sigmoid(), nn.Linear(84, 10))
#æ³¨æ„1dä¸2dçš„åŒºåˆ«ã€‚
```
```
nn.Flatten()
#é»˜è®¤ä»ç¬¬1ä¸ªç»´åº¦ï¼ˆè€Œä¸æ˜¯ç¬¬0ç»´ï¼‰å¼€å§‹æ‹‰å¹³ç›´è‡³æœ€åä¸€ä¸ªç»´åº¦
#æ‰€ä»¥Flatten()ä¸€ä¸ªäºŒç»´çŸ©é˜µä¸å‘ç”Ÿä»»ä½•å½¢çŠ¶å˜åŒ–ã€‚
```