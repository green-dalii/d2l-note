# 31 - Kaggleå®æˆ˜ - CIFAR-10

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰
[![Bilibil](https://i2.hdslb.com/bfs/archive/1060d9d14c8d840fefaf6972f8b539d05655aa5d.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1Gy4y1M7Cu)
### è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†

æˆ‘ä»¬å°†æ•°æ®åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œä¸€æ—¦æ‰¾åˆ°çš„æœ€ä½³çš„å‚æ•°ï¼Œå°±åœ¨æµ‹è¯•é›†ä¸Šæœ€åæµ‹è¯•ä¸€æ¬¡ï¼Œæµ‹è¯•é›†ä¸Šçš„è¯¯å·®ä½œä¸ºæ³›åŒ–è¯¯å·®çš„è¿‘ä¼¼ã€‚

**å¼•å…¥åŒ…&ä¸‹è½½æ•°æ®**
```
import collections
import math
import os
import shutil
# å€’è…¾æ–‡ä»¶
import pandas as pd
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',
                                '2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')
#æ­¤æ•°æ®é›†æ˜¯d2l courseå­˜åœ¨Amazonçš„demoè€Œéå®Œæ•´çš„æ¥è‡ªäºkaggleçš„cifar-10

demo = True
#åˆ¤æ–­æ˜¯ä¸‹è½½å®Œæ•´æ•°æ®é›†è¿˜æ˜¯å°æ ·

if demo:
    data_dir = d2l.download_extract('cifar10_tiny')
else:
    data_dir = '../code/data/cifar-10/'
```
**åˆ’åˆ†æ•°æ®**

```
def read_csv_labels(fname):
    #labelså­˜åœ¨csvæ–‡ä»¶é‡Œ
    with open(fname, 'r') as f:
        lines = f.readlines()[1:]
        #ç¬¬ä¸€è¡Œæ˜¯æŠ¬å¤´
        #readlines()è¯»å–æ•´ä¸ªæ–‡ä»¶ï¼Œä»¥è¡Œè¯»å–ã€‚
    tokens = [l.rstrip().split(',') for l in lines]
    #strip()åˆ é™¤å­—ç¬¦ä¸²å¤´å°¾ç‰¹å®šçš„å­—ç¬¦ï¼Œé»˜è®¤æ˜¯ç©ºæ ¼å’Œæ¢è¡Œç¬¦
    #rstrip()åœ¨æ­¤åˆ å»å³ä¾§æ¢è¡Œç¬¦
    #split()ä¸­é—´æŒ‰ç…§é€—å·åˆ†éš”
    return dict(((name, label) for name, label in tokens))
    #æŠŠ[]æ¢æˆ()çš„åˆ—è¡¨ç”Ÿæˆå¼ï¼Œå°±æ˜¯ä¸€ä¸ªgenerator
    #å¯ä»¥ç”¨list/tuple(generator)æ–¹å¼è½¬æ¢
    #å¦‚æœæ˜¯æˆå¯¹å…ƒç´ ï¼Œå°±å¯ä»¥dict(generator)è½¬æ¢æˆå­—å…¸ï¼Œç›¸å½“äºdict(list)çš„æ–¹å¼å®šä¹‰å­—å…¸
    #ä»¥ä¸Šçš„æ–¹å¼éƒ½ä¼šéå†generator
    #å½“åŒä¸€ä¸ªgeneratorè¢«éå†è¿‡ä¸€éæ—¶ï¼Œå†æ¬¡è°ƒç”¨ä¼šè¿”å›None
    #æ¯”å¦‚ç”Ÿæˆå™¨g,å…ˆlist(g)ï¼Œä¼šå¾—åˆ°æ‰€æœ‰å…ƒç´ 
    #å†tuple(g),å¾—åˆ°ç©ºå…ƒç»„ï¼Œæ‰€ä»¥éœ€è¦é‡æ–°å®šä¹‰g

labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))

def copyfile(filename, target_dir):
    os.makedirs(target_dir, exist_ok=True)
    #exist_ok(default)=Falseï¼Œå¦‚æœç›®æ ‡æ–‡ä»¶å¤¹å·²å­˜åœ¨ä¼šæŠ¥é”™
    #å¯¹äºå·²å­˜åœ¨çš„æ–‡ä»¶å¤¹ä¸ä¼šè¦†ç›–
    shutil.copy(filename, target_dir)
    #copyæ–‡ä»¶åˆ°ç›®å½•

def reorg_train_valid(data_dir, labels, valid_ratio):
    n = collections.Counter(labels.values()).most_common()[-1][1]
    #collectionsæ˜¯é«˜æ€§èƒ½å®¹é‡æ•°æ®ç±»å‹
    #è¿”å›çš„æ˜¯ä¸€ä¸ªç±»çš„å®ä¾‹
    #å…ƒç´ ä»¥å­—å…¸å½¢å¼ä¿å­˜
    #most_common(num)è¿”å›æœ€é¢‘ç¹çš„ç±»å‹å’Œé¢‘ç‡ï¼Œå¦‚æœä¸æŒ‡å®šä¸ªæ•°ï¼Œdefaultæ‰€æœ‰ç±»ä»å¤šåˆ°å°‘æ’åˆ—
    #å–æœ€å°‘çš„ç±»çš„å…ƒç´ æ•°
    n_valid_per_label = max(1, math.floor(n * valid_ratio))
    print(n_valid_per_label)
    #math.floorå‘ä¸‹å–æ•´
    #n * valid_ratioå†³å®šäº†ä¸€ä¸ªä¸Šé™
    label_count = {}
    for train_file in os.listdir(os.path.join(data_dir, 'train')):
        #os.listdir()è¿”å›æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹åˆ—è¡¨
        #trainæ–‡ä»¶å¤¹ä¸‹æ˜¯è§£å‹åçš„ä¸€å¼ å¼ å›¾ç‰‡ï¼Œå³train_file
        label = labels[train_file.split('.')[0]]
        #åˆ†ç¦»æ–‡ä»¶ï¼ˆå›¾ç‰‡ï¼‰åä¸åç¼€ï¼Œåªæå–æ–‡ä»¶åï¼Œä¹Ÿå°±æ˜¯å›¾ç‰‡çš„åºå·
        #å†åœ¨lablesè¿™ä¸ªå­—å…¸é‡Œæ‹¿å‡ºå¯¹åº”æ ‡å·çš„ç±»å‹å
        #print(labels[train_file.split('.')[0]])='ship'
        fname = os.path.join(data_dir, 'train', train_file)
        #æŠŠæ–‡ä»¶è·¯å¾„èµ‹å€¼ç»™fname
        copyfile(
            fname,
            os.path.join(data_dir, 'train_valid_test', 'train_valid', label))
        #æŠŠæ–‡ä»¶å¤åˆ¶åˆ°è®­ç»ƒéªŒè¯æ–‡ä»¶å¤¹
        if label not in label_count or label_count[label] < n_valid_per_label:
             #åˆ›å»ºéªŒè¯é›†
            #å¦‚æœlabelä¸åœ¨dict_label_counté‡Œæˆ–è€…å€¼å°äºn_valid_per_label
            #ä¹Ÿå°±æ˜¯è¯´å¦‚æœè¯¥ç±»å‹çš„æ•°ç›®æ²¡æœ‰è¾¾åˆ°ä¸Šé™
            copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'valid', label))
            #è°ƒç”¨å‰é¢å®šä¹‰çš„copyfile()å‡½æ•°
            #å…ˆåˆ›å»º../data/kaggle_cifar10_tiny/train_valid_test/valid/labels[train_file.split('.')[0]]æ–‡ä»¶å¤¹
            #ç¬¬ä¸€ä¸ªå°±æ˜¯../train_valid_test/valid/shipæ–‡ä»¶å¤¹
            #å†æŠŠå›¾ç‰‡æ”¾è¿›å»
            label_count[label] = label_count.get(label, 0) + 1
            #ç”¨å­—å…¸è®¡æ•°
            #dict.get(key, default=None),æŸ¥æ‰¾ç±»å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›é»˜è®¤å€¼
            #å€Ÿæ­¤èµ‹å€¼dict[key]
            #é€šè¿‡+1è®¡æ•°ï¼Œå¹¶é‡æ–°èµ‹å€¼
        else:
            copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'train', label))
            #åˆ›å»ºè®­ç»ƒé›†
    return n_valid_per_label
    #è¿”å›æµ‹è¯•é›†çš„å¤§å°
#æ•´ä½“æ˜¯æŠŠæ–‡ä»¶å…¨éƒ¨å¤åˆ¶åˆ°train_validå¹¶ä¸”åˆ†ä¸ºtrainå’Œvalidï¼Œæ¯ä¸ªä¸‹é¢å†ä»¥ç±»åå»ºç«‹æ–‡ä»¶å¤¹ç»†åˆ†

def reorg_test(data_dir):
    for test_file in os.listdir(os.path.join(data_dir, 'test')):
        copyfile(
            os.path.join(data_dir, 'test', test_file),
            os.path.join(data_dir, 'train_valid_test','test','unkonwn'))
        #åˆ›å»ºæµ‹è¯•é›†ï¼Œå¹¶ä¸”åªæœ‰ä¸€ä¸ªunknownæ–‡ä»¶å¤¹ï¼Œè¡¨ç¤ºæ‰€æœ‰ç±»å‹å‡ä¸ºä¸å¯çŸ¥

def reorg_cifar10_data(data_dir, valid_ratio):
    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))
    reorg_train_valid(data_dir, labels, valid_ratio)
    reorg_test(data_dir)
#è°ƒç”¨ä¸Šæ–¹ä¸‰ä¸ªå‡½æ•°

batch_size = 32 if demo else 128
valid_ratio = 0.1
reorg_cifar10_data(data_dir, valid_ratio)
```

**æ•°æ®åŠ è½½&é¢„å¤„ç†**

```
transform_train = torchvision.transforms.Compose([
    torchvision.transforms.Resize(40),
    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),
                                                ratio=(1.0, 1.)),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],
                                     [0.2023, 0.1994, 0.2010])])
transform_test = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],
                                     [0.2023, 0.1994, 0.2010])])
#æ³¨æ„Compose([])ä¼ å…¥çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå› ä¸ºæ²¡æœ‰[]é€ æˆä¼ å…¥å¤šä¸ªå‚æ•°ä¼šæŠ¥é”™

train_ds, train_valid_ds = [
    torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'train_valid_test', folder),
        transform=transform_train) for folder in ['train', 'train_valid']]
#ImageFolderæ˜¯ä¸€ä¸ªé€šç”¨çš„æ•°æ®é›†åŠ è½½å™¨ï¼Œä¹Ÿå°±æ˜¯å¯¹è‡ªå®šä¹‰çš„å›¾ç‰‡æ–‡ä»¶å¤¹
#ä¼ å…¥çš„æ˜¯æ ¹ç›®å½•åœ°å€ï¼Œä¹Ÿå°±æ˜¯å„ç±»åˆ«ç›®å½•çš„ä¸Šä¸€çº§åœ°å€
#æŠŠdata_dir/rain_valid_test/trainä½œä¸ºè®­ç»ƒé›†
#æŠŠdata_dir/rain_valid_test/train_validä½œä¸ºè®­ç»ƒéªŒè¯é›†
valid_ds, test_ds = [
    torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'train_valid_test', folder),
        transform=transform_test) for folder in ['valid', 'test']]
#æŠŠdata_dir/rain_valid_test/validä½œä¸ºéªŒè¯é›†
#æŠŠdata_dir/rain_valid_test/test/unknownä½œä¸ºæµ‹è¯•é›†
```

```
train_ds, train_valid_ds = [
    torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'train_valid_test', folder),
        transform=transform_train) for folder in ['train', 'train_valid']]
#ImageFolderæ˜¯ä¸€ä¸ªé€šç”¨çš„æ•°æ®é›†åŠ è½½å™¨ï¼Œä¹Ÿå°±æ˜¯å¯¹è‡ªå®šä¹‰çš„å›¾ç‰‡æ–‡ä»¶å¤¹
#ä¼ å…¥çš„æ˜¯æ ¹ç›®å½•åœ°å€ï¼Œä¹Ÿå°±æ˜¯å„ç±»åˆ«ç›®å½•çš„ä¸Šä¸€çº§åœ°å€
#æŠŠdata_dir/rain_valid_test/trainä½œä¸ºè®­ç»ƒé›†
#æŠŠdata_dir/rain_valid_test/train_validä½œä¸ºè®­ç»ƒéªŒè¯é›†
valid_ds, test_ds = [
    torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'train_valid_test', folder),
        transform=transform_test) for folder in ['valid', 'test']]
#æŠŠdata_dir/rain_valid_test/validä½œä¸ºéªŒè¯é›†
#æŠŠdata_dir/rain_valid_test/test/unknownä½œä¸ºæµ‹è¯•é›†
```
```
train_iter, train_valid_iter = [
    torch.utils.data.DataLoader(
        dataset, batch_size, shuffle=True, drop_last=True)
    for dataset in (train_ds, train_valid_ds)]
#drop_last=Trueè¡¨ç¤ºè£å»æœ€åä¸æ»¡batch_sizeçš„ä¸€æ‰¹æ•°æ®ï¼Œdefault=False
#ä¸ºå®ç°éšæœºSGDï¼Œå¿…é¡»shuffle=Trueï¼Œå¦åˆ™æ•°æ®é›†é¡ºåºä¹Ÿä¼šè¢«è¯¯å­¦
valid_iter = torch.utils.data.DataLoader(
    valid_ds, batch_size, shuffle=False, drop_last=True)

test_iter =  torch.utils.data.DataLoader(test_ds, batch_size, shuffle=True,
                                         drop_last=False)
```
**å®šä¹‰æ¨¡å‹**
```
def get_net():
    num_classes = 10
    net = d2l.resnet18(num_classes, 3)
    return net

loss = nn.CrossEntropyLoss(reduction='none')
```
```
def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices,
          lr_period, lr_decay):
    #éšæœºæ¢¯åº¦ä¼˜åŒ–æ”¶æ•›çš„å‰ææ˜¯lrçš„é€æ¸å‡å°ï¼Œå‡å°‘æŠ–åŠ¨
    #æ¯éš”(lr_period)ä¸ªepoch,lrå‡å°‘(lr_decay)
    trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,
                              weight_decay=wd)
    #momentumä½¿ç”¨æŒ‡æ•°åŠ æƒå¹³å‡ä¹‹åçš„æ¢¯åº¦ä»£æ›¿åŸæ¢¯åº¦è¿›è¡Œå‚æ•°æ›´æ–°ã€‚
    #å› ä¸ºæ¯ä¸ªæŒ‡æ•°åŠ æƒå¹³å‡åçš„æ¢¯åº¦å«æœ‰ä¹‹å‰æ¢¯åº¦çš„ä¿¡æ¯ï¼ŒåŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•å› æ­¤å¾—åã€‚
    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)
    #å­¦ä¹ ç‡ä¸‹é™ç‡æ¯lr_period*epoch, lr = lr * lr_decay
    num_batches, timer = len(train_iter), d2l.Timer()
    legend = ['train loss', 'train acc']
    if valid_iter is not None:
        legend.append('valid acc')
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=legend)
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])
    for epoch in range(num_epochs):
        net.train()
        metric = d2l.Accumulator(3) # loss, accuracy, numel
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            l, acc = d2l.train_batch_ch13(net, features, labels, loss, trainer, devices)
            metric.add(l, acc, labels.shape[0])
            #train_iterè¿”å›featureså’Œlabels,labelsæ˜¯ä¸€ç»´å‘é‡ï¼Œshape[0]å°±æ˜¯ä¸€ä¸ªæ ‡é‡
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
            #if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches, (metric[0] / metric[2], metric[1] / metric[2], None))
        if valid_iter is not None:
            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)
            animator.add(epoch + 1, (None, None, valid_acc))
        scheduler.step()
        #æ¯ä¸ªepochåscheduleréœ€è¦æ›´æ–°ä¸€ä¸‹
    measures = (f'train loss {metric[0] / metric[2]:.3f}, '
                f'train acc {metric[1] / metric[2]:.3f}')
    #ä¸¤å¥f''ä¹‹é—´ä¸ç”¨é€—å·ï¼Œå°±æ˜¯åŒä¸€ä¸ªå­—ç¬¦ä¸²
    if valid_iter is not None:
        measures += f', valid acc {valid_acc:.3f}'
    print(measures + f'\n{metric[2] * num_epochs / timer.sum():.1f}'
          f' examples/sec on {str(devices)}')
```