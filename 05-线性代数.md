# 05 - 线性代数
---
### 🎦本节课程视频地址👉[Bilibil](https://www.bilibili.com/video/BV1eK4y1U7Qy)

### 🎦关于线性代数相关知识，强烈推荐**3Blue1Brown**的超棒教程👉[Bilibili](https://www.bilibili.com/video/BV1ys411472E)

![3Blue1Brown视频封面](https:////i2.hdslb.com/bfs/archive/c81a8eb032f3eaa1afd604272a410ac6896f281e.jpg@380w_240h_100Q_1c.webp)

## 1.基本运算
- **向量点乘**：按元素相乘结果为标量，当相互正交时为0
$$a^Tb=\sum_{i} a_ib_i$$

- **向量范数**：分为L1范数、L2范数、p-范数、$\infty$-范数。其中定义分别为：
  - $L_1$范数：
  $$||x||_1=\sum_{i=1}^n|x_i|$$
  - $L_2$范数：
  $$||x||_2=\sqrt{\sum_{i=1}^nx_i^2}$$
  - p-范数：
  $$||x||_p=(\sum_{i=1}^n|x_i|^p)^{1\over p},p>1$$
  - $\infty$-范数：
  $$||x||_{\infty}=\max_i|x_i|$$

- **矩阵乘以向量**：需保证矩阵列数与向量行数相等，最终得到向量，(m, n)·(n, 1)=(m,1)。矩阵的作用相当于对向量进行空间的扭曲变换（可参考3Blue1Brown视频）
$$c=Ab,\ where\ c_i=\sum_jA_{ij}b_j$$

- **矩阵乘以矩阵**：相当于矩阵A分别与矩阵B中的每一列（相当于向量）做矩阵与向量乘法
$$C=AB\ where\ C_{ik}=\sum_jA_{ij}B{jk}$$

- **Frobenius范数**：将所有元素做平方和再开根号，因矩阵范数求解较麻烦，Frob范数多用于替代矩阵范数
$$||A||_{Frob}=[\sum_{ij}A_{ij}^2]^{1\over2}$$

- **特征向量和特征值**：做矩阵与向量乘法时，如果向量 $x$ 经过矩阵 $A$ 扭曲变换后，没有改变原向量 $x$ 的方向（长度可改变为比例 $\lambda$），则称 $x$ 为特征向量，$\lambda$ 为特征值，对称矩阵总能找到特征向量
$$Ax=\lambda\ x$$

## 2.线性代数的Pytorch写法
- 标量：`torch.tensor([3.0])`
- 向量：`torch.tensor([list])`
- 向量点积：`torch.dot(x,y)`，等价于先按元素乘，在求和：`sum(x * y)`
- 矩阵：`torch.arange(20).reshape(5,4)`
- 矩阵转置：`A.T`
- 两个矩阵按元素乘（哈达玛积，Hadamard product）[不常用]：`A * B`
- 矩阵求和，得到标量：`A.sum()`
- 矩阵按轴求和，则该轴维度变为1：`A.sum(axis=n)`  |  `A.sum(axis=[a,b])`
- 矩阵求和时保留维度信息：`A.sum(axis=1,keepdims=True)`
- 矩阵求均值：`A.mean()`
- 矩阵向量积$Ax$，需保证矩阵列数与向量维数相等：`torch.mv(A,x)`
- 矩阵相乘$AB$，可以看作是执行m次矩阵向量积，并拼合结果：`torch.mm(A,B)`
- $L_2$范数：`torch.norm(u)`
- $L_1$范数：`torch.abs(u).sum()`
- 矩阵的Frob范数：`torch.norm(A)`

> Markdown数学公式语法参考👉[这里](https://www.jianshu.com/p/25f0139637b7)