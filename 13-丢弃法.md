# 13 - 丢弃法

---

### 🎦 本节课程视频地址 👉
[![Bilibil](https://i1.hdslb.com/bfs/archive/f68d47e72ff00bd216c4c4fc8d44006540d91370.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1Y5411c7aY?spm_id_from=333.999.0.0)

### 动机

一个好的模型需要对输入数据的扰动鲁棒

使用有噪音的数据等价于Tikhonov正则

丢弃法：在层之间加入噪音。

### 无偏差的加入噪音

对${\bf x}$加入噪音得到${\bf x\prime}$，我们希望

${\bf E[x\prime]=x}$

希望虽然加入了噪音，但期望不发生改变。

丢弃发对每个元素进行如下扰动：

$$x_i^\prime=\begin{cases}
0 &with\ pobability\ p\\
{x_i\over1-p} &otherwise\end{cases}$$

相当于有一定概率 $p$ 使一个值变为零，或者使之变大。

### 使用丢弃法

通常将丢弃法作用在隐藏全连接层的输出上。

$${\bf h}=\sigma({\bf W_1x+b_1})$$
$${\bf h^\prime}=dropout({\bf h})$$
$${\bf o}={\bf W_2h^\prime+b_2}$$
$${\bf y}=softmax({\bf o})$$

![使用丢弃法](\Images/1_iWQzxhVlvadk6VAJjsgXgg.png)


### 推理中的丢弃法

正则项只在训练中使用：他们影响模型参数的更新，在推理过程中，丢弃法直接返回输入

$${\bf h}=dropout({\bf h})$$

这样也能保证确定性的输出。

在最原始的理念里，dropout相当于每一次取一批量的子神经网络做平均。实验中，表现出的效果更像是一个正则项。

常作用在多层感知机的隐藏层输出上；

丢弃概率使控制模型复杂度的超参数。

## 代码实现

同样以Mnist图库为例。

- **定义dropout函数**

```
import torch
from torch import nn
from d2l import torch as d2l

def dropout_layer(X, dropout):
    assert 0 <=dropout <= 1
    #如果其值为假（即为0），打印出错信息,然后终止程序运行。
    #在函数开始处检验传入参数的合法性
    if dropout == 1:
        return torch.zeros_like(X)
    if dropout == 0:
        return X
    mask = (torch.randn(X.shape)  > dropout).float()
    #randn(random normal)
    #对tensor做bool返回的也是tensor的广播式比较
    #确定哪一项元素被清零
    
    return mask * X / (1.0 -dropout)
    #为了能快速计算
```

- **训练**

```
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256
#x.shape=(1,784),y.shape=(1,10),w1.shape=(784,256),w2.shape=(256,256),w3.shape=(256,10)
dropout1, dropout2 = 0.2, .5

class Net(nn.Module):
    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2, is_training=True):
        super(Net, self).__init__()
        #super() 函数是用于调用父类(超类)的一个方法。
        #super(Class, self).xxx，或者简写为super().xxx
        #这里调用的就是Net的父类nn.Module的__init__()
        #里面包含了所需的方法。
        self.num_inputs = num_inputs
        self.training = is_training
        self.lin1 = nn.Linear(num_inputs, num_hiddens1)
        #(784,256) h1=xWT
        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)
        #(256,256)
        self.lin3 = nn.Linear(num_hiddens2, num_outputs)
        #(256,10)
        self.relu = nn.ReLU()
    
    def forward(self, X):
        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))
        #(1,784)*(784,256)
        if self.training == True:
            H1 = dropout_layer(H1, dropout1)
            #h1'=dropout(h1)
        H2 = self.relu(self.lin2(H1))
         #(1,256)*(256,256)=(1,256)
        if self.training == True:
            H2 = dropout_layer(H2, dropout2)
            #h2'=dropout(h2)
        out = self.lin3(H2)
        #(256,256)*(256,10)=(1,10)
        return out

net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)

num_epochs, lr, batch_size = 10, .5, 256
loss = nn.CrossEntropyLoss()
#打包了softmax
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
#X=(256,784)
trainer = torch.optim.SGD(net.parameters(), lr=lr)
#此处调用了net这个class里面所有的属性，w1-w3,b1-b3.
#net.parameters(recurse=True)就是返回模块和所有子模块的参数。
#返回的是一个生成器
#所以要用循环，或者内置循环的方法调用。
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```
![训练结果](\Images/微信截图_20211217154852.png)

- **简介实现**

```
net = nn.Sequential(nn.Flatten(),
                  nn.Linear(784, 256),
                  nn.ReLU(),
                  nn.Dropout(dropout1),
                  nn.Linear(256, 256),
                  nn.ReLU(),
                  nn.Dropout(dropout2),
                  nn.Linear(256, 10))
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights)

trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```
![](\Images/微信截图_20211217155233.png)