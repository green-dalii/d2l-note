## GoogLeNet

**Inception块**：小学生才做选择题，我全都要，4个路径从不同层面抽取信息，然后在输出通道维合并。

![](\Images/Screenshot-from-2018-10-17-11-14-10.png)

- 输出和输入等同高宽，通道数变多。
  - 调超参数决定通道数分配 
- 1x1：降低通道数来控制模型复杂度。
- 每条路上的通道数可能不一样。
- MaxPooling：提取空间特性，增加鲁棒性。

**FLOPS(Floating-point Operations Per Second)**

### GoogLenet

5stages, 9inceptions

![](\Images/1__rCyzi7fQzc_Q1gCqSLM1g.png)

Input: 3x224x224
Stage 1: 7x7 Conv & 3x3 MaxPool;
Stage 2: 1x1 + 3x3 Conv & 3x3 MaxPool;
Output1: 192x28x28
Stage 3: 2xInception block & 3x3 MaxPool;
Output2: 480x14x14
Stage 4: 5xInception block & 3x3 MaxPool;
Output3: 832x7x7 
Stage 5: 2xInception block & Global AvgPool
Output final: 1024x1x1

![](\Images/1_WfKerFhMvUGti7MWVQ81XQ.png)

### Inception有各种后续变种

-Inception-BN(V2)：使用了batch normalization
-Inception-V3：修改了Inception(诡异的1x7-7x1Conv)
-Inception-V4：使用残差连接

### 总结

- 使用4条有不同超参数的卷积层和池化层的通路来抽取不同的信息；
  - 它的一个主要优点是模型参数小，计算复杂度低
- GooLeNet用了9个Inception块，是第一个达到上百层的网络
  - 后续存在改进

### 代码实现

```
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

#定义Inception block
class Inception(nn.Module):
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        #super(subclass_name,self)
        #传入子类，找到父类，使子类实例继承父类实例
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
        #Conv2d, default_stride=1, default_padding=0
        #pass1 1x1conv
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        #pass2 1x1+3x3conv
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        #pass3 1x1+5x5conv
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)
        #pass4 3x3MaxPool+1x1conv
        
    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        return torch.cat((p1, p2, p3, p4), dim=1)
```
```
#net and stages
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),
                   nn.ReLU(),
                   nn.Conv2d(64, 192, kernel_size=3, padding=1),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),
                   Inception(256, 128, (128, 192), (32, 96), 64),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),
                   Inception(512, 160, (112, 224), (24, 64), 64),
                   Inception(512, 128, (128, 256), (24, 64), 64),
                   Inception(512, 112, (144, 288), (32, 64), 64),
                   Inception(528, 256, (160, 320), (32, 128), 128),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),
                   Inception(832, 384, (192, 384), (48, 128), 128),
                   nn.AdaptiveAvgPool2d((1,1)),
                   nn.Flatten())

net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))
```

```
#test
X = torch.rand(size=(1, 1, 96, 96))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
```