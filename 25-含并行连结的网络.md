# 25 - å«å¹¶è¡Œè¿æ¥çš„ç½‘ç»œGoogLeNetã€Inception V3

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰[![Bilibil](https://i2.hdslb.com/bfs/archive/537f10ce693bd7113b4eba116e64ec4ab443039d.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1b5411g7Xo)
## GoogLeNet

**Inceptionå—**ï¼šå°å­¦ç”Ÿæ‰åšé€‰æ‹©é¢˜ï¼Œæˆ‘å…¨éƒ½è¦ï¼Œ4ä¸ªè·¯å¾„ä»ä¸åŒå±‚é¢æŠ½å–ä¿¡æ¯ï¼Œç„¶ååœ¨è¾“å‡ºé€šé“ç»´åˆå¹¶ã€‚

![](\Images/Screenshot-from-2018-10-17-11-14-10.png)

- è¾“å‡ºå’Œè¾“å…¥ç­‰åŒé«˜å®½ï¼Œé€šé“æ•°å˜å¤šã€‚
  - è°ƒè¶…å‚æ•°å†³å®šé€šé“æ•°åˆ†é… 
- 1x1ï¼šé™ä½é€šé“æ•°æ¥æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ã€‚
- æ¯æ¡è·¯ä¸Šçš„é€šé“æ•°å¯èƒ½ä¸ä¸€æ ·ã€‚
- MaxPoolingï¼šæå–ç©ºé—´ç‰¹æ€§ï¼Œå¢åŠ é²æ£’æ€§ã€‚

**FLOPS(Floating-point Operations Per Second)**

### GoogLenet

5stages, 9inceptions

![](\Images/1__rCyzi7fQzc_Q1gCqSLM1g.png)

Input: 3x224x224
Stage 1: 7x7 Conv & 3x3 MaxPool;
Stage 2: 1x1 + 3x3 Conv & 3x3 MaxPool;
Output1: 192x28x28
Stage 3: 2xInception block & 3x3 MaxPool;
Output2: 480x14x14
Stage 4: 5xInception block & 3x3 MaxPool;
Output3: 832x7x7 
Stage 5: 2xInception block & Global AvgPool
Output final: 1024x1x1

![](\Images/1_WfKerFhMvUGti7MWVQ81XQ.png)

### Inceptionæœ‰å„ç§åç»­å˜ç§

-Inception-BN(V2)ï¼šä½¿ç”¨äº†batch normalization
-Inception-V3ï¼šä¿®æ”¹äº†Inception(è¯¡å¼‚çš„1x7-7x1Conv)
-Inception-V4ï¼šä½¿ç”¨æ®‹å·®è¿æ¥

### æ€»ç»“

- ä½¿ç”¨4æ¡æœ‰ä¸åŒè¶…å‚æ•°çš„å·ç§¯å±‚å’Œæ± åŒ–å±‚çš„é€šè·¯æ¥æŠ½å–ä¸åŒçš„ä¿¡æ¯ï¼›
  - å®ƒçš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯æ¨¡å‹å‚æ•°å°ï¼Œè®¡ç®—å¤æ‚åº¦ä½
- GooLeNetç”¨äº†9ä¸ªInceptionå—ï¼Œæ˜¯ç¬¬ä¸€ä¸ªè¾¾åˆ°ä¸Šç™¾å±‚çš„ç½‘ç»œ
  - åç»­å­˜åœ¨æ”¹è¿›

### ä»£ç å®ç°

```
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

#å®šä¹‰Inception block
class Inception(nn.Module):
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        #super(subclass_name,self)
        #ä¼ å…¥å­ç±»ï¼Œæ‰¾åˆ°çˆ¶ç±»ï¼Œä½¿å­ç±»å®ä¾‹ç»§æ‰¿çˆ¶ç±»å®ä¾‹
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
        #Conv2d, default_stride=1, default_padding=0
        #pass1 1x1conv
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        #pass2 1x1+3x3conv
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        #pass3 1x1+5x5conv
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)
        #pass4 3x3MaxPool+1x1conv
        
    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        return torch.cat((p1, p2, p3, p4), dim=1)
```
```
#net and stages
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),
                   nn.ReLU(),
                   nn.Conv2d(64, 192, kernel_size=3, padding=1),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),
                   Inception(256, 128, (128, 192), (32, 96), 64),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),
                   Inception(512, 160, (112, 224), (24, 64), 64),
                   Inception(512, 128, (128, 256), (24, 64), 64),
                   Inception(512, 112, (144, 288), (32, 64), 64),
                   Inception(528, 256, (160, 320), (32, 128), 128),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),
                   Inception(832, 384, (192, 384), (48, 128), 128),
                   nn.AdaptiveAvgPool2d((1,1)),
                   nn.Flatten())

net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))
```

```
#test
X = torch.rand(size=(1, 1, 96, 96))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
```