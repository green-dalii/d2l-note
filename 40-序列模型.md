# 40 - åºåˆ—æ¨¡å‹

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰
[![Bilibil](https://i0.hdslb.com/bfs/archive/527d7d30d91a5761502fa45f9c08cbd7d57bb550.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1L44y1m768)
## åºåˆ—æ¨¡å‹

- å®é™…ä¸­å¾ˆå¤šæ•°æ®æ˜¯æœ‰æ—¶åºç»“æ„çš„
  - å¦‚ç”µå½±çš„è¯„åˆ†
  - éŸ³ä¹ã€è¯­è¨€ã€æ–‡æœ¬å’Œè§†é¢‘éƒ½æ˜¯è¿ç»­çš„
  - è‚¡ç¥¨çš„ä»·æ ¼
  - äººä¸äººä¹‹é—´çš„äº’åŠ¨

**ç»Ÿè®¡å·¥å…·**

- åœ¨æ—¶é—´ $t$ è§‚å¯Ÿåˆ° $x_t$ï¼Œé‚£ä¹ˆå¾—åˆ° $T$ ä¸ªä¸ç‹¬ç«‹çš„éšæœºå˜é‡ $(x_1,...,x_T)~p(\bf x)$
- ä½¿ç”¨æ¡ä»¶æ¦‚ç‡å±•å¼€ï¼ˆè´å¶æ–¯å…¬å¼ï¼‰
$p(a,b)=p(a)p(b|a)=p(b)p(a|b)$

æ¨å¹¿åˆ°è¿™ä¸€ç³»åˆ—äº‹ä»¶åŒæ—¶å‘ç”Ÿçš„æ¦‚ç‡ï¼Œæ¢è¨€ä¹‹ï¼Œå‰ $T-1$ ä¸ªäº‹ä»¶å·²ç»å‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼Œç¬¬ $T$ ä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ï¼š

æ­£å‘ï¼š

$p({\bf x})=p(x_1)\cdot p(x_2|x_1)\cdot p(x_3|x_1,x_2)\cdot ...\cdot p(x_T|x_1,...,x_{T-1})$

åå‘ï¼š

$p({\bf x})=p(x_T)\cdot p(x_{T-1}|x_T)\cdot p(x_{T-2}|x_{T-1},x_T)\cdot ...\cdot p(x_1|x_{T-1},...,x_2)$

åå‘ä»æœªæ¥æ¨å¹¿åˆ°è¿‡å»ï¼Œå¹¶ä¸å®Œå…¨æœ‰ç‰©ç†æ„ä¹‰

å¯¹æ¡ä»¶æ¦‚ç‡å»ºæ¨¡ï¼š

$p(x_t|x_1,...,x_{t-1})=p(x_t|f(x_1,...,x_{t-1}))$

å¯¹è§è¿‡çš„æ•°æ®å»ºæ¨¡ï¼Œä¹Ÿç§°è‡ªå›å½’æ¨¡å‹(Autoregressive Model)ã€‚

![](\Images/040-01.png)

**é©¬å°”ç§‘å¤«å‡è®¾**

å‡è®¾å½“å‰æ•°æ®åªè·Ÿ $\tau$ ä¸ªè¿‡å»æ•°æ®ç‚¹ç›¸å…³

$p(x_t|x_1,...,x_{t-1})=p(x_t|x_{t-\tau},...,x_{t-1})=p(x_t|f(x_{t-\tau},...,x_{t-1}))$

**æ½œå˜é‡æ¨¡å‹**

å¼•å…¥æ½œå˜é‡ $h_t$ æ¥è¡¨ç¤ºè¿‡å»ä¿¡æ¯ $h_t=f(x_1,...,x_{t-1})$
- è¿™æ · $x_t=p(x_t|h_t)$

**æ€»ç»“**

- åœ¨æ—¶åºæ¨¡å‹ä¸­ï¼Œå½“å‰æ•°æ®è·Ÿä¹‹å‰è§‚å¯Ÿåˆ°çš„æ•°æ®ç›¸å…³
- è‡ªå›å½’æ¨¡å‹ä½¿ç”¨è‡ªèº«è¿‡å»æ•°æ®æ¥é¢„æµ‹æœªæ¥
- é©¬å°”å¯å¤«æ¨¡å‹å‡è®¾å½“å‰åªè·Ÿæœ€è¿‘å°‘æ•°æ•°æ®ç›¸å…³
- æ½œå˜é‡æ¨¡å‹ä½¿ç”¨æ½œå˜é‡æ¥æ¦‚æ‹¬å†å²ä¿¡æ¯

### ä»£ç æ¨¡æ‹Ÿé©¬å°”å¯å¤«ç®—æ³•

**åˆ›å»ºæ•°æ®é›†**
```
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l

T = 1000  # æ€»å…±äº§ç”Ÿ1000ä¸ªç‚¹
time = torch.arange(1, T + 1, dtype=torch.float32)
x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))
#torch.normal(mean, std, size)
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))
```
```
tau = 4
features = torch.zeros((T - tau, tau))
#(996, 4)å…¨é›¶
for i in range(tau):
    features[:, i] = x[i: T - tau + i]
    #å¯¹æ¯è¡Œçš„ç¬¬iåˆ—èµ‹å€¼ï¼Œ996è¡Œ
    #æŒ‰é¡ºåºæŠŠæ¯å››ä¸ªç‚¹ä½œä¸ºä¸€ä¸ªæ ·æœ¬
    #æ¯æ¬¡å‘å‰ä¸€ä¸ªç‚¹
labels = x[tau:].reshape((-1, 1))
#é¢„æµ‹ç‚¹å®å€¼x(4:1000),è½¬åŒ–ä¸ºåˆ—å‘é‡
batch_size, n_train = 16, 600
#æŠŠå‰600ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒ
train_iter = d2l.load_array((features[:n_train], labels[:n_train]),
                            batch_size, is_train=True) #x[599:603]
#Construct a PyTorch data iterator.
```
![](\Images/040-02.png)

**ç½‘ç»œå’Œè®­ç»ƒ**

```
#å®šä¹‰MLPç½‘ç»œ
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
        
def get_net():
    net = nn.Sequential(nn.Linear(4, 10),
                        nn.ReLU(),
                        nn.Linear(10, 1))
    net.apply(init_weights)
    return net

loss = nn.MSELoss(reduction='none')

def train(net, train_iter, loss, epochs, lr):
    trainer = torch.optim.Adam(net.parameters(), lr)
    for epoch in range(epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            l = loss(net(X), y)
            l.sum().backward()
            trainer.step()
        print(f'epoch {epoch + 1}, '
              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')

net = get_net()
train(net, train_iter, loss, 5, 0.01)
#åœ¨è¿™é‡Œå·²ç»è®­ç»ƒå¥½äº†ç½‘ç»œçš„æƒé‡ï¼Œæ‰€ä»¥åœ¨åé¢ç›´æ¥è°ƒç”¨å°±å¯ä»¥äº†
```
**é¢„æµ‹**

```
onestep_preds = net(features)
#å¯¹æ•´ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹
d2l.plot([time, time[tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',
         'x', legend=['data', '1-step preds'], xlim=[1, 1000],
         figsize=(6, 3))
#åˆ†åˆ«å¯¹äºå®é™…å’Œé¢„æµ‹ç»˜å›¾
```
![](\Images/040-03.png)

```
multistep_preds = torch.zeros(T)
multistep_preds[: n_train + tau] = x[: n_train + tau]
#(0:604)
for i in range(n_train + tau, T):
    #(604,1000)
    multistep_preds[i] = net(
        multistep_preds[i - tau:i].reshape((1, -1)))
    #[600,604]ç”¨äºé¢„æµ‹604å¹¶èµ‹å€¼
    #å®é™…ä¸Šå‰599ä¸ªå¹¶æ²¡æœ‰èµ·ä½œç”¨

#ä»å‰603ä¸ªé¢„æµ‹åé¢çš„396ä¸ª
#æ‰€ä»¥å¾ˆç¦»è°±

d2l.plot([time, time[tau:], time[n_train + tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy(),
          multistep_preds[n_train + tau:].detach().numpy()], 'time',
         'x', legend=['data', '1-step preds', 'multistep preds'],
         xlim=[1, 1000], figsize=(6, 3))
```
![](\Images/040-04.png)

```
max_steps = 64

features = torch.zeros((T - tau - max_steps + 1, tau + max_steps))
#(993, tau+max_steps)
for i in range(tau):
    features[:, i] = x[i: i + T - tau - max_steps + 1]
    #ç»™å‰tauä¸ªèµ‹å€¼

for i in range(tau, tau + max_steps):
    features[:, i] = net(features[:, i - tau:i]).reshape(-1)
    #é¢„æµ‹åmax_stepsä¸ª

steps = (1, 4, 16, 64)
d2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],
         [features[:, (tau + i - 1)].detach().numpy() for i in steps], 'time', 'x',
         legend=[f'{i}-step preds' for i in steps], xlim=[5, 1000],
         figsize=(6, 3))
    # steps = 1
    # time[4: 937]
    # features[:, 4] ç”»æ¯è¡Œç¬¬äº”ä¸ªå…ƒç´ 
    # steps = 4 
    # time[7: 937]
    # features[:, 7] ç”»æ¯è¡Œç¬¬å…«ä¸ªå…ƒç´ ï¼Œæˆ–è€…è¯´5-8ä¸ªå…ƒç´ æ˜¯é¢„æµ‹å‡ºæ¥çš„ï¼Œåªé€‰æ‹©ç¬¬8ä¸ª
    
#å®é™…ä¸Šè¿™ä¸ªç®—æ³•çš„åº•å±‚é€»è¾‘éƒ½æ˜¯ç”¨å‰4ä¸ªé¢„æµ‹ç¬¬5ä¸ªï¼Œç„¶åä¾æ¬¡ç±»æ¨ï¼Œç›´åˆ°max_stepå¤„
#ç”»å›¾æ˜¯ä¸ºäº†æŠŠç¬¬stepå¤„çš„é¢„æµ‹è¡¨ç¤ºå‡ºæ¥
#ä¹Ÿå°±æ˜¯è¡¨ç¤ºå‡ºé¢„æµ‹åˆ°ä¸åŒä½ç½®çš„æ‹Ÿåˆç¨‹åº¦
```
![](\Images/040-05.png)

**ä¸€ç‚¹æ„Ÿæ‚Ÿ**

é€»è¾‘æ¯”ä»£ç æœ¬èº«æ›´é‡è¦ã€‚

ä½“ç°åœ¨ä»£ç é‡Œï¼Œå°±æ˜¯é€šè¿‡ç®€å•çš„åˆ‡ç‰‡ä»£æ›¿å¤æ‚çš„å¾ªç¯ï¼Œå®ç°æƒ³è¦è¾¾åˆ°çš„æ•ˆæœï¼Œè¿™éå¸¸äº†ä¸èµ·ã€‚