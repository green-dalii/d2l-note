## 神经网络的梯度

### 🎦 本节课程视频地址 👉[Bilibil](https://www.bilibili.com/video/BV1u64y1i75a?spm_id_from=333.999.0.0)

考虑一个有d层的神经网络。

$${\bf h^t}=f_t({\bf h^{t-1}})\quad and\quad y=l\circ f_d\circ...\circ f_1({\bf x})$$

计算损失 $l$ 关于参数 ${\bf W_t}$ 的梯度

$${\partial l\over\partial{\bf W_t}}={\partial l\over\partial{\bf h^d}}{\partial {\bf h^d}\over\partial{\bf h^{d-1}}}...{\partial {\bf h^{t+1}}\over\partial{\bf h^t}}{\partial {\bf h^t}\over\partial{\bf W^t}}$$

中间相当于$d-t$次矩阵乘法。

数值稳定的常见两个问题：

- **梯度爆炸**

$$1.5^{100}\approx4\times10^{17}$$

- **梯度消失**

$$0.8^{100}\approx2\times10^{-10}$$

**例子：MLP**

加入如下MLP（为了简单省略了偏移）

$$f_t({\bf h^{t-1}})=\sigma(\bf W^th^{t-1})$$
$${\partial{\bf h_t}\over\partial{\bf h^{t-1}}}=diag(\sigma\prime(\bf W^th^{t-1}))(\bf W^t)^T$$

$$\prod_{i=t}^{d-1}{\partial{\bf h^{i+1}}\over\partial\bf h^i}=\prod_{i=t}^{d-1}diag(\sigma\prime({\bf W^ih^{i-1})})(\bf W^i)^T$$

$\sigma()$激活函数对每个元素做运算，输入一个个向量，求导打包为对角矩阵。

如果$d-t$很大，值将会很大

**梯度爆炸**

使用ReLU作为激活函数：

$$\sigma(x)=\max(0,x)\quad and\qquad \sigma\prime(x)={\begin{cases}1&if\ x\lt0\\0&otherwise\end{cases}}$$

$$\prod_{i=t}^{d-1}{\partial{\bf h^{i+1}}\over\partial{\bf h^i}}=\prod_{i=t}^{d-1}diag(\sigma\prime({\bf W^ih^{i-1}}))(\bf W^i)^T$$

其中一些元素来自于${\prod_{i=t}^{d-1}\bf (W^i)^T}$，如果$\bf W$有很大的元素，那么成绩就会很大。

**梯度爆炸的问题**

- 值超出值域(infinity)，对于16位浮点数尤为严重（数值区间6e-5 - 6e4）
- 对学习率敏感
  - 如果学习率太大-大参数值-更大的梯度
  - 如果学习率太小-训练无进展
  - 我们可能需要在训练过程中不断调整学习率

**梯度消失**

使用sigmoid作为激活函数。

![](\Images/sigmoid.png)

坐标轴远方梯度接近于0。

**梯度消失的问题**

- 梯度值变为0
  - 对16位浮点数尤为严重
- 训练没有进展
  - 不管如何选择学习率
- 对于底部层尤为严重
  - 仅仅顶部层训练的较好（反向计算）
  - 无法让神经网络更深

**总结**

当数值过大或过小会导致数值问题；

常发生在深度模型中，因为会对 $n$ 个数累乘。

### 让训练更加稳定

目标：让梯度值在合理的范围内，例如$[1e-6,1e3]$；

- 将乘法变加法：ResNet, LSTM；

- 归一化：梯度归一化（把梯度normalize），梯度剪裁（clipping）；

- 合理的权重初始和激活函数。

**让每层的方差是一个常数**

将每层的输出和梯度都看做随机变量，让每一层的**均值和方差都保持一致**。



$$
正向：\begin{array}{l}
    {\mathbb E[h_i^t]=0}\\
    {Var[h_i^t]=a} 
\end{array}
$$

$$
反向：\mathbb E\left[{\partial l\over \partial h_i^t}\right]=0\quad
Var\left[{\partial l\over \partial h_i^t}\right]=b \quad \forall i,t
$$

即损失函数对每层的每个元素的梯度的期望和偏差也服从恒定。

**权重初始化**

- 在合理值区间里随即初始参数； 
- 在训练开始的时候更容易有数值不稳定（梯度大）； 
  - 远离最优解的地方损失函数表面那可能很复杂；
  - 最优解附近表面会比较平；
- 使用$\aleph(0, 0.01)$来初始可能对小网络没问题，但不能保证深度神经网络。

以MLP为例，假设 $w^t_{i,j}$是遵从独立同分布的，那么$\mathbb E[w^t_{i,j}]=0,\ Var[w^t_{i,j}]=\gamma_t$，$h^{t-1}_i$独立于${w_{i,j}^t}$

假设没有激活函数$\bf h^t=W^th^{t-1}$，这里${\bf W^t}\in\mathbb R^{n_t\times n_{t-1}}$

$$\mathbb E[h_i^t]=\mathbb E\left[\sum_jw_{i,j}^th_j^{t-1}\right]=\sum_j\mathbb E[w_{i,j}]\mathbb E[h_j^{t-1}]=0$$

$$\begin{split}
Var[h_i^t]&=\mathbb E[(h_i^t)^2]-\sout{\mathbb E[(h_i^t)]^2}\\&=\mathbb E\left[\left(\sum_jw_i^th_j^{t-1}\right)^2\right]\\&=\mathbb E\left[\sum_j\left(w_{i,j}^t\right)^2\left(h_j^{t-1}\right)^2+\sout{\sum_{j\ne k}w_{i,j}^tw_{i,k}^th_j^{t-1}h_k^{t-1}}\right]\\&=\sum_jVar[w_{i,j}^t]Var[h_j^{t-1}]\\&=n_{t-1}\gamma_tVar[h_j^{t-1}]\end{split}$$

if input variance equals output variance:

$$n_{t-1}\gamma_t=1$$

**反向均值与方差**

跟正向情况类似

$${\partial l\over\partial{\bf h^{t-1}}}={\partial l\over\partial({{\bf h^t}}/{\bf W^t}})={\partial l\over\partial{\bf h^t}}{\bf W^t}\ \rightarrow\left({\partial l\over\partial{\bf h^{t-1}}}\right)^T=({\bf W^t})^T\left({\partial l\over\partial{\bf h^t}}\right)^T$$

可以理解为对单层每一个输出${h_i^t}$的期望都恒定

$$\begin{split}
\mathbb E\left[{\partial l\over\partial{h_i^{t-1}}}\right]&=\mathbb E\left[\left({\partial l\over\partial{h_i^t}}\right)\left({\partial {h_i^t}\over\partial{h_i^{t-1}}}\right)\right]\\
&=\mathbb E\left[\left({\partial l\over\partial{h_i^t}}\right)\left({\partial {\sum_jw_{i,j}h_j^{t-1}}\over\partial{h_i^{t-1}}}\right)\right]=0
\end{split}$$

$$\begin{split}
Var\left[{\partial l\over\partial{h_i^{t-1}}}\right]&=Var\left[\left({\partial l\over\partial{h_i^t}}\right)\left({\partial {h_i^t}\over\partial{h_i^{t-1}}}\right)\right]\\
&=Var\left[\left({\partial l\over\partial{h_i^t}}\right)\left({\partial {\sum_jw_{i,j}h_j^{t-1}}\over\partial{h_i^{t-1}}}\right)\right]\\
&=n_t\gamma_tVar\left[{\partial l\over\partial{h_i^t}}\right]\ \rightarrow n_t\gamma_t=1
\end{split}$$

**Xavier初始**

初始化权重时设置的方差是根据输入输出维度来确定的。

- 难以需要满足$n_{t-1}\gamma_t=1$和$n_t\gamma_t=1$。因为$n_{t-1}$和$n_t$的值是不可控制的。
- Xavier使得${(n_{t-1}+n_t)\gamma_t\over2}=1\ \rightarrow\ \gamma_t={2\over(n_{t-1}+n_t)}$
  - 正态分布$\aleph(0,\sqrt{2/(n_{t-1}+n_t)})$
  - 均匀分布$\Re(-\sqrt{6/(n_{t-1}+n_t)}),\sqrt{6/(n_{t-1}+n_t)}))$ 
  
     -分布$\Re[-a,a]$和方差是$a^2/3$    
- 适配权重形状变换，特别是$n_t$。

**假如线性的激活函数**

假设$\sigma(x)=\alpha x+\beta$，要使激活函数前后的均值(0)、方差不变(1)。

&emsp;${\bf h'}={\bf W^th^{t-1}}$ and ${\bf h^t}=\sigma({\bf h'})$ 

&emsp;$$\mathbb E[h_i^t]=\mathbb E[\alpha h_i'+\beta]=\beta\ \rightarrow\ \beta=0$$

&emsp;$$\begin{split} Var[h_i^t]&=\mathbb E[(h_i^t)^2]-\mathbb E[(h_i^t)]^2\\&=\mathbb E[(\alpha h_i'+\beta)^2]-\beta^2\\&=\mathbb E[(\alpha^2 (h_i')^2+2\alpha\beta h_i'+\beta^2)]-\beta^2\\&=\alpha^2Var[h_i']\end{split} \ \rightarrow\ \alpha=1$$

**反向**

假设$\sigma(x)=\alpha x+\beta$

$${\partial l\over\partial\bf h'}={\partial l\over\partial\bf h^t}{(\bf W^t)}^T\quad and\quad {\partial l\over\partial\bf h^{t-1}}=\alpha{\partial l\over\partial\bf h'}$$

$$
\begin{split}
\mathbb E\left[\partial l\over\partial h_i^{t-1}\right]&=\mathbb E\left[\left(\partial l\over\partial h_i^t\right)\left(\partial h_i^t\over\partial h_i^{t-1}\right)\right]\\
&=\mathbb E\left[\left(\partial l\over\partial \sigma(h_i')\right)\left(\partial \sigma(h_i')\over\partial h_i^{t-1}\right)\right]\\
&=0\ \rightarrow\ \beta=0
\end{split}
$$

只有$\beta=0$，才可以线性求导，使:

$${\partial l\over\partial\bf h^{t-1}}=\alpha{\partial l\over\partial\bf h'}$$

恒定成立。

$$\begin{split}
Var\left[\partial l\over\partial h_i^{t-1}\right]&=Var\left[\alpha\left(\partial l\over\partial h_i'\right)\right]\\
&=\mathbb E\left[\alpha^2\left(\partial l\over\partial h_i'\right)^2\right]-\mathbb E\left[\alpha\left(\partial l\over\partial h_i'\right)\right]^2\\
&=\alpha^2Var\left[\partial l\over\partial h_i'\right]\rightarrow\ \alpha=1
\end{split}$$

**检查常用激活函数**

使用泰勒展开：
$$\sigma(x)={1\over2}+{x\over4}-{x^3\over48}+O(x^5)$$

$$tanh(x)=0+x-{x^3\over3}+O(x^5)$$

$$relu(x)=0+x\quad x\ge0$$

![](\Images/3-Figure1-1.png)

$tanh(x),relu(x)$在零点附近，存在区间$f(x)=x$，而$sigmoid(x)$则没有，所以可以调整。但是超出了这个区间，会使其均值、方差发生变化。

调整sigmoid:

$$4\times\sigma(x)-2$$

