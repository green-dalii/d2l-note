## 双向循环神经网络

根据上**下**文，预测文本；或者说根据过去和未来的数据做预测。

### 双向RNN

![](\Images/046-01.png)

- 一个前向RNN隐藏层
- 一个反向RNN隐藏层
- 合并两个隐藏状态得到输出
  - 如果隐藏层size为256，合并(concat)就是512

$$\begin{split}
&{\bf H}^{(f)}_{t} = \phi({\bf X}_t{\bf W}_{xh}^{(f)}+{\bf H}_{t-1}^{(f)}{\bf W}_{hh}^{(f)}+{\bf b}_h^{(f)})\\
&{\bf H }_{t}^{(b)} = \phi({\bf X}_t{\bf W}_{xh}^{(b)}+{\bf \vec H}_{t-1}{\bf W}_{hh}^{(b)}+{\bf b}_h^{(b)})\\
&{\bf H}_t=\left[{\bf H}^{(f)}_{t},{\bf H }_t^{(b)}\right]\\
&{\bf O}_t={\bf H }_t{\bf W}_{hq}+{\bf b}_q
\end{split}$$

**推理**

- 可以基于句子做推理（在看到整个句子的情况下）

**总结**

- 双向循环神经网络通过反向更新的隐藏层来利用方向时间信息
- 通常用来对序列抽取特征、填空，而不是预测未来

### 代码实现

```
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps, device = 32, 35, d2l.try_gpu()
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)
# 只做一个改动bidirectional=True，自动创造反向隐藏层
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
num_epochs, lr = 500, 1
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
# 此处是一个错误示范，做预测但没有未来信息
# 所以输出结果没有理由
```
![](\Images/047-01.png)