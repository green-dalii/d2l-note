# 06 - 矩阵计算
---
### 🎦本节课程视频地址👉[Bilibil](https://www.bilibili.com/video/BV1eZ4y1w7PY)

### 🎦关于微积分相关知识，继续强烈推荐**3Blue1Brown**的超棒教程👉[Bilibili](https://space.bilibili.com/88461692/channel/detail?cid=13407&ctype=0)

## 1. 矩阵求导的意义
在深度学习里，本质是参数优化问题，例如数据的特征化、损失函数和优化算法的选择，**都是将原本不可导的问题转换为可导的目标**，进而通过数据迭代计算梯度，再沿梯度反方向微调参数，直至结果收敛在一个合适的区间。

## 2. 将导数拓展到向量
$$
\begin{array}{c|lcr}
& x & \vec{{x}} \\
\hline
y & {\partial{y}\over\partial{x}} \ \text{(标量)} & {\partial{y}\over\partial{\vec{x}}} \ \text{(行向量)} \\
\vec{y}​ & {\partial{\vec{y}}\over\partial{x}} \ \text{(列向量)} & {\partial{\vec{y}}\over\partial{\vec{x}}} \ \text{(矩阵)} \\
\end{array}
$$


- 标量对向量求导得到**行向量**：
$$
\begin{array}{c|lcr}
\text{y} & a & au & sum(\vec{x}) & ||x||^2 \\
\hline
\partial{y}\over{\partial{\vec{x}}} & \vec{0}^T & a{{\partial{u}}\over{\partial{\vec{x}}}} & \vec{1}^T & 2\vec{x}^T
\end{array}
$$

其中， $a$ 表示与向量 $\vec{x}$ 无关的函数；$\vec{0}^T$ 、 $\vec{1}^T$ 是与列向量 $\vec{x}$ 元素数相同的行向量

$$
\begin{array}{c|lcr}
\text{y} & u+v & uv & \langle{\vec{u},\vec{v}}\rangle \\
\hline
\partial{y}\over{\partial{\vec{x}}} & {\partial{u}\over{\partial{\vec{x}}}}+{\partial{v}\over{\partial{\vec{x}}}} & {\partial{u}\over{\partial{\vec{x}}}}v+{\partial{v}\over{\partial{\vec{x}}}}u & \vec{u}^T{\partial{\vec{v}}\over{\partial{\vec{x}}}}+\vec{v}^T{\partial{\vec{u}}\over{\partial{\vec{x}}}}\text{ (行向量)}
\end{array}
$$

- 向量对标量求导得到**列向量**
- 向量对向量求导得到**矩阵**：

相当于向量 $\vec{y}$ 中每一个标量对向量 $\vec{x}$ 求导

当
$$
\vec{x}=
\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix}，y=
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m
\end{bmatrix}
$$
时，

$$
{{\partial\vec{y}}\over{\partial{\vec{x}}}}
=
\begin{bmatrix} 
{\partial{y_1}}\over{\partial{\vec{x}}} \\
{\partial{y_2}}\over{\partial{\vec{x}}} \\
\vdots \\
{\partial{y_m}}\over{\partial{\vec{x}}}
\end{bmatrix}
=
\begin{bmatrix} 
{\partial{y_1}}\over{\partial{x_1}} & {\partial{y_1}}\over{\partial{x_2}} & \cdots & {\partial{y_1}}\over{\partial{x_n}} \\
{\partial{y_2}}\over{\partial{x_1}} & {\partial{y_2}}\over{\partial{x_2}} & \cdots & {\partial{y_2}}\over{\partial{x_n}}\\
\vdots & \vdots & \ddots & \vdots \\
{\partial{y_m}}\over{\partial{x_1}} & {\partial{y_m}}\over{\partial{x_2}} & \cdots & {\partial{y_m}}\over{\partial{x_n}}
\end{bmatrix}

$$
