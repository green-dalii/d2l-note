# 27 - æ®‹å·®ç½‘ç»œResNet

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰[![Bilibil](https://i2.hdslb.com/bfs/archive/300fb344d7e0f1fb18e169c9ed3ecb7af8841143.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1bV41177ap)
## ResNet

**Problem**ï¼šåŠ æ›´å¤šçš„å±‚æ€»æ˜¯æ”¹å–„ç²¾åº¦å—ï¼Ÿ

éšç€æ¨¡å‹å¤æ‚åº¦çš„å¢åŠ ï¼Œå¹¶ä¸æ€»èƒ½å¤Ÿè·ç¦»æœ€ä¼˜è§£æ›´è¿‘ï¼Œå› æ­¤ï¼Œå¦‚æœæ–°æ¨¡å‹çš„ä½œç”¨åŸŸèƒ½å¤ŸåŒ…å«å½“å‰æ¨¡å‹ï¼Œå°±èƒ½ä¿è¯è¿™ä¸€ç‚¹ã€‚

### æ®‹å·®å—(Residual blocks)

- ä¸²è”ä¸€ä¸ªå±‚æ”¹å˜å‡½æ•°ç±»ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½æ‰©å¤§å‡½æ•°ç±»ã€‚

- æ®‹å·®å—åŠ å…¥å¿«é€Ÿé€šé“ï¼ˆå³è¾¹ï¼‰æ¥å¾—åˆ°$f(x)=x+g(x)$

![](\Images/Res%20Block.png)

**ç¨‹åºæ¡†æ¶**

![](\Images/resnet_arch.png)

- é«˜å®½å‡åŠResNetå—(stride=2)
- åæ¥å¤šä¸ªé«˜å®½ä¸å˜çš„ResNet
  - ç”¨1x1Conv skipå¯ä»¥æ”¹å˜è¾“å‡ºé€šé“åŒ¹é…ResNet
- ç±»ä¼¼äºVGGå’ŒGooleNetçš„æ€»ä½“æ¶æ„
  - ä¸€èˆ¬æ˜¯5ä¸ªstage
  - 7x7Conv,BN, 3x3MaxPool
  - æ¯ä¸€ä¸ªstageçš„å…·ä½“æ¡†æ¶å¾ˆçµæ´»
  - å‡ æˆå½“å‰æ ‡é…
- ä½†æ›¿æ¢æˆäº†ResNetå—
  
###  æ€»ç»“

- æ®‹å·®å—ä½¿å¾—å¾ˆæ·±çš„ç½‘ç»œæ›´åŠ å®¹æ˜“è®­ç»ƒ
  - ç”šè‡³å¯ä»¥è®­ç»ƒä¸€åƒå±‚çš„ç½‘ç»œ
- æ®‹å·®ç½‘ç»œå¯¹éšåçš„æ·±å±‚ç¥ç»ç½‘ç»œè®¾è®¡äº§ç”Ÿäº†æ·±è¿œå½±å“ï¼Œæ— è®ºæ˜¯å·ç§¯ç´¯ç½‘ç»œè¿˜æ˜¯å…¨è¿æ¥ç±»ç½‘ç»œ

### ä»£ç å®ç°

```import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l
#å®šä¹‰Residual class
class Residual(nn.Module):
    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)
        #é«˜å®½ä¸å˜
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)
        #default_stride=1
        #é«˜å®½ä¸å˜
     
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)
        #self.relu = nn.ReLU(inplace=True)
    def forward(self, X):
        #è¿™é‡Œå†™é”™äº†forwardï¼ŒæŠ¥äº†ä¸€ä¸ªNonImplementedError
        #æ‰€ä»¥åœ¨å®šä¹‰å‡½æ•°æ—¶çš„ç¬”è¯¯ï¼Œä¸ä¼šè¿½æº¯å…·ä½“ä½ç½®ã€‚
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
```
```
#æµ‹è¯•
blk = Residual(3, 3)
X = torch.rand(4, 3, 6, 6)
Y = blk(X)
Y.shape

blk = Residual(3, 6, use_1x1conv=True, strides=2)
blk(X).shape
```
```
#å®šä¹‰Residual block
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2),
                   nn.BatchNorm2d(64), nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
#ç…§æ¬GoogleNet b1
#å‡ºç°stride=2å°±å¯ä»¥è§†ä¸ºé«˜å®½å‡åŠ
def resnet_block(input_channels, num_channels, num_residuals,
                 first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            #ç¬¬ä¸€å—é«˜å®½å‡åŠ
            blk.append(
                Residual(input_channels, num_channels, use_1x1conv=True,
                         strides=2))
        else:
            blk.append(Residual(num_channels, num_channels))
            #é«˜å®½ä¸å˜
    return blk
```

### ResNetè¡¥å……

ä¸ºä»€ä¹ˆèƒ½è®­ç»ƒå‡ºä¸Šåƒå±‚çš„æ¨¡å‹ï¼ˆè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±ï¼‰ã€‚

$$y=f(x)\quad {\partial y\over\partial w}\quad w=w-lr{\partial y\over\partial w}$$

$$y\prime=g(f(x))\quad{\partial y\prime\over\partial w}={\partial y\prime\over\partial y}{\partial y\over\partial w}$$

å› ä¸ºæ¢¯åº¦ä¼ é€’ï¼Œå°æ•°ç›¸ä¹˜ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œä½†å¯¹äºä¸€ä¸ªæ®‹å·®ç½‘ç»œï¼š

$$y\prime\prime=f(x)+g(f(x))\quad {\partial y\prime\prime\over\partial w}={\partial y\over\partial w}+{\partial y\prime\over\partial w}$$

ä¿è¯äº†ä¸‹é¢çš„å±‚ä¹Ÿå¯ä»¥æ‹¿åˆ°è¾ƒå¤§çš„æ¢¯åº¦ã€‚



