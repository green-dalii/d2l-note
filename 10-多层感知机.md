# 10 - å¤šå±‚æ„ŸçŸ¥æœº

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰[![Bilibil](https://i1.hdslb.com/bfs/archive/6a42dfd892be9e959a264a9352e87478b0cac231.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1hh411U7gn?spm_id_from=333.999.0.0)

## æ„ŸçŸ¥æœº

æœ€æ—©çš„AIæ¨¡å‹ä¹‹ä¸€ã€‚
ç»™å®šè¾“å…¥$\bf{x}$ï¼Œæƒé‡$\bf{w}$ï¼Œå’Œåç§»$b$ï¼Œæ„ŸçŸ¥æœºè¾“å‡ºï¼š

$$o=\sigma(\langle{\bf{w}},{\bf{x}}\rangle+b)$$
$$\sigma(x)=\begin{cases}
1&if\ x\gt0\\-1&otherwise
\end{cases}$$

- **äºŒåˆ†ç±»**


çº¿æ€§å›å½’è¾“å‡ºå®æ•°ï¼›
Softmaxå›å½’è¾“å‡ºæ¦‚ç‡ï¼ˆå¯ä»¥å¤šåˆ†ç±»ï¼‰ï¼›

- **è®­ç»ƒæ„ŸçŸ¥æœº**

**initialize** $w=0$ and $b=0$
**repeat**
    **if** $y_i[\langle{w,x_i}\rangle+b]\le0$ **then**
    $w\leftarrow w+y_ix_i$ and $b\leftarrow b+y_i$
    **end if**
**until** all classified correctly

ç­‰ä»·äºä½¿ç”¨æ‰¹é‡å¤§å°ä¸º1çš„æ¢¯åº¦ä¸‹é™ï¼Œå¹¶ä½¿ç”¨å¦‚ä¸‹çš„æŸå¤±å‡½æ•°

$$l(y,{\bf{x}},{\bf{w}})=max(0,-y\langle{\bf{w}},{\bf{x}}\rangle)$$

æŠŠæ¯ä¸€ä¸ªæ ·æœ¬å•ç‹¬å¸¦å…¥æ›´æ–°æ¢¯åº¦ã€‚

- **æ”¶æ•›å®šç†**

æ•°æ®åœ¨åŠå¾„$r$å†…

ä½™é‡$\rho$åˆ†ç±»ä¸¤ç±»

$$y({\bf{x}}_T{\bf{w}}+b)\ge\rho$$

å¯¹äº$||{\bf{w}}||^2+b^2\le1$
æ„ŸçŸ¥æœºä¿è¯åœ¨${r^2+1}\over\rho^2$æ­¥åæ”¶æ•›ã€‚

![æ„ŸçŸ¥æœº](Images/æ„ŸçŸ¥æœº.jfif)


- **XORé—®é¢˜**(Minsky&Papert,1969)
![XOR](Images/XOR.png)
æ„ŸçŸ¥æœºçš„åˆ†å‰²é¢æ˜¯çº¿æ€§çš„ï¼Œæ‰€ä»¥ä¸èƒ½åˆ†å‰²XORé—®é¢˜ã€‚

å› æ­¤é€ æˆäº†ç¬¬ä¸€æ¬¡AIå¯’å†¬ã€‚

## å¤šå±‚æ„ŸçŸ¥æœº(MLP)

![å¤šå±‚æ„ŸçŸ¥æœºè§£å†³XOR](Images/å¤šå±‚æ„ŸçŸ¥æœº.jpg)

æ—¢ç„¶ä¸€æ¬¡å­¦ä¸äº†ï¼Œå°±ç”¨ç®€å•å‡½æ•°çš„ç»„åˆï¼Œå±‚å±‚åµŒå¥—ï¼Œè§£å†³é—®é¢˜ã€‚

![å¤šå±‚æ„ŸçŸ¥æœº](Images/æœ‰éšè—å±‚çš„æ„ŸçŸ¥æœº.gif)

- **å•éšè—å±‚â€”â€”å•åˆ†ç±»**

![Singel Hidden Layer](Images/1_-eLjPY7UGSoQhSyW5qC6gw.gif)

**Input Layer:** ${\bf{x}}\in{\mathbb R}^n$
**Hidden Layer:** ${\bf{W_1}}\in{\mathbb R}^{m\times n},{\bf{b_1}}\in{\mathbb R}^m$
**Output Layer:** ${\bf{w_2}}\in{\mathbb R}^m,{{b_2}}\in{\mathbb R}$

éšè—å±‚çš„ç»´åº¦$n$æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚

${\bf{h}}=\sigma({\bf{W_1}}{\bf{x}}+{\bf{b_1}})$
$o={\bf{w}}_2^T{\bf{h}}+b_2$

$\sigma$æ˜¯æŒ‰å…ƒç´ çš„æ¿€æ´»å‡½æ•°
æ¿€æ´»å‡½æ•°å¿…é¡»æ˜¯éçº¿æ€§çš„  
å¦‚æœä¸ï¼š
$o=a{\bf{w_2^TW_1x}}+b\prime$
ä»ç„¶æ˜¯çº¿æ€§çš„ï¼Œç­‰äºä¸€ä¸ªå•å±‚æ„ŸçŸ¥æœºã€‚
- **å¸¸ç”¨æ¿€æ´»å‡½æ•°**
**Sigmoidå‡½æ•°**

$$sigmoid(x)={1\over1+\exp(-x)}$$

![Sigmoid Function](Images/SigmoidFunction.png)

**Tanhå‡½æ•°**

$$tanh(x)={1-\exp(-2x)\over1+\exp(-2x)}$$

![tanh](Images\tanh.png)
**ReLUå‡½æ•°**

$$ReLU(x)=\max(x,0)$$
![ReLU](Images\ReLU.png)

- **å¤šç±»åˆ†ç±»**

$$y_1,y_2,...,y_k=softmax(o_1,o_2,...,o_k)$$

**Input Layer:** ${\bf{x}}\in{\mathbb R}^n$
**Hidden Layer:** ${\bf{W_1}}\in{\mathbb R}^{m\times n},{\bf{b_1}}\in{\mathbb R}^m$
**Output Layer:** ${\bf{W_2}}\in{\mathbb R}^{m\times k},{\bf{b_2}}\in{\mathbb R}^k$

${\bf{h}}=\sigma({\bf{W_1}}{\bf{x}}+{\bf{b_1}})$
${\bf o}={\bf{W}}_2^T{\bf{h}}+\bf{b_2}$
${\bf y}=softmax({\bf o})$

- **å¤šéšè—å±‚**

${\bf{h_1}}=\sigma({\bf{W_1}}{\bf{x}}+{\bf{b_1}})$
${\bf{h_2}}=\sigma({\bf{W_2}}{\bf{x}}+{\bf{b_2}})$
${\bf{h_3}}=\sigma({\bf{W_3}}{\bf{x}}+{\bf{b_3}})$
${\bf o}={\bf{W}}_4{\bf{h_3}}+\bf{b_4}$
è¶…å‚æ•°ï¼šéšè—å±‚æ•° $k$ å’Œæ¯å±‚éšè—å±‚çš„å¤§å°$[m_1,m_2,...,m_k]$ï¼Œä¸€èˆ¬å¤§å°æ˜¯é€å±‚é€’å‡çš„ã€‚
## å¤šå±‚æ„ŸçŸ¥æœºçš„ä»£ç å®ç°

- **å¯¼å…¥åŒ…**

```
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

-  **è®¾ç½®å„å±‚å‚æ•°**

```
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True))
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True))
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))
#Parameters()æŒ‡å®šinput(Tensor)ä½œä¸ºæ¨¡å‹å‚æ•°ã€‚
#randn(m.n)è¿”å›ç¬¦åˆéšæœºæ­£æ€åˆ†å¸ƒçš„m*nå¼ é‡


params = [W1, b1, W2, b2]
```

- **å®šä¹‰ReLUæ¿€æ´»å‡½æ•°**

```
def relu(X):
    a = torch.zeros_like(X)
    #_like(X)è¡¨ç¤ºä¸è¾“å…¥Xçš„å½¢çŠ¶ç›¸åŒ
    return torch.max(X,a)
```

- **å®šä¹‰æ„ŸçŸ¥æœºç½‘ç»œå’Œè®­ç»ƒæ¨¡å‹**

```
def net(X):
    X = X.reshape(-1, num_inputs)
    # Xæ˜¯batch_size*len=256*784
    H = (X @ W1 + b1)
    # W1=784*256, H=256*256
    return (H @ W2 + b2)
    # W2=256*10

loss = nn.CrossEntropyLoss()
```
- **è®­ç»ƒ**

```
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
```
- **ç®€æ˜“å®ç°**

```
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0)
        
net.apply(init_weights);

trainer = torch.optim.SGD(net.parameters(), lr=lr)

num_epochs, lr = 10, 0.1
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```