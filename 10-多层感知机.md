# 10 - å¤šå±‚æ„ŸçŸ¥æœº

---

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‡

[![Bilibil](https://i1.hdslb.com/bfs/archive/6a42dfd892be9e959a264a9352e87478b0cac231.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1hh411U7gn?spm_id_from=333.999.0.0)

## æ„ŸçŸ¥æœºï¼ˆPerceptronï¼‰

æœ€æ—©çš„ AI æ¨¡å‹ä¹‹ä¸€ï¼Œç›¸å½“äºå•å±‚ç¥ç»ç½‘ç»œã€‚ç»™å®šè¾“å…¥å‘é‡$\bf{x}$ï¼Œæƒé‡å‘é‡$\bf{w}$ï¼Œå’Œåç§»$b$ï¼Œæ„ŸçŸ¥æœºè¾“å‡ºï¼š

$$o=\sigma(\langle{\bf{w}},{\bf{x}}\rangle+b)$$

$$
\sigma(x)=\left \{
\begin{array}{l}
1\ , \sf{if} \ \it{x>0} \\
0\ , \sf{otherwise}
\end{array}
\right.
$$

![å•å±‚ç¥ç»ç½‘ç»œ](http://zh.d2l.ai/_images/singleneuron.svg)

- **äºŒåˆ†ç±»**ï¼šæ„ŸçŸ¥æœºè¾“å‡ºç¦»æ•£çš„ä¸¤ç±»

  - VS çº¿æ€§å›å½’ï¼šè¾“å‡ºå®æ•°ï¼›

  - VS Softmax å›å½’ï¼šè¾“å‡ºç½®ä¿¡åº¦ï¼›

- **å¦‚ä½•è®­ç»ƒæ„ŸçŸ¥æœº**

```
initialize w=0 and b=0
repeat
    if y_i[<w,x_i>+b] <= 0 then:
        w <- w + y_iÂ·x_i and b <- b + y_i
    end if
until all classified correctly
```

ç­‰ä»·äºä½¿ç”¨æ‰¹é‡å¤§å°ä¸º 1 çš„æ¢¯åº¦ä¸‹é™ï¼Œå¹¶ä½¿ç”¨å¦‚ä¸‹çš„æŸå¤±å‡½æ•°

æŠŠæ¯ä¸€ä¸ªæ ·æœ¬å•ç‹¬å¸¦å…¥æ›´æ–°æ¢¯åº¦ã€‚

- **æ„ŸçŸ¥æœºæ”¶æ•›å®šç†**

  - æ•°æ®åœ¨åŠå¾„$r$å†…

  - ä½™é‡$\rho$åˆ†ç±»ä¸¤ç±»

    $$y({\bf{x}}^T{\bf{w}}+b)\ge\rho, \rho>0$$

  - å¯¹äº$||{\bf{w}}||^2+b^2\le1$ï¼Œæ„ŸçŸ¥æœºä¿è¯åœ¨${r^2+1}\over\rho^2$æ­¥åæ”¶æ•›ã€‚

![æ„ŸçŸ¥æœº](Images/æ„ŸçŸ¥æœº.jfif)

- **XOR é—®é¢˜**(Minsky&Papert,1969)

è®¡ç®—æœºç§‘å­¦å®¶**Marvin Minsky**ï¼ˆåä¸–äººä»¬ç§°ä»–ä¸ºï¼šäººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼‰å’Œ Seymour Papert åœ¨å…¶ 1969 å¹´çš„ä¹¦ã€Šæ„ŸçŸ¥å™¨ã€‹ï¼ˆPerceptronsï¼‰ä¸­æä¾›äº†å•å±‚äººå·¥ç½‘ç»œæ— æ³•æ‰§è¡Œ XOR çš„è¯æ®ã€‚

![XOR](Images/XOR.png)

å› ä¸ºæ„ŸçŸ¥æœºçš„åˆ†å‰²é¢æ˜¯çº¿æ€§çš„ï¼Œåœ¨ä¸Šå›¾ä¸­ä¸ç®¡æ€ä¹ˆåšï¼Œéƒ½æ— æ³•ç”»å‡ºä¸€æ¡çº¿å¯ä»¥å®Œç¾åˆ†ç±»çº¢ç°ä¸¤ç§ç±»åˆ«ã€‚è¿™ä¸ªç»“è®ºæ˜¯å¦‚æ­¤ä»¤äººéœ‡æƒŠï¼Œä»¥è‡³äºè®¸å¤šè®¡ç®—æœºç§‘å­¦å®¶å°†å…¶å½’å’äºç¥ç»ç½‘ç»œç ”ç©¶åœ¨ 1980 å¹´ä»£å‰çš„æŒç»­ä½è¿·çŠ¶æ€ï¼Œå› æ­¤é€ æˆäº†ç¬¬ä¸€æ¬¡ AI å¯’å†¬ã€‚
![](/Images/AI_develop2.jpg)
![](/Images/AI_develop1.jpg)

åæ¥å¼•å…¥éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå¦‚â€œ**æ ¸æ–¹æ³•**â€ã€â€œ**æ”¯æŒå‘é‡æœº**ï¼ˆSVMï¼‰â€ç­‰ï¼‰ã€å¤šå±‚æ„ŸçŸ¥æœºåï¼Œå¯¹ç‰¹å¾ç©ºé—´è¿›è¡Œéçº¿æ€§æ‰­æ›²å˜æ¢ï¼Œæ‰ä½¿ XOR ä¹‹ç±»é—®é¢˜è½¬æ¢ä¸ºçº¿æ€§å¯åˆ†é—®é¢˜å¾—ä»¥è§£å†³ã€‚

![](Images/linear_non_linear.png)

![](https://shuokay.com/content/images/why-relu-works/relu-fold.png)

> æ³¨ï¼šæŸæ—æ´ªå ¡å¤§å­¦ç­‰æœºæ„çš„ç ”ç©¶è€…åœ¨ 2020 å¹´çš„ç ”ç©¶è¯å®ï¼Œäººç±»çš®å±‚ä¸Šå±‚ä¸­å‘ç°çš„ä¸€ç§æ–°å‹ç”µä¿¡å·ï¼Œçš®å±‚ç¥ç»å…ƒæ ‘çªè‡‚ä¸­çš„å¾®å°åŒºå®¤æ¯ä¸ªéƒ½å¯ä»¥æ‰§è¡Œæ•°å­¦é€»è¾‘ä¸Šçš„å¤æ‚æ“ä½œï¼Œè€Œä¸æ˜¯åƒä¹‹å‰äººä»¬æ‰€è®¤ä¸ºçš„éœ€è¦å¤šå±‚ç¥ç»ç½‘ç»œã€‚ä¾‹å¦‚è¿æ„ŸçŸ¥æœºéƒ½æä¸å®šçš„å¼‚æˆ–è¿ç®—ï¼Œå•ä¸ªç¥ç»å…ƒå³å¯è½»æ¾è§£å†³ã€‚
> å…¶è®ºæ–‡[â€œDendritic action potentials and computation in human layer 2/3 cortical neuronsâ€](https://www.science.org/doi/10.1126/science.aax6239)åˆŠç™»åœ¨ã€ŠScienceã€‹ï¼Œå¼•å‘äº†å¹¿æ³›çš„å…³æ³¨ã€‚

## å¤šå±‚æ„ŸçŸ¥æœº(MLP)

![å¤šå±‚æ„ŸçŸ¥æœºè§£å†³XOR](Images/å¤šå±‚æ„ŸçŸ¥æœº.jpg)

æ—¢ç„¶ä¸€å±‚å­¦ä¸äº†ï¼Œå°±ç”¨ç®€å•å‡½æ•°çš„ç»„åˆï¼Œå±‚å±‚åµŒå¥—ï¼Œè§£å†³é—®é¢˜ã€‚
### å•éšè—å±‚â€”â€”å•åˆ†ç±»

![Singel Hidden Layer](Images/1_-eLjPY7UGSoQhSyW5qC6gw.gif)

- **Input Layer:** ${\bf{x}}\in{\mathbb R}^n$
- **Hidden Layer:** ${\bf{W_1}}\in{\mathbb R}^{m\times n},{\bf{b_1}}\in{\mathbb R}^m$
- **Output Layer:** ${\bf{w_2}}\in{\mathbb R}^m,{{b_2}}\in{\mathbb R}$

> éšè—å±‚çš„ç»´åº¦$m$æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚$

å•éšè—å±‚æ„ŸçŸ¥æœºæ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š

${\bf{h}}=\sigma({\bf{W_1}}{\bf{x}}+{\bf{b_1}})$

$o={\bf{w}}_2^T{\bf{h}}+b_2$

> å…¶ä¸­ï¼Œ${\bf{h}}$æ˜¯å•éšè—å±‚çš„è¾“å‡ºï¼Œ$\sigma$ æ˜¯æŒ‰å…ƒç´ çš„æ¿€æ´»å‡½æ•°(activation function)ï¼Œ**æ¿€æ´»å‡½æ•°å¿…é¡»æ˜¯éçº¿æ€§çš„ï¼**

> å¦‚æœæ¿€æ´»å‡½æ•°ä¸ºçº¿æ€§ï¼šåˆ™$o=a{\bf{w_2^TW_1x}}+b\prime$ä»ç„¶æ˜¯çº¿æ€§çš„ï¼Œç­‰äºä¸€ä¸ªå•å±‚æ„ŸçŸ¥æœºã€‚ä¹Ÿå°±æ˜¯è¯´ä¸è®ºæ·±åº¦æœ‰å¤šå°‘å±‚çš„çº¿æ€§æ„ŸçŸ¥æœºï¼Œéƒ½ç­‰ä»·äºå•å±‚æ„ŸçŸ¥æœºã€‚

- **å¸¸ç”¨æ¿€æ´»å‡½æ•°**
  - **Sigmoid å‡½æ•°**ï¼ˆå¤šç”¨äºäºŒåˆ†ç±»é—®é¢˜çš„Logisticå›å½’ï¼‰

  $$sigmoid(x)={{1}\over1+e^{-x}}$$

  ![Sigmoid Function](https://zh.d2l.ai/_images/output_mlp_76f463_39_0.svg)

  - **Tanh å‡½æ•°**

  $$tanh(x)={{1-e^{-2x}}\over1+e^{-2x}}$$
  ![tanh](https://zh.d2l.ai/_images/output_mlp_76f463_63_0.svg)
  - **ReLU å‡½æ•°(Rectified Linear Unit)**

  $$ReLU(x)=\max(x,0)$$

  ![ReLU](https://zh.d2l.ai/_images/output_mlp_76f463_15_0.svg)

  > ReLUæ¿€æ´»å‡½æ•°åº”ç”¨å¹¿æ³›çš„ä¸»è¦åŸå› æ˜¯è®¡ç®—ç®€ä¾¿ï¼Œæ±‚å¯¼æ–¹ä¾¿ï¼ŒåŒæ—¶æ²¡æœ‰æŒ‡æ•°è¿ç®—ï¼ˆä¸€æ¬¡æŒ‡æ•°è¿ç®—åœ¨CPUä¸Šç›¸å½“äºä¸Šç™¾æ¬¡ä¹˜æ³•è¿ç®—ğŸ˜®ï¼‰
### å¤šåˆ†ç±»æ„ŸçŸ¥æœºâ€”â€”ç›¸å½“äºåœ¨Softmaxå›å½’åŠ å…¥ä¸€å±‚éšè—å±‚

$$y_1,y_2,...,y_k=softmax(o_1,o_2,...,o_k)$$

- **Input Layer:** ${\bf{x}}\in{\mathbb R}^n$
- **Hidden Layer:** ${\bf{W_1}}\in{\mathbb R}^{m\times n},{\bf{b_1}}\in{\mathbb R}^m$
- **Output Layer:** ${\bf{W_2}}\in{\mathbb R}^{m\times k},{\bf{b_2}}\in{\mathbb R}^k$

  - ${\bf{h}}=\sigma({\bf{W_1}}{\bf{x}}+{\bf{b_1}})$
  - ${\bf o}={\bf{W}}_2^T{\bf{h}}+\bf{b_2}$
  - ${\bf y}=softmax({\bf o})$

![MLP](https://zh.d2l.ai/_images/mlp.svg)

> ä»¥ä¸Šå›¾ç½‘ç»œä¸ºä¾‹ï¼Œ$n=4,m=5,k=3$
### å¤šéšè—å±‚æ„ŸçŸ¥æœº

![å¤šå±‚æ„ŸçŸ¥æœº](Images/æœ‰éšè—å±‚çš„æ„ŸçŸ¥æœº.gif)

> ä¸Šå›¾å·ç§¯ç¥ç»ç½‘ç»œå¯å¤§è‡´çœ‹ä½œæ˜¯å«æœ‰ä¸‰ä¸ªéšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºã€‚å…¶ä¸­ä¸‰ä¸ªæ¿€æ´»å‡½æ•°å¿…é¡»å‡ä¸ºéçº¿æ€§å‡½æ•°

- ${\bf{h_1}}=\sigma({\bf{W_1}}{\bf{x}}+{\bf{b_1}})$
- ${\bf{h_2}}=\sigma({\bf{W_2}}{\bf{x}}+{\bf{b_2}})$
- ${\bf{h_3}}=\sigma({\bf{W_3}}{\bf{x}}+{\bf{b_3}})$
- ${\bf o}={\bf{W}}_4{\bf{h_3}}+\bf{b_4}$

> è¶…å‚æ•°ï¼š
  > - éšè—å±‚æ•°é‡ $k$ 
  > - æ¯å±‚éšè—å±‚çš„å¤§å°$[m_1,m_2,...,m_k]$

> ä¸€èˆ¬æ¯ä¸ªéšè—å±‚å¤§å°æ˜¯é€å±‚é€’å‡çš„ï¼Œæ­¤å¤„ä¸€ä¸ªå¯è§£é‡Šæ€§æ˜¯å¤šå±‚æ„ŸçŸ¥æœºç›¸å½“äºå¯¹ä¿¡æ¯åšé€å±‚â€œå‹ç¼©â€ï¼Œå³å¯¹çŸ¥è¯†åš**è’¸é¦ï¼ŒDistillation**ï¼‰

## å¤šå±‚æ„ŸçŸ¥æœºçš„ä»£ç å®ç°

- **å¯¼å…¥åŒ…**

```
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

- **è®¾ç½®å„å±‚å‚æ•°**

> ä¸€èˆ¬é€‰ç”¨2çš„å¹‚æ¬¡ä½œä¸ºéšè—å±‚æ•°é‡ï¼Œå› ä¸ºå†…å­˜åœ¨ç¡¬ä»¶ä¸­çš„åˆ†é…å’Œå¯»å€æ–¹å¼ï¼Œè¿™ä¹ˆåšå¾€å¾€å¯ä»¥åœ¨è®¡ç®—ä¸Šæ›´é«˜æ•ˆã€‚

> Parameters()æŒ‡å®šinput(Tensor)ä½œä¸ºæ¨¡å‹å‚æ•°ã€‚
> 
> torch.randn(m.n)è¿”å›ç¬¦åˆéšæœºæ­£æ€åˆ†å¸ƒçš„m*nå¼ é‡
```
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True))
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True))
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
```

- **å®šä¹‰ ReLU æ¿€æ´»å‡½æ•°**

```
def relu(X):
    a = torch.zeros_like(X)   #_like(X)è¡¨ç¤ºä¸è¾“å…¥Xçš„å½¢çŠ¶ç›¸åŒ
    return torch.max(X,a)
```

- **å®šä¹‰æ„ŸçŸ¥æœºç½‘ç»œå’Œè®­ç»ƒæ¨¡å‹**

```
def net(X):
    X = X.reshape(-1, num_inputs)   # Xæ˜¯batch_size*len=256*784
    H = relu(X @ W1 + b1)   # W1=784*256, H=256*256
    return (H @ W2 + b2)    # W2=256*10

loss = nn.CrossEntropyLoss()
```

- **è®­ç»ƒ**

```
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
```

- **ç®€æ˜“å®ç°**

```
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0)

net.apply(init_weights);

trainer = torch.optim.SGD(net.parameters(), lr=lr)

num_epochs, lr = 10, 0.1
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

## Pytorch æ¨¡å—å‚è€ƒæ–‡æ¡£

- `torch.nn.ReLU`PytorchåŸºæœ¬ç¥ç»ç½‘ç»œç»„ä»¶ReLUå±‚ ğŸ§[ä¸­æ–‡](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#torchnn) | [å®˜æ–¹è‹±æ–‡](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)

---

## Q&AğŸ¤“

**Qï¼šå¦‚æœåœ¨æ¨¡å‹å‚æ•°åˆå§‹åŒ–æ—¶ï¼Œå¯¹Wä¸é‡‡ç”¨é«˜æ–¯éšæœºåˆ†å¸ƒ`torch.randn()`ï¼Œè€Œæ˜¯å…¨é›¶`torch.zeros()`æˆ–å…¨1`torch.ones()`åˆ†å¸ƒï¼Œä¼šäº§ç”Ÿä»€ä¹ˆï¼Ÿ**

**ğŸ™‹â€â™‚ï¸**ï¼šç»è¿‡å®éªŒï¼Œå¯¹Wã€bé‡‡ç”¨å…¨é›¶æˆ–å…¨1åˆå§‹åŒ–ï¼Œæˆ–Wå…¨é›¶ã€bå…¨1ï¼Œæˆ–Wå…¨1ã€bå…¨é›¶ï¼Œæ¨¡å‹éƒ½æ— æ³•è®­ç»ƒï¼Œæ¨æµ‹åº”è¯¥æ˜¯ä»¥ä¸Šå››ç§åˆå§‹åŒ–ä¸‹ï¼Œæ— æ³•è®¡ç®—æ¢¯åº¦ï¼Œåˆ™æ— æ³•è¿›è¡Œå‚æ•°æ›´æ–°ã€‚å…·ä½“åŸå› æœ‰å¾…åç»­è®¨è®ºã€‚

**Qï¼šå¦‚æœå°†ReLUæ¿€æ´»å‡½æ•°æ¢æˆå…¶ä»–éçº¿æ€§å‡½æ•°ï¼Œå¯¹ç»“æœæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ**

**ğŸ™‹â€â™‚ï¸**ï¼šé€šè¿‡Benchmarkï¼Œç»“æœå¦‚å›¾ï¼š

![benchmark](Images/non_linear_activation_benchmark.png)

- å¦‚æœæ¢æˆ**Sigmoid**å‡½æ•°ï¼Œæ”¶æ•›æ¯”ReLUã€Tanhæ…¢ï¼ˆReLUå·®ä¸å¤š2ä¸ªEpochå°±åŸºæœ¬æ”¶æ•›ï¼‰ï¼Œè®­ç»ƒæ—¶é—´ç›¸å·®ä¸å¤§ï¼Œç²¾åº¦ä¸å¦‚ReLUã€Tanhï¼›
- å¦‚æœæ¢æˆ**Tanh**å‡½æ•°ï¼Œæ”¶æ•›é€Ÿåº¦ä¸ReLUç›¸å½“ï¼Œè®­ç»ƒæ—¶é—´ç›¸å·®ä¸å¤§ï¼Œç²¾åº¦ä¸ReLUç›¸å½“

**Qï¼šå¦‚æœä¸¤ä¸ªæ„ŸçŸ¥æœºæ¨¡å‹å¤æ‚åº¦ç›¸å½“ï¼Œä¸€ä¸ªæ˜¯æµ…å±‚ã€æ¯å±‚ç¥ç»å…ƒæ•°é‡å¤šçš„æ„ŸçŸ¥æœºï¼Œå¦ä¸€ä¸ªæ˜¯å¤šå±‚ã€æ¯å±‚ç¥ç»å…ƒæ•°é‡å°‘çš„æ„ŸçŸ¥æœºï¼Œä¸ºä»€ä¹ˆéƒ½å€¾å‘äºä½¿ç”¨åè€…ç»“æ„ï¼Ÿ**

**ğŸ™‹â€â™‚ï¸**ï¼šç†è®ºä¸Šæ¥è¯´ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨æ¿€æ´»å‡½æ•°æ˜¯éçº¿æ€§çš„æƒ…å†µä¸‹ï¼Œæ˜¯å¯ä»¥è¾¾åˆ°ç›¸ä¼¼çš„ç²¾åº¦ï¼ˆç†æƒ³æƒ…å†µä¸‹ï¼Œå•éšè—å±‚çš„éçº¿æ€§æ„ŸçŸ¥æœºç†è®ºä¸Šå¯ä»¥æ‹Ÿåˆä»»æ„å‡½æ•°ï¼‰ã€‚ä½†æ˜¯æ›´æ·±çš„ç½‘ç»œï¼Œæ¨¡å‹è®­ç»ƒèµ·æ¥æ›´å®¹æ˜“ã€æ”¶æ•›æ›´å¿«ï¼Œæµ…å±‚ç½‘ç»œå¾ˆéš¾å­¦ä¹ ã€‚å¤šå±‚æ„ŸçŸ¥æœºè§£å†³ä»»åŠ¡ç±»ä¼¼äºâ€œ**åŒ–æ•´ä¸ºé›¶ã€é€ä¸ªå‡»ç ´**â€ç­–ç•¥ã€‚