# 41 - æ–‡æœ¬é¢„å¤„ç†

### ğŸ¦ æœ¬èŠ‚è¯¾ç¨‹è§†é¢‘åœ°å€ ğŸ‘‰[![Bilibil](https://i2.hdslb.com/bfs/archive/851de14b6b74db43bf94c4682bc5c6415ea20ad5.jpg@640w_400h_100Q_1c.webp)](https://www.bilibili.com/video/BV1Fo4y1Q79L)
## æ–‡æœ¬é¢„å¤„ç†

**ä¸‹è½½/è¯»å–æ–‡æœ¬**

```
import collections
import re
from d2l import torch as d2l

d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',
                                '090b5e7e70c295757f55df93cb0a180b9691891a')

def read_time_machine():  
    with open(d2l.download('time_machine'), 'r') as f:
        lines = f.readlines()
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]
    #åœ¨æ–¹æ‹¬å·é‡Œçš„"^"è¡¨ç¤ºéå­—æ¯çš„å…¨éƒ¨ï¼Œ+è¡¨ç¤ºè¿ç»­è‡³å°‘1ä¸ªä»¥ä¸Š
    #subæŠŠlineé‡Œç¬¦åˆ[^A-Za-z]+çš„éƒ½æ›¿æ¢ä¸ºç©ºæ ¼ã€‚
    #æŠŠéå¤§å°å†™è‹±æ–‡å­—æ¯çš„å­—ç¬¦å…¨éƒ½å˜æˆç©ºæ ¼ï¼Œå»é™¤é¦–ä½ç©ºæ ¼ï¼Œå…¨éƒ¨å˜å°å†™
lines = read_time_machine()
print(f'# æ–‡æœ¬æ€»è¡Œæ•°: {len(lines)}')
print(lines[0])
print(lines[10])
```
**è¯å…ƒåŒ–(tokenize)**

```
# æ¯ä¸ªæ–‡æœ¬åºåˆ—è¢«æ‹†åˆ†æˆä¸€ä¸ªè¯å…ƒåˆ—è¡¨ï¼Œè¯å…ƒï¼ˆtokenï¼‰æ˜¯æ–‡æœ¬çš„åŸºæœ¬å•ä½ã€‚
def tokenize(lines, token='word'):      
    if token == 'word':
        #æŒ‰å•è¯
        return [line.split() for line in lines]
        #æŒ‰ç©ºæ ¼åˆ†å‰²å¥å­
    elif token == 'char':
        #æŒ‰å­—æ¯
        return [list(line) for line in lines]
        #æŠŠå­—ç¬¦ä¸²å˜æˆåˆ—è¡¨ï¼ŒæŒ‰å•ä¸ªå­—ç¬¦
    else:
        print('é”™è¯¯ï¼šæœªçŸ¥è¯å…ƒç±»å‹ï¼š' + token)

tokens = tokenize(lines)
for i in range(11):
    print(tokens[i])
```

**å®šåˆ¶ç±»å°è£…æ‰€æœ‰åŠŸèƒ½å‘¢**

```
#æŠŠå­—ç¬¦ä¸²æ˜ å°„åˆ°ä»0å¼€å§‹çš„æ•°å­—ç´¢å¼•ä¸Š
class Vocab:  
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        # min_freqæœ€å°‘å‡ºç°çš„æ¬¡æ•°
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # æŒ‰å‡ºç°é¢‘ç‡æ’åº
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                   reverse=True)
        # ä»å¤§åˆ°å°æ’åˆ—
        self.idx_to_token = ['<unk>'] + reserved_tokens
        # è¯­æ–™åº“ä¸­ä¸å­˜åœ¨æˆ–å·²åˆ é™¤çš„ä»»ä½•è¯å…ƒ
        # å°–æ‹¬å·åœ¨NLPä¸­ä»£è¡¨ä¸€ç±»ç‰¹æ®Šçš„token
        self.token_to_idx = {token: idx
                             for idx, token in enumerate(self.idx_to_token)}
        #è¿™è¡Œä»£ç å¥½åƒæ²¡å•¥ç”¨
        self.idx_to_token, self.token_to_idx = [], dict()
        for token, freq in self._token_freqs: # a Counter
            if freq < min_freq:
                break
                #ä»å¤§åˆ°å°æ’åˆ—ï¼Œå°‘äºé¢‘ç‡çš„å‰”é™¤
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                #æŠŠtokenæ”¾å…¥token_to_idxåˆ—è¡¨ï¼ŒåŒæ—¶åˆ—è¡¨é•¿åº¦åŠ 1
                self.token_to_idx[token] = len(self.idx_to_token) - 1
                #æŠŠtokenä¸åœ¨åˆ—è¡¨çš„å¯¹åº”ç´¢å¼•ä¼ å…¥å­—å…¸

    def __len__(self):
        return len(self.idx_to_token)
        #ä½¿å®ä¾‹å¯ä»¥è¢«len()è°ƒç”¨

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
            # dict.get(key, default=None)
            # æŠŠä¸è®¤è¯†çš„å­—ç¬¦åšæˆunknown
            
        return [self.__getitem__(token) for token in tokens]
        #é€’å½’ï¼Œè‡ªå·±è°ƒç”¨è‡ªå·±
    #tokensä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²çš„åˆ—è¡¨/å…ƒç»„
    #å–å‡ºæ¯ä¸ªtokençš„å­—ç¬¦ä¸²
    #å†è°ƒç”¨è‡ªå·±ï¼Œå­—ç¬¦ä¸²æ»¡è¶³ not isinstance(tokens, (list, tuple)) == True
    #è¿”å›å…¶ç´¢å¼•

    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        #ç´¢å¼•æ—¶æ•°å­—
        return [self.idx_to_token[index] for index in indices]
        #ç´¢å¼•æ˜¯åˆ—è¡¨ã€å…ƒç»„

    @property
    #è£…é¥°å™¨ï¼ŒæŠŠå±æ€§å˜æ–¹æ³•
    #self.unk = 0
    #æ˜¯åªè¯»çš„
    def unk(self):  
        return 0

    @property
    #self.token_freqs = self._token_freqs
    def token_freqs(self):
        return self._token_freqs

def count_corpus(tokens):  
    if len(tokens) == 0 or isinstance(tokens[0], list):
        tokens = [token for line in tokens for token in line]
        #for line in tokens:
        #    for token in line:
        #å¯¹åŒé‡å¾ªç¯çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œåštokenæ“ä½œæ”¾åœ¨æ–°çš„åˆ—è¡¨tokensé‡Œ
        #è¿™é‡Œå°±æŠŠä¸¤å±‚åµŒå¥—åˆ—è¡¨å˜æˆäº†ä¸€å±‚åµŒå¥—
        #tokensçš„æ¯ä¸€è¡Œçš„å­—ç¬¦ä¸²åˆ—è¡¨(line)é‡Œçš„å•è¯å­—ç¬¦ä¸²(token)
        #è¿”å›çš„æ˜¯æ•´ä¸ªæ–‡æœ¬(tokens)æ‰€æœ‰çš„å­—ç¬¦ä¸²
    return collections.Counter(tokens)
    #åˆ©ç”¨collections.Counterè®¡ç®—æ¯ä¸ªå‡ºç°çš„é¢‘ç‡å¹¶åšæˆå­—å…¸ï¼ˆè¯­æ–™ï¼‰
```

**å†ç”¨ä¸€ä¸ªå‡½æ•°é›†æˆæ‰€æœ‰åŠŸèƒ½**

```
vocab = Vocab(tokens)
print(list(vocab.token_to_idx.items())[:10])

def load_corpus_time_machine(max_tokens=-1):  
    lines = read_time_machine()
    tokens = tokenize(lines, 'char')
    vocab = Vocab(tokens)
    # å› ä¸ºæ—¶å…‰æœºå™¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œ
    # æ‰€ä»¥å°†æ‰€æœ‰æ–‡æœ¬è¡Œå±•å¹³åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­
    corpus = [vocab[token] for line in tokens for token in line]
    # è¿”å›æ¯ä¸€ä¸ªtokençš„ç´¢å¼•
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab

corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab)
```
**æ„Ÿæ‚Ÿ**

ç¨‹åºå°è£…å¾ˆé‡è¦ï¼Œé€šè¿‡ç±»ã€å‡½æ•°ï¼Œæ‰“åŒ…ä¸€ç³»åˆ—åŠŸèƒ½ï¼Œå®ç°ä»è¾“å…¥åˆ°è¾“å‡ºã€‚